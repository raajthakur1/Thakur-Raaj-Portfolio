{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Raaj's Portfolio Welcome to my documentation for my engineering/CS courses. This site contains documentation for all of my assignments and projects. Purpose The goal of this portfolio is to: Document my assignments and projects for this course Share my progress with the community Courses Engineering I Engineering II Civil Engineering Honors Data Analytics AP Networking Honors Senior Engineering Book to Box Office Thank you for visiting my portfolio!","title":"Home"},{"location":"#raajs-portfolio","text":"Welcome to my documentation for my engineering/CS courses. This site contains documentation for all of my assignments and projects.","title":"Raaj's Portfolio"},{"location":"#purpose","text":"The goal of this portfolio is to: Document my assignments and projects for this course Share my progress with the community","title":"Purpose"},{"location":"#courses","text":"Engineering I Engineering II Civil Engineering Honors Data Analytics AP Networking Honors Senior Engineering Book to Box Office Thank you for visiting my portfolio!","title":"Courses"},{"location":"about/","text":"About Me My name is Raaj Thakur, and I'm in the Class of 2026 at Charlotte Latin School. Thank you for visiting my porfolio!","title":"About"},{"location":"about/#about-me","text":"My name is Raaj Thakur, and I'm in the Class of 2026 at Charlotte Latin School. Thank you for visiting my porfolio!","title":"About Me"},{"location":"courses/ap_networking/","text":"AP Networking Welcome to my documentation for the AP Networking course for the '25 - '26 school year. Projects Component Cards & Software Slips Command Line Interface Troubleshooting Determining Security Controls for Devices Implementing Security for Devices Networking and Data Movement Network Foundations Why Can't These Two Computers Talk to Each Other? Physical and Logical Addressing Course Overview AP Networking is an AP Career Kickstart\u2122 course that teaches students how to configure network hardware, use protocols to enable reliable and accurate transmission of data, and to protect the transmission of data within and between computer networks. Technical skills learned include: Designing a secure network: Determining appropriate endpoints, network appliances, transmission media, and communication protocols to meet network requirements\u202f Enabling reliable, accurate, and secure transmission in the context of the OSI and TCP/IP models\u202f Configuring a secure network: Constructing, connecting, and documenting network components using appropriate media, communication protocols, and commands\u202f Troubleshooting common issues by testing connectivity, verifying configuration, and monitoring congestion\u202f Protecting computer networks: Identifying potential vulnerabilities in data, devices, and networks\u202f Implementing security controls that address potential vulnerabilities","title":"Overview"},{"location":"courses/ap_networking/#ap-networking","text":"Welcome to my documentation for the AP Networking course for the '25 - '26 school year.","title":"AP Networking"},{"location":"courses/ap_networking/#projects","text":"Component Cards & Software Slips Command Line Interface Troubleshooting Determining Security Controls for Devices Implementing Security for Devices Networking and Data Movement Network Foundations Why Can't These Two Computers Talk to Each Other? Physical and Logical Addressing","title":"Projects"},{"location":"courses/ap_networking/#course-overview","text":"AP Networking is an AP Career Kickstart\u2122 course that teaches students how to configure network hardware, use protocols to enable reliable and accurate transmission of data, and to protect the transmission of data within and between computer networks.","title":"Course Overview"},{"location":"courses/ap_networking/#technical-skills-learned-include","text":"Designing a secure network: Determining appropriate endpoints, network appliances, transmission media, and communication protocols to meet network requirements\u202f Enabling reliable, accurate, and secure transmission in the context of the OSI and TCP/IP models\u202f Configuring a secure network: Constructing, connecting, and documenting network components using appropriate media, communication protocols, and commands\u202f Troubleshooting common issues by testing connectivity, verifying configuration, and monitoring congestion\u202f Protecting computer networks: Identifying potential vulnerabilities in data, devices, and networks\u202f Implementing security controls that address potential vulnerabilities","title":"Technical skills learned include:"},{"location":"courses/ap_networking/cli/","text":"Command Line Interface Project Introduction This project was focused on learning the command line interface of Linux and MacOS. Since Linux is the backbone of networks, IoT systems, and most servers, a strong understanding of the CLI is essential for working with networks. The project included various small activities to teach the class about the CLI, including: Map the Maze Part I: Conceptual overview of file system and introduction to CLI commands. Ubuntu CLI Tutorial: Practice with using CLI commands in an Ubuntu VM Map the Maze Part II: Technical exploration of using CLI commands in Ubuntu, MacOS, and how to share files between a host and VM House Activity: Practice using CLI commands to navigate a \"house\" through the terminal to execute tasks using cd , rm , ls , and more. Reflection Prerequisites In order to run Ubuntu on the M1 Mac minis in the computer lab, UTM was installed and Ubuntu was run inside a VM using Ubuntu with UTM, QEMU, and Apple Hypervisor for Apple Silicon. Also, MKDocs had to be installed on the Mac minis in order to build documentation and push it to GitHub. Unfortunately, simply running pip3 install mkdocs installed the mkdocs package installed it at ~/Library/Python/3.12, the user-specific folder that hosts packages tied to the system-wide python. However, by default, ZSH does not know about this folder, so the ZSH configuration profile had to be edited to point to this folder. Anytime mkdocs needed to be run, python3 -m mkdocs had to be specified rather than just being able to run mkdocs . To fix this issue, the process was: Type find ~/Library/Python -name mkdocs to find the specific path where mkdocs is installed. It should return /Users/*username*/Library/Python/3.xx/bin/mkdocs Open the ZSH configuration file with nano ~/.zshrc , then add this line: export PATH=\"$HOME/Library/Python/3.xx/bin:$PATH\" Refresh the ZSH profile with source ~/.zshrc Run mkdocs --version to confirm it works. It should return something like mkdocs, version 1.6.1 from /Users/*username*/Library/Python/3.xx/lib/python/site-packages/mkdocs (Python 3.xx) After completing these steps, MKDocs commands should be able to run by simply specifying mkdocs + command (build, serve, gh-deploy, etc.) rather than writing out python3 -m mkdocs . Although functionality remains unchanged, MKDocs is much easier and more convenient to work with after making these changes. Map the Maze Part I This assignment was an introduction to the file system of a computer. Important terms learned in this assignment include: Term Definition Root Directory Very top of the file system Folder/Directory Container used to organize files and other folders File Single digital object that holds data in various formats; contains a name and an extension Path \"Address\" of a file or folder inside the file system; either an absolute or relative path Absolute Path Complete address of a file or directory (starting from root) Relative Path Location of a file or directory starting from the working directory Drawing a Filesystem The first activity in this assignment was to draw out a file system with the root at the top, a home folder, 3 sub-folders, and 2 sample files. This section of the assignment was a simple introduction to the file system on a computer and helped in visualizing how a file system works. Text-Based Filesystem & Partner Activity The next activity was to type up a file system similar to the one that was drawn earlier then to trade file systems with a partner and ask them how they would find specific files in the file system. MacOS Terminal Commands After learning about the file system, we learned about essential MacOS/Linux commands and their purposes for the CLI, such as: Command Purpose pwd Prints working directory (absolute path) ls Lists files and directories within the current directory cd Changes working directory to specified directory mkdir Makes a new directory within the current working directory `touch`` Makes a new file in the current working directory or an cp Copies specified files to a specified directory mv Moves specified files to a specified directory open Opens any file or folder within the directory if just a file/folder name is given, or any file on the computer if an absolute path is specified rm Permanently removes specified files rmdir Permanently removes specified directories Ubuntu CLI Tutorial After learning about essential Linux & MacOS CLI commands, we practiced using them in an Ubuntu VM to get familiar with both the commands and using Ubuntu. Following this Ubuntu tutorial made by Canonical , I practiced using the command line in Ubuntu. I used commands like ls , cd , rm , touch , cp , and a couple more. Map the Maze Part II This activity was completed mostly within the Ubuntu VM and involved creating files and directories, editing them with nano , and transferring files between the host and VM. The instructions for the activity were: Run pwd to show current directory Outputs /home/ubuntu (username is ubuntu in the VM) Use cd Documents to enter the documents folder and run pwd to show path Outputs /home/ubuntu/Documents Make a folder titled MazeGame in Documents and enter it with mkdir MazeGame and cd MazeGame Create 3 clue files with touch clue1.txt clue2.txt clue3.txt Add \"Congratulations! You found the first clue.\" to clue1.txt with nano clue1.txt Screenshot of the nano text editor for clue1.txt in the Ubuntu VM. Share a file with the host by copying clue1.txt to ~/hostshare/ with cp clue1.txt ~/hostshare/ On the first attempt, clue1.txt did not show up on the Mac anywhere after putting in in the hostshare folder in the VM. In order to enable file sharing between the host and VM, I needed to follow the following steps: Make a folder anywhere on the Mac, then go into UTM settings and set that folder as the shared directory for the VM In the VM, type sudo mount -t davfs http://127.0.0.1:9843/ ~/hostshare/ in order to set ~/hostshare as a shared directory between the host (M1 Mac mini) and the VM Restart the VM, then mount the shared disk inside UTM by selecting a shared folder Once these steps were completed, files were able to be shared between the host and VM through ~/hostshare in the VM and the folder on the Mac that was selected as a shared directory in UTM's settings. House Sitting Activity The final activity in this project was the House Sitting Activity, where CLI commands were used to execute tasks within a \"house\" in the file system. Prerequisites Before starting, open terminal, change directories to Downloads with cd Downloads , then clone the project by typing sudo git clone https://github.com/thewangclass/CK-Building-Content-Knowledge-Workshop . Once this is completed, the house can be explored by following the instructions. Procedure Walk to the house and go inside. Use cd house Where can we go? ls returns bedroom1, bedroom2, garage, kitchen, and main_entrance Go inside the main entrance Use cd main_entrance See if there is anything around in the main entrance, such as instructions. ls returns instructions.txt, unopened_mail1.txt, unopened_mail2.txt, unopened_mail3.txt, and shoerack Open the instructions. Use cat instructions.txt to display the contents in the terminal. Leave the main entrance and go back to the house level. Use cd .. to go up one level in the file system in order to access other rooms Go inside the kitchen Use cd kitchen Check out what's inside the kitchen. ls returns apple, banana, cereal, crackers, donut, milk, orange \"Eat\" 2 items of food by removing them. Use rm -r apple donut to remove them. The -r argument after rm tells the rm command to act recursively and remove all of the arguments, not just the first one. You smell something, but you cannot see it. Find out what it is. Use ls -a in order to list all files, including hidden ones. This reveals everything from just writing ls , but also includes .rotten_bananas, a hidden file (files beginning with a period are hidden). Throw away the bananas. Use rm .rotten_bananas to delete the file Check the bedrooms to make sure everything is okay. Use cd .. to move up one level, then use cd bedroom1 to enter bedroom 1. Conduct a thorough search to look for anything unusual. Use ls -a to reveal everything, including hidden files. There's a hidden file titled .secret_diary.txt. Reveal the contents using cat .secret_diary.txt Now, enter bedroom 2 and explore it Use cd .. to move up one level then cd bedroom2 to enter the 2nd bedroom. Then, use ls -a to reveal all files. There was only regular items such as a chair, desk, and messy bed. Explore each item with ls -a [name] in order to list out the contents of each item. Desk should have a file titled search_desk.txt. The lights just went out and you are lost. Figure out where you currently are. Use pwd to print your location (directory) Navigate to the garage to figure out what is wrong with the lights. Use cd .. then cd garage to go to the garage. The lights suddenly come back. What is in the garage? Use ls -a to list the garage's contents. There are 3 cardboard boxes, 4 garbage bags, and a hose in the garage. Remove all of the garbage as a favor to your friend. Use rm -r garbage1 garbage2 garbage3 to get rid of all the garbage bags. Then, cd into each cardboard box and see which ones have garbage in them. Use rm -r on whichever boxes have garbage in them to delete them. Return to the main entrance. Use cd ../.. to go up 2 levels, then use cd main_entrance to enter the main entrance. Leave a note for your friend. Use touch goodbye_note.txt to create a file, then use nano goodbye_note.txt to add a message. There may be a hidden room in the house that was not previously explored. Look for it. Use cd .. to go up one level, then use ls -a to reveal all files/directories. It should reveal .hidden_basement. Enter the hidden basement with cd .hidden_basement , then list all files/directories with ls -a . This should reveal .hidden_stash. Reflection This project was a thorough introduction and exploration of the CLI for MacOS and Linux, and taught important commands such as cd , ls , and many more. The CLI is essential in networking environments since servers never have any kind of GUI to interact with. Instead, people interact with servers solely throught the CLI. Therefore, in order to work with IoT devices, servers, and basically anything related to networking, a strong grasp of how to use the CLI effectively is imperative. While working with a CLI was a bit intimidating at first, it was easy enough to get familiar with the commands and effectively carry out tasks.","title":"Command Line Interface"},{"location":"courses/ap_networking/cli/#command-line-interface","text":"","title":"Command Line Interface"},{"location":"courses/ap_networking/cli/#project-introduction","text":"This project was focused on learning the command line interface of Linux and MacOS. Since Linux is the backbone of networks, IoT systems, and most servers, a strong understanding of the CLI is essential for working with networks. The project included various small activities to teach the class about the CLI, including: Map the Maze Part I: Conceptual overview of file system and introduction to CLI commands. Ubuntu CLI Tutorial: Practice with using CLI commands in an Ubuntu VM Map the Maze Part II: Technical exploration of using CLI commands in Ubuntu, MacOS, and how to share files between a host and VM House Activity: Practice using CLI commands to navigate a \"house\" through the terminal to execute tasks using cd , rm , ls , and more. Reflection","title":"Project Introduction"},{"location":"courses/ap_networking/cli/#prerequisites","text":"In order to run Ubuntu on the M1 Mac minis in the computer lab, UTM was installed and Ubuntu was run inside a VM using Ubuntu with UTM, QEMU, and Apple Hypervisor for Apple Silicon. Also, MKDocs had to be installed on the Mac minis in order to build documentation and push it to GitHub. Unfortunately, simply running pip3 install mkdocs installed the mkdocs package installed it at ~/Library/Python/3.12, the user-specific folder that hosts packages tied to the system-wide python. However, by default, ZSH does not know about this folder, so the ZSH configuration profile had to be edited to point to this folder. Anytime mkdocs needed to be run, python3 -m mkdocs had to be specified rather than just being able to run mkdocs . To fix this issue, the process was: Type find ~/Library/Python -name mkdocs to find the specific path where mkdocs is installed. It should return /Users/*username*/Library/Python/3.xx/bin/mkdocs Open the ZSH configuration file with nano ~/.zshrc , then add this line: export PATH=\"$HOME/Library/Python/3.xx/bin:$PATH\" Refresh the ZSH profile with source ~/.zshrc Run mkdocs --version to confirm it works. It should return something like mkdocs, version 1.6.1 from /Users/*username*/Library/Python/3.xx/lib/python/site-packages/mkdocs (Python 3.xx) After completing these steps, MKDocs commands should be able to run by simply specifying mkdocs + command (build, serve, gh-deploy, etc.) rather than writing out python3 -m mkdocs . Although functionality remains unchanged, MKDocs is much easier and more convenient to work with after making these changes.","title":"Prerequisites"},{"location":"courses/ap_networking/cli/#map-the-maze-part-i","text":"This assignment was an introduction to the file system of a computer. Important terms learned in this assignment include: Term Definition Root Directory Very top of the file system Folder/Directory Container used to organize files and other folders File Single digital object that holds data in various formats; contains a name and an extension Path \"Address\" of a file or folder inside the file system; either an absolute or relative path Absolute Path Complete address of a file or directory (starting from root) Relative Path Location of a file or directory starting from the working directory","title":"Map the Maze Part I"},{"location":"courses/ap_networking/cli/#drawing-a-filesystem","text":"The first activity in this assignment was to draw out a file system with the root at the top, a home folder, 3 sub-folders, and 2 sample files. This section of the assignment was a simple introduction to the file system on a computer and helped in visualizing how a file system works.","title":"Drawing a Filesystem"},{"location":"courses/ap_networking/cli/#text-based-filesystem-partner-activity","text":"The next activity was to type up a file system similar to the one that was drawn earlier then to trade file systems with a partner and ask them how they would find specific files in the file system.","title":"Text-Based Filesystem &amp; Partner Activity"},{"location":"courses/ap_networking/cli/#macos-terminal-commands","text":"After learning about the file system, we learned about essential MacOS/Linux commands and their purposes for the CLI, such as: Command Purpose pwd Prints working directory (absolute path) ls Lists files and directories within the current directory cd Changes working directory to specified directory mkdir Makes a new directory within the current working directory `touch`` Makes a new file in the current working directory or an cp Copies specified files to a specified directory mv Moves specified files to a specified directory open Opens any file or folder within the directory if just a file/folder name is given, or any file on the computer if an absolute path is specified rm Permanently removes specified files rmdir Permanently removes specified directories","title":"MacOS Terminal Commands"},{"location":"courses/ap_networking/cli/#ubuntu-cli-tutorial","text":"After learning about essential Linux & MacOS CLI commands, we practiced using them in an Ubuntu VM to get familiar with both the commands and using Ubuntu. Following this Ubuntu tutorial made by Canonical , I practiced using the command line in Ubuntu. I used commands like ls , cd , rm , touch , cp , and a couple more.","title":"Ubuntu CLI Tutorial"},{"location":"courses/ap_networking/cli/#map-the-maze-part-ii","text":"This activity was completed mostly within the Ubuntu VM and involved creating files and directories, editing them with nano , and transferring files between the host and VM. The instructions for the activity were: Run pwd to show current directory Outputs /home/ubuntu (username is ubuntu in the VM) Use cd Documents to enter the documents folder and run pwd to show path Outputs /home/ubuntu/Documents Make a folder titled MazeGame in Documents and enter it with mkdir MazeGame and cd MazeGame Create 3 clue files with touch clue1.txt clue2.txt clue3.txt Add \"Congratulations! You found the first clue.\" to clue1.txt with nano clue1.txt Screenshot of the nano text editor for clue1.txt in the Ubuntu VM. Share a file with the host by copying clue1.txt to ~/hostshare/ with cp clue1.txt ~/hostshare/ On the first attempt, clue1.txt did not show up on the Mac anywhere after putting in in the hostshare folder in the VM. In order to enable file sharing between the host and VM, I needed to follow the following steps: Make a folder anywhere on the Mac, then go into UTM settings and set that folder as the shared directory for the VM In the VM, type sudo mount -t davfs http://127.0.0.1:9843/ ~/hostshare/ in order to set ~/hostshare as a shared directory between the host (M1 Mac mini) and the VM Restart the VM, then mount the shared disk inside UTM by selecting a shared folder Once these steps were completed, files were able to be shared between the host and VM through ~/hostshare in the VM and the folder on the Mac that was selected as a shared directory in UTM's settings.","title":"Map the Maze Part II"},{"location":"courses/ap_networking/cli/#house-sitting-activity","text":"The final activity in this project was the House Sitting Activity, where CLI commands were used to execute tasks within a \"house\" in the file system.","title":"House Sitting Activity"},{"location":"courses/ap_networking/cli/#prerequisites_1","text":"Before starting, open terminal, change directories to Downloads with cd Downloads , then clone the project by typing sudo git clone https://github.com/thewangclass/CK-Building-Content-Knowledge-Workshop . Once this is completed, the house can be explored by following the instructions.","title":"Prerequisites"},{"location":"courses/ap_networking/cli/#procedure","text":"Walk to the house and go inside. Use cd house Where can we go? ls returns bedroom1, bedroom2, garage, kitchen, and main_entrance Go inside the main entrance Use cd main_entrance See if there is anything around in the main entrance, such as instructions. ls returns instructions.txt, unopened_mail1.txt, unopened_mail2.txt, unopened_mail3.txt, and shoerack Open the instructions. Use cat instructions.txt to display the contents in the terminal. Leave the main entrance and go back to the house level. Use cd .. to go up one level in the file system in order to access other rooms Go inside the kitchen Use cd kitchen Check out what's inside the kitchen. ls returns apple, banana, cereal, crackers, donut, milk, orange \"Eat\" 2 items of food by removing them. Use rm -r apple donut to remove them. The -r argument after rm tells the rm command to act recursively and remove all of the arguments, not just the first one. You smell something, but you cannot see it. Find out what it is. Use ls -a in order to list all files, including hidden ones. This reveals everything from just writing ls , but also includes .rotten_bananas, a hidden file (files beginning with a period are hidden). Throw away the bananas. Use rm .rotten_bananas to delete the file Check the bedrooms to make sure everything is okay. Use cd .. to move up one level, then use cd bedroom1 to enter bedroom 1. Conduct a thorough search to look for anything unusual. Use ls -a to reveal everything, including hidden files. There's a hidden file titled .secret_diary.txt. Reveal the contents using cat .secret_diary.txt Now, enter bedroom 2 and explore it Use cd .. to move up one level then cd bedroom2 to enter the 2nd bedroom. Then, use ls -a to reveal all files. There was only regular items such as a chair, desk, and messy bed. Explore each item with ls -a [name] in order to list out the contents of each item. Desk should have a file titled search_desk.txt. The lights just went out and you are lost. Figure out where you currently are. Use pwd to print your location (directory) Navigate to the garage to figure out what is wrong with the lights. Use cd .. then cd garage to go to the garage. The lights suddenly come back. What is in the garage? Use ls -a to list the garage's contents. There are 3 cardboard boxes, 4 garbage bags, and a hose in the garage. Remove all of the garbage as a favor to your friend. Use rm -r garbage1 garbage2 garbage3 to get rid of all the garbage bags. Then, cd into each cardboard box and see which ones have garbage in them. Use rm -r on whichever boxes have garbage in them to delete them. Return to the main entrance. Use cd ../.. to go up 2 levels, then use cd main_entrance to enter the main entrance. Leave a note for your friend. Use touch goodbye_note.txt to create a file, then use nano goodbye_note.txt to add a message. There may be a hidden room in the house that was not previously explored. Look for it. Use cd .. to go up one level, then use ls -a to reveal all files/directories. It should reveal .hidden_basement. Enter the hidden basement with cd .hidden_basement , then list all files/directories with ls -a . This should reveal .hidden_stash.","title":"Procedure"},{"location":"courses/ap_networking/cli/#reflection","text":"This project was a thorough introduction and exploration of the CLI for MacOS and Linux, and taught important commands such as cd , ls , and many more. The CLI is essential in networking environments since servers never have any kind of GUI to interact with. Instead, people interact with servers solely throught the CLI. Therefore, in order to work with IoT devices, servers, and basically anything related to networking, a strong grasp of how to use the CLI effectively is imperative. While working with a CLI was a bit intimidating at first, it was easy enough to get familiar with the commands and effectively carry out tasks.","title":"Reflection"},{"location":"courses/ap_networking/component-cards-and-software-slips/","text":"Component Cards & Software Slips Project Introduction Component Cards and Software Slips was the first project of the year, and was designed to teach us about the various hardware and software components of a PC, and to teach us about how they all worked together. The project included various small activities to help us learn, including: Silent Signals Metal to Magic A Component Song Silent Signals Silent Signals was the first part of the project. In this activity, my partner and I each received a 3D printed card with a white side and a blue side, and our goal was to communicate various messages with each other solely with the cards, and without any kind of gesturing or speaking. From the start, we knew that we needed to establish some kind of standardized method of communication with each other. We mutually decided that to communicate numbers, we should use the card to tap the desk to represent the quantity of the number, and for boolean questions (yes/no, A/B, etc.) that we needed to assign one color to option 1 and the other color to option 2. Unfortunately, we couldn't talk, so we ended up picking different colors than each other. After trying out my system with partners, our new goal was to design a communication system to communicate the following information without gestures or speaking: The number 3 The month October \u201cYes\u201d to a yes/no question My plan for this was to: Start by tapping the table thrice to communicate the integer value of 3 Waiting a bit, then tapping the table 10 times to communicate October, since it's the 10th month Lastly, to communicate \"yes\" to a yes/no question, I would show the white side of the card. Although there's no way to guarantee that my partner would know white means yes, if you think about the card as an on/off switch, white would likely be \"on\" since it's light, and blue would be \"off\" since it's darker. Therefore, white would signify \"yes.\" This method required a bit of luck that my partner would understand what I was trying to communicate, since there was no way to actually talk with him to tell him what each color meant. Silent Signals Reflection The purpose of \"Silent Signals\" was to highlight the importance of a shared communication protocol. Similarly to how my partner and I were able to communicate more easily when we had a mutual understanding of what each others' signals meant, in order for machines to communicate effectively, they need to understand each others' protocols. Component Song The next part of Component Cards and Software Slips was the Component Song. Using ChatGPT, I generated song lyrics that described the multiple hardware and software components of a computer, and I then used Suno in order to have AI-generated vocals and instrumentals for the song. Song lyrics and link to song Component Song Reflection Creating the Component Song was a fun way to learn about the different parts of a computer, and helped me learn about what exactly each part does. Metal to Magic Metal to Magic was the main component of this project. In Metal to Magic, we formally learned about the many different hardware and software components of a PC, such as: Hardware Components Hardware Purpose CPU Executes instructions from programs RAM Temporarily stores data and instructions the CPU is currently using SSD/HDD Computer's long term data storage sytem GPU Processor specialized for parallel processing; useful for graphics processing and matrix multiplication Motherboard Main circuit board inside the computer; connects all hardware components and allows them to communicate via buses PSU Device that powers the system NIC Translates data between computer and network; enables WiFi capabilities Cooling System Keeps components cool in order to prevent overheating and thermal throttling I/O Devices Tools the user utilizes to interact with the computer (keyboard, microphone, mouse, camera, display, etc) Software Components Software Purpose Firmware/UEFI Starts the PC and hands off to OS; motherboard firmware Drivers Lets the OS talk to hardware OS Manages files, hardware, programs, UI, and much more Libraries/Runtimes Pre-written code for apps shared building blocks for apps Applications Programs for the user to interact with to complete tasks Hardware and Software Flowcharts After learning about what each component of a PC did, we needed to learn about how they all worked together. So, with a partner, I took cards with hardware and software components and arranged them to represent how individual components all interact with each other to get tasks done. We started off with keeping the hardware and software flowcharts separate, then at the end, trying to combine both hardware and software cards to visualize what exactly a computer does when printing out an essay. Hardware Flowchart Here, I tried to emphasize how virtually everything communicates through the motherboard, which is why I had the motherboard at the center and all of the components connected individually to the board. Software Flowchart Making the software component flowchart was a bit easier, since with software, there is a clearer hierarchy of the different layers, with the UEFI being at the lowest level and the runtimes and apps being at the highest level. Printing an Essay Flowchart (Hardware + Software) Before designing this flowchart, I thought through what exactly happened on my computer when I hit print, and did my best to represent that in my flowchart. Build a PC Activity With this knowledge about what the hardware and software components of a PC do and how they work together, a partner and I were given the task to upgrade a PC with a given budget to be most optimized for a task, such as gaming, video editing in 4K, AI training, etc. We chose to build a PC that was specialized for 4K video editing, and were given a budget of $1000 to upgrade along with a list of parts for purchase. Current Parts List CPU: mid-range 4-core processor RAM: 8 GB Storage: 256 GB SSD GPU: basic integrated graphics PSU: 500W basic model Standard cooling Basic NIC (network card) Motherboard that supports most modern upgrades Parts for Purchase Component Upgrade Option Cost CPU Mid-range 6-core processor $150 High-end 8-core processor $300 RAM 16 GB total RAM $150 32 GB total RAM $300 Storage 512 GB SSD $150 1 TB SSD $250 2 TB HDD (extra, for bulk storage) $100 GPU Mid-range graphics card (good for gaming, video) $250 High-end graphics card (best for gaming, ML) $400 Cooling System Enhanced air cooling $100 Liquid cooling system $200 NIC 2.5 Gbps network card $100 Other Extra case fans, RGB lighting, style upgrades $50 What we Decided to Do Before deciding anything immediately, my partner an I reflected on what exactly a video editing workflow demanded the most. We decided that the biggest limiting factor in our current PC build was the 8GB of RAM, since video editing consumes a lot of RAM. Although 32GB RAM would have been nice, we only had the budget to increase to 16GB, which is still enough for 4K editing (total spent: $150). Next, we upgraded the cooling system, since video editing often requires long exports, which often take multiple hours. This would inevitably generate a lot of heat, so maximum cooling was necessary to ensure that the sustained performance of the system was good. We went with the enhanced air cooling system along with extra case fans (total spent: $300). Next, we upgraded the GPU, since our current build only had a very weak integrated graphics chip. Video editing leans a lot on the GPU, and the current integrated graphics would make 4K video editing a nightmare. We went with the mid-range graphics card, since we didn't have the budget for a high-end graphics card (total: $550). After that, we upgraded the CPU to the high-end 8 core processor, because along with using the GPU a lot, video editing requires a CPU with strong multithreaded performance, so we went with the highest core-count CPU available (total: $850). Finally, with the remaining $150, we upgraded to the 512GB SSD, since video files take up a lot of storage. More storage would have been nice, but we didn't have the budget for anything else, and we figured that the editor could always use a NAS or cloud storage for storing older files (total: $1000). Metal to Magic Reflection Overall, Metal to Magic was very helpful in understanding not only what the individual components of a computer do, but also in explaining how the components work together. I learned about how standardized protocols are essential for machines to communicate with each other, and even for parts a machine to communicate with other parts.","title":"Component Cards & Software Slips"},{"location":"courses/ap_networking/component-cards-and-software-slips/#component-cards-software-slips","text":"","title":"Component Cards &amp; Software Slips"},{"location":"courses/ap_networking/component-cards-and-software-slips/#project-introduction","text":"Component Cards and Software Slips was the first project of the year, and was designed to teach us about the various hardware and software components of a PC, and to teach us about how they all worked together. The project included various small activities to help us learn, including: Silent Signals Metal to Magic A Component Song","title":"Project Introduction"},{"location":"courses/ap_networking/component-cards-and-software-slips/#silent-signals","text":"Silent Signals was the first part of the project. In this activity, my partner and I each received a 3D printed card with a white side and a blue side, and our goal was to communicate various messages with each other solely with the cards, and without any kind of gesturing or speaking. From the start, we knew that we needed to establish some kind of standardized method of communication with each other. We mutually decided that to communicate numbers, we should use the card to tap the desk to represent the quantity of the number, and for boolean questions (yes/no, A/B, etc.) that we needed to assign one color to option 1 and the other color to option 2. Unfortunately, we couldn't talk, so we ended up picking different colors than each other. After trying out my system with partners, our new goal was to design a communication system to communicate the following information without gestures or speaking: The number 3 The month October \u201cYes\u201d to a yes/no question My plan for this was to: Start by tapping the table thrice to communicate the integer value of 3 Waiting a bit, then tapping the table 10 times to communicate October, since it's the 10th month Lastly, to communicate \"yes\" to a yes/no question, I would show the white side of the card. Although there's no way to guarantee that my partner would know white means yes, if you think about the card as an on/off switch, white would likely be \"on\" since it's light, and blue would be \"off\" since it's darker. Therefore, white would signify \"yes.\" This method required a bit of luck that my partner would understand what I was trying to communicate, since there was no way to actually talk with him to tell him what each color meant. Silent Signals Reflection The purpose of \"Silent Signals\" was to highlight the importance of a shared communication protocol. Similarly to how my partner and I were able to communicate more easily when we had a mutual understanding of what each others' signals meant, in order for machines to communicate effectively, they need to understand each others' protocols.","title":"Silent Signals"},{"location":"courses/ap_networking/component-cards-and-software-slips/#component-song","text":"The next part of Component Cards and Software Slips was the Component Song. Using ChatGPT, I generated song lyrics that described the multiple hardware and software components of a computer, and I then used Suno in order to have AI-generated vocals and instrumentals for the song. Song lyrics and link to song Component Song Reflection Creating the Component Song was a fun way to learn about the different parts of a computer, and helped me learn about what exactly each part does.","title":"Component Song"},{"location":"courses/ap_networking/component-cards-and-software-slips/#metal-to-magic","text":"Metal to Magic was the main component of this project. In Metal to Magic, we formally learned about the many different hardware and software components of a PC, such as: Hardware Components Hardware Purpose CPU Executes instructions from programs RAM Temporarily stores data and instructions the CPU is currently using SSD/HDD Computer's long term data storage sytem GPU Processor specialized for parallel processing; useful for graphics processing and matrix multiplication Motherboard Main circuit board inside the computer; connects all hardware components and allows them to communicate via buses PSU Device that powers the system NIC Translates data between computer and network; enables WiFi capabilities Cooling System Keeps components cool in order to prevent overheating and thermal throttling I/O Devices Tools the user utilizes to interact with the computer (keyboard, microphone, mouse, camera, display, etc) Software Components Software Purpose Firmware/UEFI Starts the PC and hands off to OS; motherboard firmware Drivers Lets the OS talk to hardware OS Manages files, hardware, programs, UI, and much more Libraries/Runtimes Pre-written code for apps shared building blocks for apps Applications Programs for the user to interact with to complete tasks","title":"Metal to Magic"},{"location":"courses/ap_networking/component-cards-and-software-slips/#hardware-and-software-flowcharts","text":"After learning about what each component of a PC did, we needed to learn about how they all worked together. So, with a partner, I took cards with hardware and software components and arranged them to represent how individual components all interact with each other to get tasks done. We started off with keeping the hardware and software flowcharts separate, then at the end, trying to combine both hardware and software cards to visualize what exactly a computer does when printing out an essay.","title":"Hardware and Software Flowcharts"},{"location":"courses/ap_networking/component-cards-and-software-slips/#hardware-flowchart","text":"Here, I tried to emphasize how virtually everything communicates through the motherboard, which is why I had the motherboard at the center and all of the components connected individually to the board.","title":"Hardware Flowchart"},{"location":"courses/ap_networking/component-cards-and-software-slips/#software-flowchart","text":"Making the software component flowchart was a bit easier, since with software, there is a clearer hierarchy of the different layers, with the UEFI being at the lowest level and the runtimes and apps being at the highest level.","title":"Software Flowchart"},{"location":"courses/ap_networking/component-cards-and-software-slips/#printing-an-essay-flowchart-hardware-software","text":"Before designing this flowchart, I thought through what exactly happened on my computer when I hit print, and did my best to represent that in my flowchart.","title":"Printing an Essay Flowchart (Hardware + Software)"},{"location":"courses/ap_networking/component-cards-and-software-slips/#build-a-pc-activity","text":"With this knowledge about what the hardware and software components of a PC do and how they work together, a partner and I were given the task to upgrade a PC with a given budget to be most optimized for a task, such as gaming, video editing in 4K, AI training, etc. We chose to build a PC that was specialized for 4K video editing, and were given a budget of $1000 to upgrade along with a list of parts for purchase. Current Parts List CPU: mid-range 4-core processor RAM: 8 GB Storage: 256 GB SSD GPU: basic integrated graphics PSU: 500W basic model Standard cooling Basic NIC (network card) Motherboard that supports most modern upgrades Parts for Purchase Component Upgrade Option Cost CPU Mid-range 6-core processor $150 High-end 8-core processor $300 RAM 16 GB total RAM $150 32 GB total RAM $300 Storage 512 GB SSD $150 1 TB SSD $250 2 TB HDD (extra, for bulk storage) $100 GPU Mid-range graphics card (good for gaming, video) $250 High-end graphics card (best for gaming, ML) $400 Cooling System Enhanced air cooling $100 Liquid cooling system $200 NIC 2.5 Gbps network card $100 Other Extra case fans, RGB lighting, style upgrades $50 What we Decided to Do Before deciding anything immediately, my partner an I reflected on what exactly a video editing workflow demanded the most. We decided that the biggest limiting factor in our current PC build was the 8GB of RAM, since video editing consumes a lot of RAM. Although 32GB RAM would have been nice, we only had the budget to increase to 16GB, which is still enough for 4K editing (total spent: $150). Next, we upgraded the cooling system, since video editing often requires long exports, which often take multiple hours. This would inevitably generate a lot of heat, so maximum cooling was necessary to ensure that the sustained performance of the system was good. We went with the enhanced air cooling system along with extra case fans (total spent: $300). Next, we upgraded the GPU, since our current build only had a very weak integrated graphics chip. Video editing leans a lot on the GPU, and the current integrated graphics would make 4K video editing a nightmare. We went with the mid-range graphics card, since we didn't have the budget for a high-end graphics card (total: $550). After that, we upgraded the CPU to the high-end 8 core processor, because along with using the GPU a lot, video editing requires a CPU with strong multithreaded performance, so we went with the highest core-count CPU available (total: $850). Finally, with the remaining $150, we upgraded to the 512GB SSD, since video files take up a lot of storage. More storage would have been nice, but we didn't have the budget for anything else, and we figured that the editor could always use a NAS or cloud storage for storing older files (total: $1000).","title":"Build a PC Activity"},{"location":"courses/ap_networking/component-cards-and-software-slips/#metal-to-magic-reflection","text":"Overall, Metal to Magic was very helpful in understanding not only what the individual components of a computer do, but also in explaining how the components work together. I learned about how standardized protocols are essential for machines to communicate with each other, and even for parts a machine to communicate with other parts.","title":"Metal to Magic Reflection"},{"location":"courses/ap_networking/device-security/","text":"Implementing Security in Devices Table of Contents Project Introduction NIST and OWASP Guidelines Designing a Secure Password and Implementing it in Ubuntu Incorporating MFA in Ubuntu with Google Authenticator Patching Outdated Software in Ubuntu Reflection Project Introduction This project was focused on implementing different methods to secure devices such as creating/using password algorithms to create secure passwords, installing MFA tools in Ubuntu, patching outdated software, and incorporating NIST/OWASP guidelines. NIST and OWASP Guidelines NIST (National Institute of Standards and Technology) and OWASP (Open Worldwide Application Security Project) are both organizations that help develop standards and guidelines to help improve security, among other things. Both of these organizations release on creating strong passwords and securing devices. Common themes between the two organizations include: Use passwords that are both complex enough to be strong yet memorable enough to remember, as well as being lengthy Incorporate some sort of multi factor authentication (MFA) for services and devices, such as biometric authentication, verification with a one time password (OTP), or security questions Designing a Secure Password and Implementing it in Ubuntu Creating a Password Algorithm to Make a New Password Before this activity, the password for the Ubuntu VM is just \"ubuntu\". As one can imagine, \"ubuntu\" is not a very secure password! To generate a new password, I came up with the following algorithm, ensuring that a complex, long, yet memorable password could be used: Write out as many digits of \u03c0 as you can. The most you can easily write down is the base of the new password Then, split your name in half and add the first half to the front of the digits of \u03c0 and the second half to the end. If your name has an odd amount of characters, put the greater half at the start and the smaller half at the end. For example, if your name is Max and you know 10 digits of \u03c0, your password so far would be Ma3.141592653x. Then, replace all instances of lowercase a with @, lowercase i with !, lowercase e with 3, lowercase o with 0, lowercase s with 5, and lowercase l with 1. The final password for the example would be M@3.141592653x Implementing the New Password in Ubuntu To secure Ubuntu, the newly-generated password needed to be implemented. In order to change the password, the passwd command was run, which prompted for the current password (\"ubuntu\") then the new password (which was generated using the algorithm listed above). After entering the new password under the passwd command, the password should have changed (note: in the screenshot below, new password was mistyped a few times, which is why the computer printed \"sorry, passwords do not match\" a few times. The password did end up changing). In order to test to see if the new password works, sudo ls /root was run. The expected output is all of the contents of the /root directory. Since this command requires admin (sudo) privileges, it asked for the password. The newly-changed password was entered, and it worked, meaning that the passwd command successfully changed the password in Ubuntu, and that it works when running tasks requiring root access. Running the command returned snap, since that was the only item present in the /root directory. This indicates that the command worked, therefore the password is valid. Now, Ubuntu is more secure than before due to a much more complex password. Incorporating MFA in Ubuntu with Google Authenticator After the secure password was implemented, the next step in securing the Ubuntu VM was to implement some sort of MFA. Due to its widespread usage and relative simplicity, Google Authenticator was chosen as the method for implementing MFA. Google Authenticator works by generating a new TOTP (Temporary One Time Password) every 30 seconds with an algorithm. The Authenticator application and Google's server have the same algorithm, so when the code generated on the phone is used to authenticate, the account/service connects to Google's server to verify that the code is correct. If Google's server has the same code as the one that was inputted by the user, then the user is granted access to the account/service. To install Google Authenticator in Ubuntu, the following commands needed to be run: sudo apt update : Updates system packages to make sure the system is up to date, which helps prevent installation errors and ensures that the system is secure before setting up anything new. sudo apt install libpam-google-authenticator -y : Installs the Google Authenticator package for Ubuntu via the apt package manager and agrees to install the package with the -y flag. After installing the Google Authenticator package, the command google-authenticator was run to start the configuration process. To set it up, the following questions were answered: Prompt Answer Do you want authentication tokens to be time-based (y/n) y Update your .google_authenticator file? y Disallow multiple uses of the same token? y (Token valid for 30 seconds) Press Enter Enable rate-limiting? y After answering the questions, the terminal displayed a QR code which can be used to link Ubuntu to a Google Account for use with Google Authenticator. It also displayed recovery codes, which can be used in place of the TOTP generated in the app in case there is an issue with the app or the device is not connected to the internet. Those codes can only be used once. Additionally, it provided a secret key, which can also be used to link Ubuntu to the Google Authenticator app (in fact, the QR code displayed is just the secret key represented as a scannable code). Unfortunately, since the school iPads don't have the Google Authenticator, a TOTP website had to be used as a workaround ( the website should never be used as an actual method of security; it was used solely to verify that Google Authenticator is working. Never enter a secret key into any website ). To verify that Google Authenticator was working, the secret key was copied and pasted into the website, which enabled the website to generate a 6 digit verification code every 30 seconds. In Ubuntu, the google-authenticator command was re-run, and when it prompted to enter a code from the app, the code from the website was entered. It accepted the code, which means that the Google Authenticator installation works. The last step was to configure Ubuntu to require both a password and a code from Google Authenticator when logging in. The steps to do so were: Ensure PAM + keyboard-interactive are enabled in sshd. To do so, edit the SSH server configuration file with sudo nano /etc/ssh/sshd_config , find the line KbdInteractiveAuthentication no , and change it to KbdInteractiveAuthentication yes . Then, make sure UsePAM yes exists, and that it is set to \"yes\". Additionally, change #PasswordAuthentication no to #PasswordAuthentication yes if it isn't already set to \"yes\". Once these changes are made, save and exit the file, then restart the SSH server with sudo systemctl restart ssh . Add Google Authenticator to the SSH PAM stack. To do so, open the SSH PAM configuration file with sudo nano /etc/pam.d/sshd , then right below the header comment, add auth required pam_google_authenticator.so . Save and exit the file, then restart SSH again with sudo systemctl restart ssh . Test the login. To do so, SSH to the VM with ssh [username]@localhost (note: replace [username] with your actual Ubuntu username). It should prompt for botha password and verification code, indicating that Google Authenticator is set up for SSH. Patching Outdated Software in Ubuntu The last step in increasing the security of Ubuntu was to patch outdated software. Every computer needs regular patches to stay secure, since they fix bugs, security holes, and performance issues. This was done with the following steps: Check for available updates: To list out the packages that can be updated, sudo apt list --upgradable was run. On my specific VM, 1158 packages were outdated and had an upgrade available. Apply the updates (patches): To install the updates, sudo apt upgrade was run. This installs all the updates for the packages that were listed as \"upgradable\". Check the update history to verify that the update was installed: To verify that the update was installed, open the log file for apt with cat /var/log/apt/history.log . It returns the date, time, and list of packages that were updated. However, it is difficult to see a specific update due to the quantity of information in this log. Searching the log for a specific update to ensure a specific package has been updated: To search for updates on a specific date, the command grep \"yyyy-mm\" /var/log/apt/history.log can be used (replace yyyy-mm with a specific year and month) to show updates in a specific month. To show every package, software, and dependency that is installed on the system, grep \"Install:\" /var/log/apt/history.log was run. An example output is: Install: python3:amd64 (3.10.12-0ubuntu0.22.04.1, automatic) Part Meaning Install: This line lists newly installed software packages python3:amd64 The name of the package (Python 3 for 64-bit architecture (3.10.12-Oubuntu0.22.04.1) The version number of the package automatic Means the package was installed as a dependency (another program needed it) Example output from grep \"Install:\" /var/log/apt/history.log : The output is very long and unreadable. To search for a specific package, \"Install\" can be replaced with the package name (nano, firefox, etc.), meaning the command would like like this: grep \"firefox\" /var/lig/apt/history.log . To show recent updates, the last lines of history.log can be shown with tail -n 20 /var/log/apt/history.log (note: 20 can be replaced with the desired amount of lines (10, 20, 32, 67, 102, etc.)). Ensuring that automatic updates have been applied: Ubuntu schedules automatic updates. The command ls -l /var/lib/apt/periodic can be run to see when Ubuntu last ran an automatic update. Reflection In modern times, securing devices is more important than ever. As our lives become more reliant on digital tools, keeping our digital lives safe from hackers is of the utmost importance. Having secure passwords for devices and online accounts is one of the easiest ways to be protected from hackers, and everybody should consider making secure passwords. However, just having a secure password often is not enough to be safe. This is why MFA is vital, since in combination with a secure password, it makes it exceptionally difficult for a hacker to break into a system or account. Since most enterprise servers and systems are Ubuntu-based, learning how to implement these changes in Ubuntu is very helpful in many careers.","title":"Implementing Security for Devices"},{"location":"courses/ap_networking/device-security/#implementing-security-in-devices","text":"","title":"Implementing Security in Devices"},{"location":"courses/ap_networking/device-security/#table-of-contents","text":"","title":"Table of Contents"},{"location":"courses/ap_networking/device-security/#project-introduction","text":"","title":"Project Introduction"},{"location":"courses/ap_networking/device-security/#nist-and-owasp-guidelines","text":"","title":"NIST and OWASP Guidelines"},{"location":"courses/ap_networking/device-security/#designing-a-secure-password-and-implementing-it-in-ubuntu","text":"","title":"Designing a Secure Password and Implementing it in Ubuntu"},{"location":"courses/ap_networking/device-security/#incorporating-mfa-in-ubuntu-with-google-authenticator","text":"","title":"Incorporating MFA in Ubuntu with Google Authenticator"},{"location":"courses/ap_networking/device-security/#patching-outdated-software-in-ubuntu","text":"","title":"Patching Outdated Software in Ubuntu"},{"location":"courses/ap_networking/device-security/#reflection","text":"","title":"Reflection"},{"location":"courses/ap_networking/device-security/#project-introduction_1","text":"This project was focused on implementing different methods to secure devices such as creating/using password algorithms to create secure passwords, installing MFA tools in Ubuntu, patching outdated software, and incorporating NIST/OWASP guidelines.","title":"Project Introduction"},{"location":"courses/ap_networking/device-security/#nist-and-owasp-guidelines_1","text":"NIST (National Institute of Standards and Technology) and OWASP (Open Worldwide Application Security Project) are both organizations that help develop standards and guidelines to help improve security, among other things. Both of these organizations release on creating strong passwords and securing devices. Common themes between the two organizations include: Use passwords that are both complex enough to be strong yet memorable enough to remember, as well as being lengthy Incorporate some sort of multi factor authentication (MFA) for services and devices, such as biometric authentication, verification with a one time password (OTP), or security questions","title":"NIST and OWASP Guidelines"},{"location":"courses/ap_networking/device-security/#designing-a-secure-password-and-implementing-it-in-ubuntu_1","text":"","title":"Designing a Secure Password and Implementing it in Ubuntu"},{"location":"courses/ap_networking/device-security/#creating-a-password-algorithm-to-make-a-new-password","text":"Before this activity, the password for the Ubuntu VM is just \"ubuntu\". As one can imagine, \"ubuntu\" is not a very secure password! To generate a new password, I came up with the following algorithm, ensuring that a complex, long, yet memorable password could be used: Write out as many digits of \u03c0 as you can. The most you can easily write down is the base of the new password Then, split your name in half and add the first half to the front of the digits of \u03c0 and the second half to the end. If your name has an odd amount of characters, put the greater half at the start and the smaller half at the end. For example, if your name is Max and you know 10 digits of \u03c0, your password so far would be Ma3.141592653x. Then, replace all instances of lowercase a with @, lowercase i with !, lowercase e with 3, lowercase o with 0, lowercase s with 5, and lowercase l with 1. The final password for the example would be M@3.141592653x","title":"Creating a Password Algorithm to Make a New Password"},{"location":"courses/ap_networking/device-security/#implementing-the-new-password-in-ubuntu","text":"To secure Ubuntu, the newly-generated password needed to be implemented. In order to change the password, the passwd command was run, which prompted for the current password (\"ubuntu\") then the new password (which was generated using the algorithm listed above). After entering the new password under the passwd command, the password should have changed (note: in the screenshot below, new password was mistyped a few times, which is why the computer printed \"sorry, passwords do not match\" a few times. The password did end up changing). In order to test to see if the new password works, sudo ls /root was run. The expected output is all of the contents of the /root directory. Since this command requires admin (sudo) privileges, it asked for the password. The newly-changed password was entered, and it worked, meaning that the passwd command successfully changed the password in Ubuntu, and that it works when running tasks requiring root access. Running the command returned snap, since that was the only item present in the /root directory. This indicates that the command worked, therefore the password is valid. Now, Ubuntu is more secure than before due to a much more complex password.","title":"Implementing the New Password in Ubuntu"},{"location":"courses/ap_networking/device-security/#incorporating-mfa-in-ubuntu-with-google-authenticator_1","text":"After the secure password was implemented, the next step in securing the Ubuntu VM was to implement some sort of MFA. Due to its widespread usage and relative simplicity, Google Authenticator was chosen as the method for implementing MFA. Google Authenticator works by generating a new TOTP (Temporary One Time Password) every 30 seconds with an algorithm. The Authenticator application and Google's server have the same algorithm, so when the code generated on the phone is used to authenticate, the account/service connects to Google's server to verify that the code is correct. If Google's server has the same code as the one that was inputted by the user, then the user is granted access to the account/service. To install Google Authenticator in Ubuntu, the following commands needed to be run: sudo apt update : Updates system packages to make sure the system is up to date, which helps prevent installation errors and ensures that the system is secure before setting up anything new. sudo apt install libpam-google-authenticator -y : Installs the Google Authenticator package for Ubuntu via the apt package manager and agrees to install the package with the -y flag. After installing the Google Authenticator package, the command google-authenticator was run to start the configuration process. To set it up, the following questions were answered: Prompt Answer Do you want authentication tokens to be time-based (y/n) y Update your .google_authenticator file? y Disallow multiple uses of the same token? y (Token valid for 30 seconds) Press Enter Enable rate-limiting? y After answering the questions, the terminal displayed a QR code which can be used to link Ubuntu to a Google Account for use with Google Authenticator. It also displayed recovery codes, which can be used in place of the TOTP generated in the app in case there is an issue with the app or the device is not connected to the internet. Those codes can only be used once. Additionally, it provided a secret key, which can also be used to link Ubuntu to the Google Authenticator app (in fact, the QR code displayed is just the secret key represented as a scannable code). Unfortunately, since the school iPads don't have the Google Authenticator, a TOTP website had to be used as a workaround ( the website should never be used as an actual method of security; it was used solely to verify that Google Authenticator is working. Never enter a secret key into any website ). To verify that Google Authenticator was working, the secret key was copied and pasted into the website, which enabled the website to generate a 6 digit verification code every 30 seconds. In Ubuntu, the google-authenticator command was re-run, and when it prompted to enter a code from the app, the code from the website was entered. It accepted the code, which means that the Google Authenticator installation works. The last step was to configure Ubuntu to require both a password and a code from Google Authenticator when logging in. The steps to do so were: Ensure PAM + keyboard-interactive are enabled in sshd. To do so, edit the SSH server configuration file with sudo nano /etc/ssh/sshd_config , find the line KbdInteractiveAuthentication no , and change it to KbdInteractiveAuthentication yes . Then, make sure UsePAM yes exists, and that it is set to \"yes\". Additionally, change #PasswordAuthentication no to #PasswordAuthentication yes if it isn't already set to \"yes\". Once these changes are made, save and exit the file, then restart the SSH server with sudo systemctl restart ssh . Add Google Authenticator to the SSH PAM stack. To do so, open the SSH PAM configuration file with sudo nano /etc/pam.d/sshd , then right below the header comment, add auth required pam_google_authenticator.so . Save and exit the file, then restart SSH again with sudo systemctl restart ssh . Test the login. To do so, SSH to the VM with ssh [username]@localhost (note: replace [username] with your actual Ubuntu username). It should prompt for botha password and verification code, indicating that Google Authenticator is set up for SSH.","title":"Incorporating MFA in Ubuntu with Google Authenticator"},{"location":"courses/ap_networking/device-security/#patching-outdated-software-in-ubuntu_1","text":"The last step in increasing the security of Ubuntu was to patch outdated software. Every computer needs regular patches to stay secure, since they fix bugs, security holes, and performance issues. This was done with the following steps: Check for available updates: To list out the packages that can be updated, sudo apt list --upgradable was run. On my specific VM, 1158 packages were outdated and had an upgrade available. Apply the updates (patches): To install the updates, sudo apt upgrade was run. This installs all the updates for the packages that were listed as \"upgradable\". Check the update history to verify that the update was installed: To verify that the update was installed, open the log file for apt with cat /var/log/apt/history.log . It returns the date, time, and list of packages that were updated. However, it is difficult to see a specific update due to the quantity of information in this log. Searching the log for a specific update to ensure a specific package has been updated: To search for updates on a specific date, the command grep \"yyyy-mm\" /var/log/apt/history.log can be used (replace yyyy-mm with a specific year and month) to show updates in a specific month. To show every package, software, and dependency that is installed on the system, grep \"Install:\" /var/log/apt/history.log was run. An example output is: Install: python3:amd64 (3.10.12-0ubuntu0.22.04.1, automatic) Part Meaning Install: This line lists newly installed software packages python3:amd64 The name of the package (Python 3 for 64-bit architecture (3.10.12-Oubuntu0.22.04.1) The version number of the package automatic Means the package was installed as a dependency (another program needed it) Example output from grep \"Install:\" /var/log/apt/history.log : The output is very long and unreadable. To search for a specific package, \"Install\" can be replaced with the package name (nano, firefox, etc.), meaning the command would like like this: grep \"firefox\" /var/lig/apt/history.log . To show recent updates, the last lines of history.log can be shown with tail -n 20 /var/log/apt/history.log (note: 20 can be replaced with the desired amount of lines (10, 20, 32, 67, 102, etc.)). Ensuring that automatic updates have been applied: Ubuntu schedules automatic updates. The command ls -l /var/lib/apt/periodic can be run to see when Ubuntu last ran an automatic update.","title":"Patching Outdated Software in Ubuntu"},{"location":"courses/ap_networking/device-security/#reflection_1","text":"In modern times, securing devices is more important than ever. As our lives become more reliant on digital tools, keeping our digital lives safe from hackers is of the utmost importance. Having secure passwords for devices and online accounts is one of the easiest ways to be protected from hackers, and everybody should consider making secure passwords. However, just having a secure password often is not enough to be safe. This is why MFA is vital, since in combination with a secure password, it makes it exceptionally difficult for a hacker to break into a system or account. Since most enterprise servers and systems are Ubuntu-based, learning how to implement these changes in Ubuntu is very helpful in many careers.","title":"Reflection"},{"location":"courses/ap_networking/network_foundations/","text":"Networking Foundations: Layers, Cables, and Connectivity This project was focused on exploring aspects of a network that make up layers 1 and 2 of the OSI model, such as Ethernet cables, IP addresses, physical layout, and more. These aspects were investigated through various activities. Table of Contents Exploring IP Addresses in Shared and Bridged Modes in UTM Network Topology Activity Cable Constructing and Testing Exploring Layers 1 and 2 of the OSI Model (Network Access Layer of TCP/IP Model) Building and Testing a SOHO Network Reflection Exploring IP Addresses in Shared and Bridged Modes in UTM This activity explored how Shared (NAT) and Bridged network modes in UTM affect a VM's IP address and how it connects to the internet. Part 1: Exploring Shared (NAT) Mode Before doing anything, the VM had to be set to Shared (NAT) mode. To do so, the VM's settings were edited in UTM by right clicking the VM, selecting network settings in the left sidebar, and changing the network mode to \"Shared\". Once the VM was set to Shared Network mode, the internal IP address was found with ip a , resulting in the following output: Term Explanation inet IPv4 address (192.168.64.2) /24 Represents the subnet mask (255.255.255.0), which signifies that the first 24 bits (192.168.64) of the IP address designate the network and the remaining 8 bits designate the device on the network brd 192.168.64.255 The broadcast address for the subnet scope global dynamic enp0s1 \u201cGlobal\u201d means this IP can reach outside the VM (through NAT), and \u201cdynamic\u201d means it was assigned automatically (DHCP). Private IP addresses reveal how the device interacts with the LAN, not the internet. To see a device's IP on the internet, otherwise known as a public IP address , a website such as https://whatismyipaddress.com can be used to find it. The VM's public IP was 173.95.44.210. In Shared mode, the public and private IPs were different. 192.168.64.2 belonged to the LAN whereas 173.95.44.210 belonged to the internet. Shared mode might be used by a VM when connecting to the internet because it appears as the same device as the host machine on the LAN and is subject to the same firewall and device restrictions as the host. Shared mode also makes it easier to connect multiple VMs on a single computer since they all share an IP address, making it easier to work with than dealing with individual IPs for each VM. Part 2: Exploring Bridged Mode After exploring Shared (NAT) Mode in UTM, bridged mode was explored. To switch the VM from Shared to Bridged mode, the VM was shut down, then its network settings were changed. Once this change was made, the VM was started again. Once Ubuntu was loaded, a similar procedure was followed. The IP address was again found with ip a . However, the results were different: Then, the public IP was checked with the same website . While the private IP was completely different, the public IP was exactly the same (173.95.44.210). While the private IP address changed from 192.168.64.2 to 10.1235.30, the public IP address did not change at all. This is because the IP address of the device has no impact on the router's IP address, which is what determines the public IP. In Bridged Mode, the VM appears as an entirely different computer on the LAN, which is very different to Shared Mode. This quality would lead to an organization often choosing Bridged over Shared mode in a corporate environment because corporations may use a VM to test out new software, and while testing, they would want to make sure that the software is in an environment as similar to as a real computer as possible. Shared mode simply does not provide such an environment, whereas Bridged mode is ideal for such an application. However, the fact that Bridged mode behaves like a separate computer poses a security risk, as it could provide an opportunity to bypass network or device restrictions imposed on the host machine. Comparison Table Mode Private IP Public IP Shared (NAT) 192.168.64.2 173.95.44.210 Bridged 10.12.25.30 173.95.44.210 When comparing the two modes, Shared mode appears as the same device as the host on the network, whereas Bridged mode appears as a separate device. Due to this quality, Shared mode provides a safer, more controlled environment whereas Bridged mode provides an environment akin to a physical computer. Reflection How did your IP addresses change between Shared and Bridged mode? My private IP address changed between Shared and Bridged mode, since it switched from being the same as the Mac\u2019s IP address to being a completely different address on the LAN. The public IP address did not change, since that is controlled by the router, not the individual devices connected to it. What did this experiment teach you about how local and public networks communicate? This experiment taught me about how Public IP addresses are based off of the router, and the IP addresses of endpoint devices on the LAN do not affect the Public IP address. It also showed me how Bridged and Shared mode in UTM are different, since a Bridged VM has an IP separate from the host, whereas a Shared VM has the same IP as the host. Why might IT professionals use different network configurations for home, business, or lab environments? IT professionals might use different network configurations for home, business, or lab environments. In business environments, IT professionals would likely want to keep the amount of IP addresses available to a minimum, so they would use Shared mode. On the other hand, lab environments and home users may use Bridged mode since it would allow them to simulate how a full computer with the virtualized OS would work, since Bridged mode behaves like a separate computer on the network (it has its own MAC and IP address). Which mode do you think is best for classroom use, and why? Shared mode is better for classroom use since it is easier for IT to manage, and it ensures that students can\u2019t bypass the school\u2019s firewall or network restrictions. Network Topology Activity The next activity was the Network Topology Activity. Network topology is the way that computers, devices, and other pieces of technology are arranged and connected within a network. It\u2019s like a map or blueprint that shows how data travels from one device to another. Every network includes devices that either send, receive, or direct data. The connect and communicate in the following way: - End Devices: These are the computers, smartphones, printers, and other tools people use. They send and receive data. - Networking Devices: These control or manage traffic between devices. - Switch: Connects multiple devices on the same local network (like in a classroom or office). - Router: Connects different networks (like your home network to the internet). - Access Point: Allows wireless devices to connect using Wi-Fi. - Cables or Wireless Connections: The \u201croads\u201d that data travels along. - Examples include Ethernet cables, fiber optic cables, or radio waves (for Wi-Fi). When data travels, it moves in packets \u2014 small chunks of information that include details about where they\u2019re coming from and where they\u2019re going. The topology determines which path those packets take and how efficiently they reach their destination. Example of a network with 3 computers, 1 networking device (router in this case) and 1 printer: Common network topologies include: Star Topography Once central switch or hub in the middle All computers connect to that central point Common in office network and home Wi-Fi routers Bus Topography A single straight line (\"backbone cable\") with all computers branching off. Common in early Ethernet networks (now outdated). Ring Topography Devices form a circle with connections between neighbors Data travels one way (or both in dual-ring) Common in some legacy fiber networks and token ring systems Mesh Topography Every device connects to multiple others If one path breaks, another cna still carry data Common in data centers and wireless mesh networks with many IoT devices Hybrid Topography Combine two or more (for example, multiple Star networks connected in a Bus layout) Common in large organizations with multiple departments or floors Reflection: For a small business, the star topology is the easiest to set up because it only requires connecting each device to a central router, making installation and troubleshooting simple. The mesh topology is the most reliable when a connection fails since it provides multiple paths for data to travel, allowing the network to keep functioning even if one link goes down. However, that same redundancy makes mesh the most expensive to implement due to the large number of cables and connections required. A school would most likely use a star or hybrid topology\u2014star within classrooms and a hybrid structure between floors or buildings\u2014because it offers a balance of scalability, cost-efficiency, and control. Overall, the physical layout of a network topology directly affects speed and reliability: topologies with shared paths (like bus or ring) can create bottlenecks or single points of failure, while layouts with dedicated or redundant paths (like star and mesh) provide faster performance and greater resilience due to the multiple pathways for data to travel across. Cable Constructing and Testing After learning about network topography, the next activity was building and testing cables. Every network, from a classroom computer lab to a corporate data center - relies on Ethernet cables to carry data between devices. Professional technicians and network engineers often make and test their own cables instead of buying them pre-made, allowing them to create cables at precise lengths, verify quality, and save on cost. Building and testing cables works at the Physical Layer (Layer 1) of the OSI Model. This is where data is converted to electrical signals that travel through wires. At this layer: Cable quality determines speed and signal reliability Misplaced wires or bad crimps can cause communication failures Proper testing ensures that the cable works before being installed in a network When constructing and testing the Ethernet cables, the following equipment was used: Tool Purpose When It's Used Cable Stripper Removes the outer jacket from the Ethernet cable without damaging the internal wires. Used to expose the eight colored wires within the cable Cable Cutter Cuts the cable cleanly and evenly before and after stripping Used at the start and end for making precise cuts RJ45 Crimp Tool Presses the metal pins inside the RJ45 connector into each wire, securing them and creating an electrical connection Used after the wires are arranged an inserted Pass-Through RJ45 Connectors Plastic connectors that attach to each cable end. The wires pass through the front for visibility before crimping Used on both ends of the cable Cable Tester Verifies that all eight wires are connected from one end to the other. Lights indicate pass or fail Used after crimping both ends Cat5e/Cat6 Ethernet Cable The actual networking cable containing eight copper wires twisted into pairs Used throughout the entire process The most common wiring standard for Ethernet cable is T568B, which is the most common in U.S. networks. e most common wiring standard for Ethernet cable is T568B, which is the most common in U.S. networks. Pin Wire Color Function (T568B Standard) 1 White/Orange Transmit + (TX+) 2 Orange Transmit \u2013 (TX\u2013) 3 White/Green Receive + (RX+) 4 Blue Not used in 10/100 Mbps; part of Pair 3 in Gigabit Ethernet 5 White/Blue Not used in 10/100 Mbps; part of Pair 3 in Gigabit Ethernet 6 Green Receive \u2013 (RX\u2013) 7 White/Brown Not used in 10/100 Mbps; part of Pair 4 in Gigabit Ethernet 8 Brown Not used in 10/100 Mbps; part of Pair 4 in Gigabit Ethernet Steps for Constructing the Cable Step 1: Prepare the cable Measure approximately 12 inches of cable Use the cutter section of the crimp tool to make a clean, straight cut Use the stripper to remove about 3 inches of the outer plastic jacket Step 2: Untwist and arrange the wires Untwist the four pairs without damaging the wire Arrange them in the T568B order: White/Orange, Orange, White/Green, Blue, WHite/Blue, Green, White/Brown, Brown Straighten the wires until they are flat and parallel Trim them evenly to about 0.5 inches in length using the wire cutter Step 3: Insert the wires into the RJ45 connector Hold the RJ45 connector with the clip facing down and gold pins facing up Slide all eight wires carefully into the connector while maintaining the correct color order Double check that the color order is correct before proceeding Step 4: Crimp the connector Insert the RJ45 connector into the RJ45 crimp tool Squeeze the handles firmly until a click is heard The crimp tool will press the metal contacts into the wires and cut off the excess ends Lightly tug on the connector to confirm that it's secure Step 5: Add 2 cable sleeves to the cable, then repeat steps 1-4 on the other end of the cable Wire Stripping Demonstration Video Testing the Cable Once the cable was constructed, it had to be tested to ensure that it can establish a reliable physical link within a network. To do so, an Ethernet cable testing tool was used. Both ends of the cable were plugged into the device, and each wire (1-8) was tested. Successful connections were indicated by an LED. Reflection The most challenging step in creating my Ethernet cable was feeding the small individual wires into the RJ45 connector, since keeping all eight conductors perfectly straight, aligned in the T568B order, and pushed fully into the connector without any of them slipping out was very difficult and required lots of precision. Maintaining the correct wire order is critical because Ethernet relies on twisted pairs arranged in a specific pattern to reduce interference and ensure signals travel along the proper transmit and receive paths; even a single pair swapped or misaligned can cause slow speeds, failed connections, or complete link loss. Building and testing the cable directly connects to the Physical Layer (Layer 1) of the OSI model, since that layer is responsible for transmitting electrical signals between devices. If a cable were built incorrectly but never tested, a real network could experience intermittent failures, dropped packets, or devices that appear \u201cbroken,\u201d leading to difficult troubleshooting and wasted time. Labeling the cable and using professional tools\u2014like crimpers, strippers, and testers\u2014mirrors real-world industry practices because technicians need to identify cables quickly, ensure reliable terminations, and verify functionality before installation, all of which maintain network organization and uptime in professional environments. Exploring Layers 1 and 2 of the OSI Model (Network Access Layer of TCP/IP Model) In this activity, OSI Layers 1 and 2 were investigated (equivalent to Network Access Layer of the TCP/IP model). OSI Layer 1 moves bits across cables, Wi-Fi, and hardware connections, and Layer 2 uses MAC Addresses and frames to send data across a network. The Ubuntu VM was used to find the MAC address, explore the network interface, and see live network traffic. Layer 1 To reveal important information about the network interface, a core component of Layer 1, the ethtool command was used in Ubuntu. To install it, run sudo apt install ethtool -y . Once ethtool was installed, the network interfaces were listed with ip link show . In the Ubuntu VM, interface enp0s1 was in use, so to reveal its information, sudo ethtool enp0s1 was run. It reports details such as speed, duplex mode, link detected (yes/no), and the driver version. However, in the VM, many data points were listed as \"unknown\" since the NIC is virtualized and don't actually communicate with the network. The virtual NICs relay data to and from the hosts's physical NIC and acts as a bridge that lets the VM send and receive network traffic through the host's real hardware. For this reason, data such as speed and duplex were listed as \"unknown.\" Ethtool reports the Transceiver as \"Internal\" since the VM's NIC is virtual and doesn't actually have a physical transceiver. Layer 2 OSI Layer 2 is responsible for making sure that data travels safely and accurately devices on the same local network. In this layer, data is packaged into frames, smaller packets of data. Frames include the data, the source and destination MAC addresses, and error checking information to detect damage during transmission. When one device wants to communicate with another on the same network, it uses ARP ( A ddress R esolution P rotocol) to match an IP address with the correct MAC address. This information can be found in Ubuntu with the following commands: arp -n \u21d2 lists devices and their MAC addresses known by Ubuntu ip -s link \u21d2 shows how many packets have been sent or received by each interface When a message is sent from one device to another on the same network, the physical layer turns the data into signals and sends them through Ethernet or Wi-Fi, and the data link layer packages the data into frames and tags the frames with MAC addresses so they reach the correct device. To explore Layer 2, ip link show was run, which returned the following information: Interface name: enp0s1 MAC address: 92:9f:78:7c:ae:77o Broadcast Address: ff:ff:ff:ff:ff:ff After obtaining this information, arp -n was run to see IP and MAC addresses of devices that the VM has communicated with on the local network. ARP connects IP Addresses (OSI Layer 3) with MAC Addresses (OSI Layer 2) so that the computer knows where to physically send data. Next, network traffic statistics were revealed with ip -s link , which returned the following information: RX packets (received): 16605 TX packets (transmitted): 2519 Errors:0 Lastly, tcpdump was used to observe data packets as they move across the VM's NIC. Using the command sudo tcpdump -c 5 captures the first five live packets travelling through the VM's NIC, and reports the source and destination MAC addresses as well as the protocols being used. In the tcpdump capture, both the source and destination MAC addresses were revealed, since they identify the physical NICs involved in the data transfer. In the capture, Firefox was opened, and the packets captured are the data that Firefox fetched and received from its servers. This information is important for network monitoring and troubleshooting since it reveals how devices are communicating at OSI Layer 2, making it possible to detect misconfigurations, unexpected traffic, and connectivity failures. Reflection Through the investigation of Layers 1 and 2 of the OSI Model, I learned that the Ubuntu VM uses the network interface enp0s1, and that it reveals itself on the local network using its MAC address. The MAC address can be obtained through using ip link show , and can be monitored using tcpdump . A MAC address is different from an IP address, since a MAC address is permanent and unique to each NIC, whereas an IP address is assigned by a router in Layer 3. ARP links these two addresses together, since it maps each IP address to the correct MAC address, allowing for devices to communicate with each other using IP addresses. When viewing live packets in tcpdump , the capture revealed information about the MAC addresses that send and receive packets. Layer 1 and 2 work together by having Layer 1 convert data into electrical (Ethernet) or radio (Wi-Fi) signals for transmission, while Layer 2 packages that data into frames, assigns MAC addresses, and ensures that the signals traveling across the medium reach the correct device on the local network. Building and Testing a SOHO Network In this activity, a small office / home office (SOHO) networks was designed and simulated with Ubuntu VMs. A SOHO network is the most common network, as it is the kind of network setup most people use at home or in a small business. It connects multiple devices like computers, printers, and IoT devices so they can share internet access, files, and other resources. A typical SOHO network contains the following components: Modem: Connects network to the Internet Service Provider (ISP) Router: Directs (routes) traffic between local devices and the internet; assigns IP addresses Switch: Expands the number of wired ports available for computers, printers, and desktop phones Access Point (AP): Provides Wi-Fi to laptops, tablets, and cell phones Devices: Endpoints like computers, printers, and smart TVs that use the network Designing a SOHO Network A SOHO network was designed with the following devices: 2 Ubuntu computers 1 printer 1 smartphone or tablet 1 router 1 switch 1 access point 1 cloud-connected device (NAS, smart TV, or other IoT device) Additionally, the diagram includes an IP addressing plan using 192.168.50.0/24. This means that each device has an IP of 192.168.50.x, where 0 < x < 99 and the first 24 bits represent the network (meaning the last 8 bits represent the device). Simulating the SOHO and Testing in Ubuntu To test the SOHO, 2 computers running Ubuntu VMs in Bridged Network mode were used (labelled Computer A and Computer B). To start, once the VMs were set to Bridged Mode, ip a was run on both computers. Computer A's IP was 10.12.26.1 and Computer B's IP was 10.12.26.18. Computer A's IP Computer B's IP Once the IP addresses were found, the two computers pinged each other with ping [Other Computer's IP Address] . Pinging Computer B from Computer A Pinging Computer A from Computer B Once the computers could connect to each other, the following commands were run on each computer to explore what's happening inside the network: Command Purpose arp -a Lists devices and the unique MAC address of nearby devices that the computer recognizes on the local network netstat -r Displays how data is being send through the network ifconfig Shows the active network interfaces and IP configuration sudo traceroute google.com Displays every \"hop\" packets take to reach Google's DNS server and helps visualize how data travels through routers across the internet These commands reveal how the SOHO network handles communication over the local network, device identification, routing, and internet connectivity. The arp -a results show that the VM learned the MAC address of the default gateway (192.168.4.1), which reveals that all non-local communication is handled by the router. The netstat -r output shows that the VM's network uses a 192.168.64.0/24 networking plan, where the first 24 bits of the IP (192.168.64) designate the network, and the last 8 bits (0-99) designate the device on the network. The ifconfig output reveals that the VM uses the network adapter enp0s1 with an IP of 192.168.64.7 and has a MAC address of 92:9f:78:7c:ae:77. And, since the flags include \"UP\" and \"RUNNING\", it is known that there is an active network connection on enp0s1. The output from sudo traceroute 8.8.8.8 demonstrates the path data takes from the VM to Google's public DNS server, 8.8.8.8. The first \"hop\" is to the local router, followed by the ISP's internal network, and then national network infrastructure before reaching Google. This demonstrates how the SOHO hands off traffic from the devices to the router, then to the ISP, and eventually national and global internet infrastructure. Overall, these tools showcase how the SOHO network resolves local devices with ARP, directs outbound traffic with a routing table (revealed with netstat -r ), identifies interfaces and addresses (MAC and IP) with ifconfig , and routes packets to the internet through traceroute . Once the information about the SOHO was revealed, the next step in simulating and testing the SOHO was to enable a firewall in Ubuntu. A firewall acts like a security gate at the entrance of the network, as it blocks unwanted connections and allows safe traffic. Firewalls operate at OSI Layers 3 and 4, filtering packets based on rules. To enable ufw, a common firewall used in Ubuntu, the following steps were used: Install ufw with sudo apt install ufw Check ufw's status with sudo ufw status If it says \"inactive,\" turn it on with sudo ufw enable and re-check it with sudo ufw status Next, internet reachability was tested with traceroute . Earlier, basic reachability was tested with traceroute 8.8.8.8 , however, testing traceroute with google.com instead of 8.8.8.8 tests both basic reachability and the DNS. Running traceroute google.com yielded the following result: When compared to traceroute 8.8.8.8 ( pictured below ), running traceroute google.com takes a lot more steps since google.com actually encompasses many different DNS servers, and there is a lot of internal routing within google's many servers once the packets reach google's public server. On the other hand, 8.8.8.8 is one specific server, which yields a relatively simple path for the data to take. Lastly, to demonstrate OSI Layer 7 (Application Layer), a simple web server was created with python on each computer. This demonstrates the Application Layer by showing how one device can serve files to another over HTTP. The following commands were used to create the server: hostname -I : Not necessary to start the server, but useful to run since it prints the hostname of the computer through which the server will be accessible cd ~ : Navigates to the home folder python3 -m http.server 8080 : Starts a simple HTTP server on port 8080 with Python 3 These commands were run on Computer B. On Computer A, 10.12.24.243:8080 was entered in a web browser to access the server, where the contents of Computer B's home folder were accessible over the local network. Reflection The SOHO network investigation demonstrated how the VM\u2019s network interface identifies itself on the local network through its assigned IP address and its unique MAC address, both of which were visible in ifconfig and ip a . A MAC address, which permanently identifies the physical or virtual network interface, differs from an IP address because an IP is assigned by the router and can change based on the network, while the MAC remains constant and operates at Layer 2. ARP plays a crucial role in linking these two addressing systems by mapping each known IP address to the correct MAC address, allowing data to be delivered to the correct device on the local network even when only an IP address is used. The tcpdump capture revealed the source and destination MAC addresses for each packet, along with the protocols in use, showing real-time communication between applications\u2014such as a browser\u2014and remote servers. These observations highlighted how Layers 1 and 2 work together: Layer 1 converts data into electrical or radio signals for transmission across cables or Wi-Fi, while Layer 2 wraps that data into frames, assigns MAC addresses, performs error detection, and ensures the signals reach the correct local device. Together, these layers form the foundation of a functioning SOHO network by enabling reliable physical transmission and accurate local delivery of data. Overall Reflection This project tied together OSI concepts with practical skills by demonstrating how Layer 1 physical media and Layer 2 data-link framing enable Layer 3 IP addressing and higher-layer services in a SOHO network. Building and testing Ethernet cables reinforced the importance of correct pair order (T568B), solid crimps, and cable verification at Layer 1 because physical faults immediately affect link reliability and throughput. Observations with ip , arp , and tcpdump showed how MAC addresses and ARP operate at Layer 2 to map IP addresses at Layer 3, and how NAT/bridged modes alter private addressing while leaving the router's public IP unchanged. Topology choices\u2014such as star for small offices or hybrid layouts for larger deployments\u2014were linked to resilience and troubleshooting complexity, with mesh designs offering redundancy at higher cost. Simulating a SOHO network using virtual machines highlighted real-world trade-offs: bridged VMs behave like distinct hosts on the LAN and expose distinct security considerations, whereas NAT/Shared mode simplifies management but hides device-level behavior. Key challenges included precise cable termination and ensuring virtualized NICs report meaningful Layer 1 metrics, both of which complicate fault isolation during lab exercises and field deployments. Professionally relevant skills gained include systematic testing and documentation of cables and addresses, interpreting interface and routing tables to locate faults, and designing simple IP plans and firewall rules suitable for small networks. Together, these lessons emphasize that reliable networking depends on correct physical construction, consistent addressing and topology choices, and disciplined monitoring\u2014skills that directly transfer to network administration, troubleshooting, and infrastructure planning.","title":"Network Foundations"},{"location":"courses/ap_networking/network_foundations/#networking-foundations-layers-cables-and-connectivity","text":"This project was focused on exploring aspects of a network that make up layers 1 and 2 of the OSI model, such as Ethernet cables, IP addresses, physical layout, and more. These aspects were investigated through various activities.","title":"Networking Foundations: Layers, Cables, and Connectivity"},{"location":"courses/ap_networking/network_foundations/#table-of-contents","text":"Exploring IP Addresses in Shared and Bridged Modes in UTM Network Topology Activity Cable Constructing and Testing Exploring Layers 1 and 2 of the OSI Model (Network Access Layer of TCP/IP Model) Building and Testing a SOHO Network Reflection","title":"Table of Contents"},{"location":"courses/ap_networking/network_foundations/#exploring-ip-addresses-in-shared-and-bridged-modes-in-utm","text":"This activity explored how Shared (NAT) and Bridged network modes in UTM affect a VM's IP address and how it connects to the internet.","title":"Exploring IP Addresses in Shared and Bridged Modes in UTM"},{"location":"courses/ap_networking/network_foundations/#part-1-exploring-shared-nat-mode","text":"Before doing anything, the VM had to be set to Shared (NAT) mode. To do so, the VM's settings were edited in UTM by right clicking the VM, selecting network settings in the left sidebar, and changing the network mode to \"Shared\". Once the VM was set to Shared Network mode, the internal IP address was found with ip a , resulting in the following output: Term Explanation inet IPv4 address (192.168.64.2) /24 Represents the subnet mask (255.255.255.0), which signifies that the first 24 bits (192.168.64) of the IP address designate the network and the remaining 8 bits designate the device on the network brd 192.168.64.255 The broadcast address for the subnet scope global dynamic enp0s1 \u201cGlobal\u201d means this IP can reach outside the VM (through NAT), and \u201cdynamic\u201d means it was assigned automatically (DHCP). Private IP addresses reveal how the device interacts with the LAN, not the internet. To see a device's IP on the internet, otherwise known as a public IP address , a website such as https://whatismyipaddress.com can be used to find it. The VM's public IP was 173.95.44.210. In Shared mode, the public and private IPs were different. 192.168.64.2 belonged to the LAN whereas 173.95.44.210 belonged to the internet. Shared mode might be used by a VM when connecting to the internet because it appears as the same device as the host machine on the LAN and is subject to the same firewall and device restrictions as the host. Shared mode also makes it easier to connect multiple VMs on a single computer since they all share an IP address, making it easier to work with than dealing with individual IPs for each VM.","title":"Part 1: Exploring Shared (NAT) Mode"},{"location":"courses/ap_networking/network_foundations/#part-2-exploring-bridged-mode","text":"After exploring Shared (NAT) Mode in UTM, bridged mode was explored. To switch the VM from Shared to Bridged mode, the VM was shut down, then its network settings were changed. Once this change was made, the VM was started again. Once Ubuntu was loaded, a similar procedure was followed. The IP address was again found with ip a . However, the results were different: Then, the public IP was checked with the same website . While the private IP was completely different, the public IP was exactly the same (173.95.44.210). While the private IP address changed from 192.168.64.2 to 10.1235.30, the public IP address did not change at all. This is because the IP address of the device has no impact on the router's IP address, which is what determines the public IP. In Bridged Mode, the VM appears as an entirely different computer on the LAN, which is very different to Shared Mode. This quality would lead to an organization often choosing Bridged over Shared mode in a corporate environment because corporations may use a VM to test out new software, and while testing, they would want to make sure that the software is in an environment as similar to as a real computer as possible. Shared mode simply does not provide such an environment, whereas Bridged mode is ideal for such an application. However, the fact that Bridged mode behaves like a separate computer poses a security risk, as it could provide an opportunity to bypass network or device restrictions imposed on the host machine.","title":"Part 2: Exploring Bridged Mode"},{"location":"courses/ap_networking/network_foundations/#comparison-table","text":"Mode Private IP Public IP Shared (NAT) 192.168.64.2 173.95.44.210 Bridged 10.12.25.30 173.95.44.210 When comparing the two modes, Shared mode appears as the same device as the host on the network, whereas Bridged mode appears as a separate device. Due to this quality, Shared mode provides a safer, more controlled environment whereas Bridged mode provides an environment akin to a physical computer.","title":"Comparison Table"},{"location":"courses/ap_networking/network_foundations/#reflection","text":"How did your IP addresses change between Shared and Bridged mode? My private IP address changed between Shared and Bridged mode, since it switched from being the same as the Mac\u2019s IP address to being a completely different address on the LAN. The public IP address did not change, since that is controlled by the router, not the individual devices connected to it. What did this experiment teach you about how local and public networks communicate? This experiment taught me about how Public IP addresses are based off of the router, and the IP addresses of endpoint devices on the LAN do not affect the Public IP address. It also showed me how Bridged and Shared mode in UTM are different, since a Bridged VM has an IP separate from the host, whereas a Shared VM has the same IP as the host. Why might IT professionals use different network configurations for home, business, or lab environments? IT professionals might use different network configurations for home, business, or lab environments. In business environments, IT professionals would likely want to keep the amount of IP addresses available to a minimum, so they would use Shared mode. On the other hand, lab environments and home users may use Bridged mode since it would allow them to simulate how a full computer with the virtualized OS would work, since Bridged mode behaves like a separate computer on the network (it has its own MAC and IP address). Which mode do you think is best for classroom use, and why? Shared mode is better for classroom use since it is easier for IT to manage, and it ensures that students can\u2019t bypass the school\u2019s firewall or network restrictions.","title":"Reflection"},{"location":"courses/ap_networking/network_foundations/#network-topology-activity","text":"The next activity was the Network Topology Activity. Network topology is the way that computers, devices, and other pieces of technology are arranged and connected within a network. It\u2019s like a map or blueprint that shows how data travels from one device to another. Every network includes devices that either send, receive, or direct data. The connect and communicate in the following way: - End Devices: These are the computers, smartphones, printers, and other tools people use. They send and receive data. - Networking Devices: These control or manage traffic between devices. - Switch: Connects multiple devices on the same local network (like in a classroom or office). - Router: Connects different networks (like your home network to the internet). - Access Point: Allows wireless devices to connect using Wi-Fi. - Cables or Wireless Connections: The \u201croads\u201d that data travels along. - Examples include Ethernet cables, fiber optic cables, or radio waves (for Wi-Fi). When data travels, it moves in packets \u2014 small chunks of information that include details about where they\u2019re coming from and where they\u2019re going. The topology determines which path those packets take and how efficiently they reach their destination. Example of a network with 3 computers, 1 networking device (router in this case) and 1 printer: Common network topologies include: Star Topography Once central switch or hub in the middle All computers connect to that central point Common in office network and home Wi-Fi routers Bus Topography A single straight line (\"backbone cable\") with all computers branching off. Common in early Ethernet networks (now outdated). Ring Topography Devices form a circle with connections between neighbors Data travels one way (or both in dual-ring) Common in some legacy fiber networks and token ring systems Mesh Topography Every device connects to multiple others If one path breaks, another cna still carry data Common in data centers and wireless mesh networks with many IoT devices Hybrid Topography Combine two or more (for example, multiple Star networks connected in a Bus layout) Common in large organizations with multiple departments or floors","title":"Network Topology Activity"},{"location":"courses/ap_networking/network_foundations/#reflection_1","text":"For a small business, the star topology is the easiest to set up because it only requires connecting each device to a central router, making installation and troubleshooting simple. The mesh topology is the most reliable when a connection fails since it provides multiple paths for data to travel, allowing the network to keep functioning even if one link goes down. However, that same redundancy makes mesh the most expensive to implement due to the large number of cables and connections required. A school would most likely use a star or hybrid topology\u2014star within classrooms and a hybrid structure between floors or buildings\u2014because it offers a balance of scalability, cost-efficiency, and control. Overall, the physical layout of a network topology directly affects speed and reliability: topologies with shared paths (like bus or ring) can create bottlenecks or single points of failure, while layouts with dedicated or redundant paths (like star and mesh) provide faster performance and greater resilience due to the multiple pathways for data to travel across.","title":"Reflection:"},{"location":"courses/ap_networking/network_foundations/#cable-constructing-and-testing","text":"After learning about network topography, the next activity was building and testing cables. Every network, from a classroom computer lab to a corporate data center - relies on Ethernet cables to carry data between devices. Professional technicians and network engineers often make and test their own cables instead of buying them pre-made, allowing them to create cables at precise lengths, verify quality, and save on cost. Building and testing cables works at the Physical Layer (Layer 1) of the OSI Model. This is where data is converted to electrical signals that travel through wires. At this layer: Cable quality determines speed and signal reliability Misplaced wires or bad crimps can cause communication failures Proper testing ensures that the cable works before being installed in a network When constructing and testing the Ethernet cables, the following equipment was used: Tool Purpose When It's Used Cable Stripper Removes the outer jacket from the Ethernet cable without damaging the internal wires. Used to expose the eight colored wires within the cable Cable Cutter Cuts the cable cleanly and evenly before and after stripping Used at the start and end for making precise cuts RJ45 Crimp Tool Presses the metal pins inside the RJ45 connector into each wire, securing them and creating an electrical connection Used after the wires are arranged an inserted Pass-Through RJ45 Connectors Plastic connectors that attach to each cable end. The wires pass through the front for visibility before crimping Used on both ends of the cable Cable Tester Verifies that all eight wires are connected from one end to the other. Lights indicate pass or fail Used after crimping both ends Cat5e/Cat6 Ethernet Cable The actual networking cable containing eight copper wires twisted into pairs Used throughout the entire process The most common wiring standard for Ethernet cable is T568B, which is the most common in U.S. networks. e most common wiring standard for Ethernet cable is T568B, which is the most common in U.S. networks. Pin Wire Color Function (T568B Standard) 1 White/Orange Transmit + (TX+) 2 Orange Transmit \u2013 (TX\u2013) 3 White/Green Receive + (RX+) 4 Blue Not used in 10/100 Mbps; part of Pair 3 in Gigabit Ethernet 5 White/Blue Not used in 10/100 Mbps; part of Pair 3 in Gigabit Ethernet 6 Green Receive \u2013 (RX\u2013) 7 White/Brown Not used in 10/100 Mbps; part of Pair 4 in Gigabit Ethernet 8 Brown Not used in 10/100 Mbps; part of Pair 4 in Gigabit Ethernet","title":"Cable Constructing and Testing"},{"location":"courses/ap_networking/network_foundations/#steps-for-constructing-the-cable","text":"Step 1: Prepare the cable Measure approximately 12 inches of cable Use the cutter section of the crimp tool to make a clean, straight cut Use the stripper to remove about 3 inches of the outer plastic jacket Step 2: Untwist and arrange the wires Untwist the four pairs without damaging the wire Arrange them in the T568B order: White/Orange, Orange, White/Green, Blue, WHite/Blue, Green, White/Brown, Brown Straighten the wires until they are flat and parallel Trim them evenly to about 0.5 inches in length using the wire cutter Step 3: Insert the wires into the RJ45 connector Hold the RJ45 connector with the clip facing down and gold pins facing up Slide all eight wires carefully into the connector while maintaining the correct color order Double check that the color order is correct before proceeding Step 4: Crimp the connector Insert the RJ45 connector into the RJ45 crimp tool Squeeze the handles firmly until a click is heard The crimp tool will press the metal contacts into the wires and cut off the excess ends Lightly tug on the connector to confirm that it's secure Step 5: Add 2 cable sleeves to the cable, then repeat steps 1-4 on the other end of the cable Wire Stripping Demonstration Video","title":"Steps for Constructing the Cable"},{"location":"courses/ap_networking/network_foundations/#testing-the-cable","text":"Once the cable was constructed, it had to be tested to ensure that it can establish a reliable physical link within a network. To do so, an Ethernet cable testing tool was used. Both ends of the cable were plugged into the device, and each wire (1-8) was tested. Successful connections were indicated by an LED.","title":"Testing the Cable"},{"location":"courses/ap_networking/network_foundations/#reflection_2","text":"The most challenging step in creating my Ethernet cable was feeding the small individual wires into the RJ45 connector, since keeping all eight conductors perfectly straight, aligned in the T568B order, and pushed fully into the connector without any of them slipping out was very difficult and required lots of precision. Maintaining the correct wire order is critical because Ethernet relies on twisted pairs arranged in a specific pattern to reduce interference and ensure signals travel along the proper transmit and receive paths; even a single pair swapped or misaligned can cause slow speeds, failed connections, or complete link loss. Building and testing the cable directly connects to the Physical Layer (Layer 1) of the OSI model, since that layer is responsible for transmitting electrical signals between devices. If a cable were built incorrectly but never tested, a real network could experience intermittent failures, dropped packets, or devices that appear \u201cbroken,\u201d leading to difficult troubleshooting and wasted time. Labeling the cable and using professional tools\u2014like crimpers, strippers, and testers\u2014mirrors real-world industry practices because technicians need to identify cables quickly, ensure reliable terminations, and verify functionality before installation, all of which maintain network organization and uptime in professional environments.","title":"Reflection"},{"location":"courses/ap_networking/network_foundations/#exploring-layers-1-and-2-of-the-osi-model-network-access-layer-of-tcpip-model","text":"In this activity, OSI Layers 1 and 2 were investigated (equivalent to Network Access Layer of the TCP/IP model). OSI Layer 1 moves bits across cables, Wi-Fi, and hardware connections, and Layer 2 uses MAC Addresses and frames to send data across a network. The Ubuntu VM was used to find the MAC address, explore the network interface, and see live network traffic.","title":"Exploring Layers 1 and 2 of the OSI Model (Network Access Layer of TCP/IP Model)"},{"location":"courses/ap_networking/network_foundations/#layer-1","text":"To reveal important information about the network interface, a core component of Layer 1, the ethtool command was used in Ubuntu. To install it, run sudo apt install ethtool -y . Once ethtool was installed, the network interfaces were listed with ip link show . In the Ubuntu VM, interface enp0s1 was in use, so to reveal its information, sudo ethtool enp0s1 was run. It reports details such as speed, duplex mode, link detected (yes/no), and the driver version. However, in the VM, many data points were listed as \"unknown\" since the NIC is virtualized and don't actually communicate with the network. The virtual NICs relay data to and from the hosts's physical NIC and acts as a bridge that lets the VM send and receive network traffic through the host's real hardware. For this reason, data such as speed and duplex were listed as \"unknown.\" Ethtool reports the Transceiver as \"Internal\" since the VM's NIC is virtual and doesn't actually have a physical transceiver.","title":"Layer 1"},{"location":"courses/ap_networking/network_foundations/#layer-2","text":"OSI Layer 2 is responsible for making sure that data travels safely and accurately devices on the same local network. In this layer, data is packaged into frames, smaller packets of data. Frames include the data, the source and destination MAC addresses, and error checking information to detect damage during transmission. When one device wants to communicate with another on the same network, it uses ARP ( A ddress R esolution P rotocol) to match an IP address with the correct MAC address. This information can be found in Ubuntu with the following commands: arp -n \u21d2 lists devices and their MAC addresses known by Ubuntu ip -s link \u21d2 shows how many packets have been sent or received by each interface When a message is sent from one device to another on the same network, the physical layer turns the data into signals and sends them through Ethernet or Wi-Fi, and the data link layer packages the data into frames and tags the frames with MAC addresses so they reach the correct device. To explore Layer 2, ip link show was run, which returned the following information: Interface name: enp0s1 MAC address: 92:9f:78:7c:ae:77o Broadcast Address: ff:ff:ff:ff:ff:ff After obtaining this information, arp -n was run to see IP and MAC addresses of devices that the VM has communicated with on the local network. ARP connects IP Addresses (OSI Layer 3) with MAC Addresses (OSI Layer 2) so that the computer knows where to physically send data. Next, network traffic statistics were revealed with ip -s link , which returned the following information: RX packets (received): 16605 TX packets (transmitted): 2519 Errors:0 Lastly, tcpdump was used to observe data packets as they move across the VM's NIC. Using the command sudo tcpdump -c 5 captures the first five live packets travelling through the VM's NIC, and reports the source and destination MAC addresses as well as the protocols being used. In the tcpdump capture, both the source and destination MAC addresses were revealed, since they identify the physical NICs involved in the data transfer. In the capture, Firefox was opened, and the packets captured are the data that Firefox fetched and received from its servers. This information is important for network monitoring and troubleshooting since it reveals how devices are communicating at OSI Layer 2, making it possible to detect misconfigurations, unexpected traffic, and connectivity failures.","title":"Layer 2"},{"location":"courses/ap_networking/network_foundations/#reflection_3","text":"Through the investigation of Layers 1 and 2 of the OSI Model, I learned that the Ubuntu VM uses the network interface enp0s1, and that it reveals itself on the local network using its MAC address. The MAC address can be obtained through using ip link show , and can be monitored using tcpdump . A MAC address is different from an IP address, since a MAC address is permanent and unique to each NIC, whereas an IP address is assigned by a router in Layer 3. ARP links these two addresses together, since it maps each IP address to the correct MAC address, allowing for devices to communicate with each other using IP addresses. When viewing live packets in tcpdump , the capture revealed information about the MAC addresses that send and receive packets. Layer 1 and 2 work together by having Layer 1 convert data into electrical (Ethernet) or radio (Wi-Fi) signals for transmission, while Layer 2 packages that data into frames, assigns MAC addresses, and ensures that the signals traveling across the medium reach the correct device on the local network.","title":"Reflection"},{"location":"courses/ap_networking/network_foundations/#building-and-testing-a-soho-network","text":"In this activity, a small office / home office (SOHO) networks was designed and simulated with Ubuntu VMs. A SOHO network is the most common network, as it is the kind of network setup most people use at home or in a small business. It connects multiple devices like computers, printers, and IoT devices so they can share internet access, files, and other resources. A typical SOHO network contains the following components: Modem: Connects network to the Internet Service Provider (ISP) Router: Directs (routes) traffic between local devices and the internet; assigns IP addresses Switch: Expands the number of wired ports available for computers, printers, and desktop phones Access Point (AP): Provides Wi-Fi to laptops, tablets, and cell phones Devices: Endpoints like computers, printers, and smart TVs that use the network","title":"Building and Testing a SOHO Network"},{"location":"courses/ap_networking/network_foundations/#designing-a-soho-network","text":"A SOHO network was designed with the following devices: 2 Ubuntu computers 1 printer 1 smartphone or tablet 1 router 1 switch 1 access point 1 cloud-connected device (NAS, smart TV, or other IoT device) Additionally, the diagram includes an IP addressing plan using 192.168.50.0/24. This means that each device has an IP of 192.168.50.x, where 0 < x < 99 and the first 24 bits represent the network (meaning the last 8 bits represent the device).","title":"Designing a SOHO Network"},{"location":"courses/ap_networking/network_foundations/#simulating-the-soho-and-testing-in-ubuntu","text":"To test the SOHO, 2 computers running Ubuntu VMs in Bridged Network mode were used (labelled Computer A and Computer B). To start, once the VMs were set to Bridged Mode, ip a was run on both computers. Computer A's IP was 10.12.26.1 and Computer B's IP was 10.12.26.18. Computer A's IP Computer B's IP Once the IP addresses were found, the two computers pinged each other with ping [Other Computer's IP Address] . Pinging Computer B from Computer A Pinging Computer A from Computer B Once the computers could connect to each other, the following commands were run on each computer to explore what's happening inside the network: Command Purpose arp -a Lists devices and the unique MAC address of nearby devices that the computer recognizes on the local network netstat -r Displays how data is being send through the network ifconfig Shows the active network interfaces and IP configuration sudo traceroute google.com Displays every \"hop\" packets take to reach Google's DNS server and helps visualize how data travels through routers across the internet These commands reveal how the SOHO network handles communication over the local network, device identification, routing, and internet connectivity. The arp -a results show that the VM learned the MAC address of the default gateway (192.168.4.1), which reveals that all non-local communication is handled by the router. The netstat -r output shows that the VM's network uses a 192.168.64.0/24 networking plan, where the first 24 bits of the IP (192.168.64) designate the network, and the last 8 bits (0-99) designate the device on the network. The ifconfig output reveals that the VM uses the network adapter enp0s1 with an IP of 192.168.64.7 and has a MAC address of 92:9f:78:7c:ae:77. And, since the flags include \"UP\" and \"RUNNING\", it is known that there is an active network connection on enp0s1. The output from sudo traceroute 8.8.8.8 demonstrates the path data takes from the VM to Google's public DNS server, 8.8.8.8. The first \"hop\" is to the local router, followed by the ISP's internal network, and then national network infrastructure before reaching Google. This demonstrates how the SOHO hands off traffic from the devices to the router, then to the ISP, and eventually national and global internet infrastructure. Overall, these tools showcase how the SOHO network resolves local devices with ARP, directs outbound traffic with a routing table (revealed with netstat -r ), identifies interfaces and addresses (MAC and IP) with ifconfig , and routes packets to the internet through traceroute . Once the information about the SOHO was revealed, the next step in simulating and testing the SOHO was to enable a firewall in Ubuntu. A firewall acts like a security gate at the entrance of the network, as it blocks unwanted connections and allows safe traffic. Firewalls operate at OSI Layers 3 and 4, filtering packets based on rules. To enable ufw, a common firewall used in Ubuntu, the following steps were used: Install ufw with sudo apt install ufw Check ufw's status with sudo ufw status If it says \"inactive,\" turn it on with sudo ufw enable and re-check it with sudo ufw status Next, internet reachability was tested with traceroute . Earlier, basic reachability was tested with traceroute 8.8.8.8 , however, testing traceroute with google.com instead of 8.8.8.8 tests both basic reachability and the DNS. Running traceroute google.com yielded the following result: When compared to traceroute 8.8.8.8 ( pictured below ), running traceroute google.com takes a lot more steps since google.com actually encompasses many different DNS servers, and there is a lot of internal routing within google's many servers once the packets reach google's public server. On the other hand, 8.8.8.8 is one specific server, which yields a relatively simple path for the data to take. Lastly, to demonstrate OSI Layer 7 (Application Layer), a simple web server was created with python on each computer. This demonstrates the Application Layer by showing how one device can serve files to another over HTTP. The following commands were used to create the server: hostname -I : Not necessary to start the server, but useful to run since it prints the hostname of the computer through which the server will be accessible cd ~ : Navigates to the home folder python3 -m http.server 8080 : Starts a simple HTTP server on port 8080 with Python 3 These commands were run on Computer B. On Computer A, 10.12.24.243:8080 was entered in a web browser to access the server, where the contents of Computer B's home folder were accessible over the local network.","title":"Simulating the SOHO and Testing in Ubuntu"},{"location":"courses/ap_networking/network_foundations/#reflection_4","text":"The SOHO network investigation demonstrated how the VM\u2019s network interface identifies itself on the local network through its assigned IP address and its unique MAC address, both of which were visible in ifconfig and ip a . A MAC address, which permanently identifies the physical or virtual network interface, differs from an IP address because an IP is assigned by the router and can change based on the network, while the MAC remains constant and operates at Layer 2. ARP plays a crucial role in linking these two addressing systems by mapping each known IP address to the correct MAC address, allowing data to be delivered to the correct device on the local network even when only an IP address is used. The tcpdump capture revealed the source and destination MAC addresses for each packet, along with the protocols in use, showing real-time communication between applications\u2014such as a browser\u2014and remote servers. These observations highlighted how Layers 1 and 2 work together: Layer 1 converts data into electrical or radio signals for transmission across cables or Wi-Fi, while Layer 2 wraps that data into frames, assigns MAC addresses, performs error detection, and ensures the signals reach the correct local device. Together, these layers form the foundation of a functioning SOHO network by enabling reliable physical transmission and accurate local delivery of data.","title":"Reflection"},{"location":"courses/ap_networking/network_foundations/#overall-reflection","text":"This project tied together OSI concepts with practical skills by demonstrating how Layer 1 physical media and Layer 2 data-link framing enable Layer 3 IP addressing and higher-layer services in a SOHO network. Building and testing Ethernet cables reinforced the importance of correct pair order (T568B), solid crimps, and cable verification at Layer 1 because physical faults immediately affect link reliability and throughput. Observations with ip , arp , and tcpdump showed how MAC addresses and ARP operate at Layer 2 to map IP addresses at Layer 3, and how NAT/bridged modes alter private addressing while leaving the router's public IP unchanged. Topology choices\u2014such as star for small offices or hybrid layouts for larger deployments\u2014were linked to resilience and troubleshooting complexity, with mesh designs offering redundancy at higher cost. Simulating a SOHO network using virtual machines highlighted real-world trade-offs: bridged VMs behave like distinct hosts on the LAN and expose distinct security considerations, whereas NAT/Shared mode simplifies management but hides device-level behavior. Key challenges included precise cable termination and ensuring virtualized NICs report meaningful Layer 1 metrics, both of which complicate fault isolation during lab exercises and field deployments. Professionally relevant skills gained include systematic testing and documentation of cables and addresses, interpreting interface and routing tables to locate faults, and designing simple IP plans and firewall rules suitable for small networks. Together, these lessons emphasize that reliable networking depends on correct physical construction, consistent addressing and topology choices, and disciplined monitoring\u2014skills that directly transfer to network administration, troubleshooting, and infrastructure planning.","title":"Overall Reflection"},{"location":"courses/ap_networking/networking_data_movement/","text":"Data Movement and Types of Networks Project Introduction This project was focused on understanding how a LAN works, how data travels between devices on a LAN, and introducing binary, decimal, and hexadecimal numbers and their uses in networking applications. Planning and Design Technical Development Testing and Evaluation Reflection Planning and Design The main objectives for this project were to explore how Local Area Networks (LANs) enable communication between devices that are in the same environment (house, school, building, etc.), showcase how IP and MAC addresses are used to identify devices (as well as learning about their differences), and to learn to use ICMP tools to test connections and find details about devices on the LAN. Components of a LAN and Representing them as Parts of a City Component Purpose Analogy Endpoint Devices (Computers, Printers, Phones, etc.) Endpoint Devices are devices that send or receive data across a network. They are the ones that use the network. Houses in the city who communicate with other houses Ethernet Cables and Wi-Fi The physical (Ethernet) or wireless (Wi-Fi) connections that allow data to move between devices on a network. Roads that interconnect everything Switch A device that connects multiple endpoint devices within the same network and directs Roundabout that guides traffic and ensures that everyone goes to the right location Router Post office that decides which mail should stay local (within LAN) and should be mailed to other cities (Internet) Data The letters and packages that people mail to each other within the city and outside the city Important LAN Terms Term Definition LAN (Local Area Network) A network that connects endpoint devices within 1 building/campus and lets them share printers, files, and internet connections efficiently over the LAN Host Any device (computer, printer, phone, etc.) that can send or receive data on the network. Each host has a unique IP address assigned by the router. Switch A device that connects multiple devices on the LAN. It works by learning the MAC address (hardware identifier unique to each NIC) of each device plugged in. When data comes in, the switch forwards it exclusively to the intended device. Router A device that connects the LAN to other networks and the Internet. It uses IP addresses to send incoming data to the intended device. Packet Segmented data that contains the sender's IP (return address), receiver's IP (destination), and the \"payload\" (the actual message/data). When packets arrive, the endpoint device reassembles it into complete data. IP Address Unique identifier for endpoint devices on a network. Different IP address ranges mean different things (192.168.x.x and 10.x.x.x vs 169.254.x.x) Diagram of My House's LAN In my house, almost everything is connected via Wi-Fi, except for the switch and Home Assistant Server that handles all of the smart home tasks in my house. Everything is directly connected to the router, which communicates with the internet and cloud services. OSI Model - The Seven Layers of Networking Networks are often represented by the OSI model, which consists of seven categories that encompass all networking-related tasks. They are: Layer Name Description Analogy 7 Application An email client, web browser, or any internet-connected application (Spotify, Adobe Creative Cloud, Safari, etc. Human-computer interaction layer. 6 Presentation Translates data (encryption, compression) Ensures data is in a usable form. 5 Session Manages the connection Maintains connections and is responsible for controlling parts and sessions. 4 Transport Breaks data into segments Transmits data using transmission protocols such as TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) 3 Network Routes packages using IP addresses Decides which physical path the data will take. 2 Data Link Transfers frames via MAC (media access control) addresses Defines the format of data on the network. 1 Physical Wires, signals, routers, switches, Wi-Fi Transmits raw bit streams over the physical medium. OSI Cards Diagram The following images showcase an activity about understanding how real-world examples are categorized into the OSI layers: First Attempt First attempt at matching OSI cards with example and ordering them correctly. Correct Organization Correct organization of OSI cards with their examples. OSI vs TCP/IP Model TCP/IP Layer Corresponding OSI Layers Functions Application 7 (Application), 6 (Presentation), 5 (Session) Apps, HTTP, FTP Transport 4 (Transport) TCP/UDP, data segmentation Internet 3 (Network) IP addressing, routing Network Access 2 (Data Link), 1 (Physical) Physical and Data Link Representing Numbers with Different Bases To demonstrate how number bases work, cards representing base-10 (decimal), base-2 (binary), and base-5 were provided. The task was to represent different numerical values with the different bases. Base 10 501 = 5 * 10 2 + 0 * 10 1 + 1 * 10 0 = 500 + 0 + 1 = 401 473 = 4 * 10 2 + 7 * 10 1 + 3 * 10 0 = 400 + 70 + 3 = 473 324 = 3 * 10 2 + 2 * 10 1 + 4 * 10 0 = 300 + 20 + 4 = 324 Base 5 84 (Decimal) = 3 * 5 2 + 1 * 5 1 + 4 * 5 0 = 300 + 50 + 4 = 314 (Base-5) = 84 (Base-10) 37 (Decimal) = 1 * 5\u00b2 + 2 * 5\u00b9 + 2 * 5\u2070 = 25 + 10 + 2 = 122 (Base-5) = 37 (Base-10) Technical Development In this section of the project, an Ubuntu VM was used to apply and verify networking concepts in the CLI. Using Ubuntu to Convert Between Binary and Decimal Converting between binary and decimal by hand is very difficult and tedious. Thankfully, the bc CLI tool can be used to perform the conversion. To convert from binary to decimal, the command echo \"obase=10; ibase=2; [Insert Binary Number]\" | bc and echo \"obase=2; 45\" | bc can be used to convert from binary to decimal and from decimal to binary, respectively. Below is a breakdown of these commands: Part Meaning echo Prints text \"obase=10; ibase=2; 101101\" The conversion formula \u2014 it tells the calculator what number base to use. obase \u201cOutput base\u201d \u2014 the numbering system you want the result in. ibase \u201cInput base\u201d \u2014 the numbering system your starting number is in. bc The basic calculator program built into Ubuntu \u2014 it performs the conversion. Below is what these commands look like in the Ubuntu CLI: Representing Network Information as Hexadecimal and Binary Although decimal is easily readable by humans, computers process information in binary. Any time a computer works with decimal numbers, it first has to convert it to binary in order to do any operations with it. To see Ubuntu's IP address in binary, ipcalc must be installed with sudo apt install ipcalc -y . After installing it, run ipcalc [IP address] to display network information in binary. Since hexadecimal can represent up to the number 15 in each character, it can store more information in less space when compared with decimal and especially compared to binary. To view network information in hexadecimal form in Ubuntu, the command printf '%02X%02X%02X%02X\\n' 192 168 64 2 , where 192, 168, 64, and 2 should be replaced with the digits from your IP address (found with ifconfig ). The output was C0A84002. Breaking this down: C0 = 192 A8 = 168 40 = 64 02 = 2 MAC addresses are also written in hexadecimal form; each pair of characters represents a byte of the device's hardware address. To find the MAC address in Ubuntu, run ip addr show . Exploring LAN Components through Ubuntu Information about the LAN can be identified in Ubuntu through CLI commands such as ifconfig , ping , and traceroute . These three commands are essential for finding information about the LAN as well as troubleshooting. What Each Command Does Command Purpose Example Output ifconfig Shows a computer's IP address on the network inet 192.168.1.202 ping Tests if another computer or website is reachable within the LAN or over the internet 64 bytes from google.com ... traceroute Shows every step data takes to reach a destination 1 192.168.54.1 \u21d2 2 10.0.0.1 \u21d2 3 142.250.190.78, etc. Before running any of these commands, they had to be installed on the computer with sudo apt install net-tools -y to install ifconfig , sudo apt install iputils-ping -y to install ping , and sudo apt install traceroute -y to install traceroute . ifconfig is mainly used to display information relating how a computer connects to the LAN. It outputs information such as inet , the IP address assigned to the computer, netmask , which defines the portion of the IP address identifies the network and device, and broadcast , which is used to send messages to all devices on the same LAN. In the image above, inet, netmask, and broadcast are all outlined in red. Inet is the IP address of the computer (192.168.64.2). Netmask (255.255.255.0) defines which part of the IP address identifies the network, and which part identifies the device. When translated to binary, the netmask is 11111111.11111111.11111111.00000000. Each 1 means \"network\", and each 0 means \"host\", meaning that 192.168.64 identifies the network, and 2 identifies the device on the network. The range of valid IP addresses in the network would be anywhere from 192.168.64.1 - 192.168.64.254 (since 192.168.64.255 is the broadcast). ping is mainly used to test internet connection and DNS. Pinging a public DNS server, such as 8.8.8.8 (Google's public DNS server) and 1.1.1.1 (Cloudflare's public DNS server), is a simple way to ensure that a computer can reach the internet. Once internet connection is verified, pinging a domain such as google.com or cloudflare.com can test the DNS. If 8.8.8.8 is reachable but google.com isn't, that means that there is an issue with the DNS. To run these commands in Ubuntu and macOS, run ping -c 4 8.8.8.8 then ping google.com . In the image below, a ping was sent to google.com and other domains, but they didn't respond. So, the DNS was reset with sudo resolvectl flush-caches , which made google.com and cloudlfare.com reachable. After each ping, the system prints how long the data took to travel to the domain, which is a good indicator of internet speed. traceroute is used to show the route data takes to reach a destination. To test it with google.com (any domain will work), run traceroute google.com . This command will return a list of \"hops\" that show the path the data takes across routers to reach Google's server. When it was tested, the data took 23 \"hops\" and 55.121 ms to reach google.com. The data from traceroute can be confusing. Below is a table explaining what each part of the output means: Part What it Means Example Hop # The number on the left that shows how many \"steps\" away that device is from the computer 1, 2, 3, ... Hostname/IP Address The name or IP of the router or device that handled the packet 10.12.16.1, 209.85.244.81, etc. ms (milliseconds) The round-trip time (how long it took to reach that hop and come back) 209.85.244.81: 20.175 ms * Stars represent a connection to a router or device that refuses to return information * * * In the example above: Hop 1: *_gateway (192.168.64.1) is the local network gateway (what connects the LAN to the internet) Hop 2-10: Local or ISP routers Hop 11-20: Internet backbone routers (large, powerful routers moving data across regions or countries) Hop 23: Google's server (64.233.176.113) Roadmap Representing OSI Layers **Reflection: This roadmap represents a data packet as a plane flying from an airport to another airport. It represents the different OSI layers as aspects of an airport, such as the runway being the physical layer. I think that the Transport layer is the most important since it ensures that everything arrives safely and in order. This project helped me understand how the TCP/IP model is a simplified version of the OSI model but both aim to describe how data is transferred. Testing and Evaluation Throughout the activity, many labs were performed involving many different commands. Below, a table summarizes all of the commands used and their purpose. Concept Test Performed Verification Result Network Path Mapping traceroute google.com Successfully traced the route from the local router to Google\u2019s servers, totaling 23 hops in roughly 3 seconds. Data Exchange netcat between virtual machines Verified reliable message delivery over TCP between two VMs. Binary and Decimal Conversion bc command Confirmed accurate conversions between binary and decimal formats. Hexadecimal Output printf command Correctly formatted and displayed IP address as hexadecimal (e.g., C0A84002 ). LAN Interface Details ifconfig command Displayed inet , netmask , and broadcast parameters for network interfaces. Address and Range Calculation ipcalc Produced valid network, broadcast, and usable host address ranges. Open Port Detection netstat Revealed listening ports and established TCP connections between devices. Internet Reachability Test ping google.com and ping 8.8.8.8 Reported full packet reception with ~22 ms average response time. Reflection By working through the various labs exploring data movement, I learned how exactly data is transferred between devices, both over a LAN and through the internet. Learning about the OSI and TCP/IP models helped simplify and understand how data communication works. Designing a roadmap to visualize data transfer helped me understand OSI layers even better. Additionally, converting between binary, decimal, and hexadecimal helped me understand the different number systems and showed the different advantages of each system. By using ipcalc , I learned how IP addresses are represented in binary. Using ping and traceroute demonstrated the path that data takes to transfer from one system to another and demonstrated a real world application of the OSI layers.","title":"Networking and Data Movement"},{"location":"courses/ap_networking/networking_data_movement/#data-movement-and-types-of-networks","text":"","title":"Data Movement and Types of Networks"},{"location":"courses/ap_networking/networking_data_movement/#project-introduction","text":"This project was focused on understanding how a LAN works, how data travels between devices on a LAN, and introducing binary, decimal, and hexadecimal numbers and their uses in networking applications. Planning and Design Technical Development Testing and Evaluation Reflection","title":"Project Introduction"},{"location":"courses/ap_networking/networking_data_movement/#planning-and-design","text":"The main objectives for this project were to explore how Local Area Networks (LANs) enable communication between devices that are in the same environment (house, school, building, etc.), showcase how IP and MAC addresses are used to identify devices (as well as learning about their differences), and to learn to use ICMP tools to test connections and find details about devices on the LAN.","title":"Planning and Design"},{"location":"courses/ap_networking/networking_data_movement/#components-of-a-lan-and-representing-them-as-parts-of-a-city","text":"Component Purpose Analogy Endpoint Devices (Computers, Printers, Phones, etc.) Endpoint Devices are devices that send or receive data across a network. They are the ones that use the network. Houses in the city who communicate with other houses Ethernet Cables and Wi-Fi The physical (Ethernet) or wireless (Wi-Fi) connections that allow data to move between devices on a network. Roads that interconnect everything Switch A device that connects multiple endpoint devices within the same network and directs Roundabout that guides traffic and ensures that everyone goes to the right location Router Post office that decides which mail should stay local (within LAN) and should be mailed to other cities (Internet) Data The letters and packages that people mail to each other within the city and outside the city","title":"Components of a LAN and Representing them as Parts of a City"},{"location":"courses/ap_networking/networking_data_movement/#important-lan-terms","text":"Term Definition LAN (Local Area Network) A network that connects endpoint devices within 1 building/campus and lets them share printers, files, and internet connections efficiently over the LAN Host Any device (computer, printer, phone, etc.) that can send or receive data on the network. Each host has a unique IP address assigned by the router. Switch A device that connects multiple devices on the LAN. It works by learning the MAC address (hardware identifier unique to each NIC) of each device plugged in. When data comes in, the switch forwards it exclusively to the intended device. Router A device that connects the LAN to other networks and the Internet. It uses IP addresses to send incoming data to the intended device. Packet Segmented data that contains the sender's IP (return address), receiver's IP (destination), and the \"payload\" (the actual message/data). When packets arrive, the endpoint device reassembles it into complete data. IP Address Unique identifier for endpoint devices on a network. Different IP address ranges mean different things (192.168.x.x and 10.x.x.x vs 169.254.x.x)","title":"Important LAN Terms"},{"location":"courses/ap_networking/networking_data_movement/#diagram-of-my-houses-lan","text":"In my house, almost everything is connected via Wi-Fi, except for the switch and Home Assistant Server that handles all of the smart home tasks in my house. Everything is directly connected to the router, which communicates with the internet and cloud services.","title":"Diagram of My House's LAN"},{"location":"courses/ap_networking/networking_data_movement/#osi-model-the-seven-layers-of-networking","text":"Networks are often represented by the OSI model, which consists of seven categories that encompass all networking-related tasks. They are: Layer Name Description Analogy 7 Application An email client, web browser, or any internet-connected application (Spotify, Adobe Creative Cloud, Safari, etc. Human-computer interaction layer. 6 Presentation Translates data (encryption, compression) Ensures data is in a usable form. 5 Session Manages the connection Maintains connections and is responsible for controlling parts and sessions. 4 Transport Breaks data into segments Transmits data using transmission protocols such as TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) 3 Network Routes packages using IP addresses Decides which physical path the data will take. 2 Data Link Transfers frames via MAC (media access control) addresses Defines the format of data on the network. 1 Physical Wires, signals, routers, switches, Wi-Fi Transmits raw bit streams over the physical medium.","title":"OSI Model - The Seven Layers of Networking"},{"location":"courses/ap_networking/networking_data_movement/#osi-cards-diagram","text":"The following images showcase an activity about understanding how real-world examples are categorized into the OSI layers:","title":"OSI Cards Diagram"},{"location":"courses/ap_networking/networking_data_movement/#first-attempt","text":"First attempt at matching OSI cards with example and ordering them correctly.","title":"First Attempt"},{"location":"courses/ap_networking/networking_data_movement/#correct-organization","text":"Correct organization of OSI cards with their examples.","title":"Correct Organization"},{"location":"courses/ap_networking/networking_data_movement/#osi-vs-tcpip-model","text":"TCP/IP Layer Corresponding OSI Layers Functions Application 7 (Application), 6 (Presentation), 5 (Session) Apps, HTTP, FTP Transport 4 (Transport) TCP/UDP, data segmentation Internet 3 (Network) IP addressing, routing Network Access 2 (Data Link), 1 (Physical) Physical and Data Link","title":"OSI vs TCP/IP Model"},{"location":"courses/ap_networking/networking_data_movement/#representing-numbers-with-different-bases","text":"To demonstrate how number bases work, cards representing base-10 (decimal), base-2 (binary), and base-5 were provided. The task was to represent different numerical values with the different bases.","title":"Representing Numbers with Different Bases"},{"location":"courses/ap_networking/networking_data_movement/#base-10","text":"501 = 5 * 10 2 + 0 * 10 1 + 1 * 10 0 = 500 + 0 + 1 = 401 473 = 4 * 10 2 + 7 * 10 1 + 3 * 10 0 = 400 + 70 + 3 = 473 324 = 3 * 10 2 + 2 * 10 1 + 4 * 10 0 = 300 + 20 + 4 = 324","title":"Base 10"},{"location":"courses/ap_networking/networking_data_movement/#base-5","text":"84 (Decimal) = 3 * 5 2 + 1 * 5 1 + 4 * 5 0 = 300 + 50 + 4 = 314 (Base-5) = 84 (Base-10) 37 (Decimal) = 1 * 5\u00b2 + 2 * 5\u00b9 + 2 * 5\u2070 = 25 + 10 + 2 = 122 (Base-5) = 37 (Base-10)","title":"Base 5"},{"location":"courses/ap_networking/networking_data_movement/#technical-development","text":"In this section of the project, an Ubuntu VM was used to apply and verify networking concepts in the CLI.","title":"Technical Development"},{"location":"courses/ap_networking/networking_data_movement/#using-ubuntu-to-convert-between-binary-and-decimal","text":"Converting between binary and decimal by hand is very difficult and tedious. Thankfully, the bc CLI tool can be used to perform the conversion. To convert from binary to decimal, the command echo \"obase=10; ibase=2; [Insert Binary Number]\" | bc and echo \"obase=2; 45\" | bc can be used to convert from binary to decimal and from decimal to binary, respectively. Below is a breakdown of these commands: Part Meaning echo Prints text \"obase=10; ibase=2; 101101\" The conversion formula \u2014 it tells the calculator what number base to use. obase \u201cOutput base\u201d \u2014 the numbering system you want the result in. ibase \u201cInput base\u201d \u2014 the numbering system your starting number is in. bc The basic calculator program built into Ubuntu \u2014 it performs the conversion. Below is what these commands look like in the Ubuntu CLI:","title":"Using Ubuntu to Convert Between Binary and Decimal"},{"location":"courses/ap_networking/networking_data_movement/#representing-network-information-as-hexadecimal-and-binary","text":"Although decimal is easily readable by humans, computers process information in binary. Any time a computer works with decimal numbers, it first has to convert it to binary in order to do any operations with it. To see Ubuntu's IP address in binary, ipcalc must be installed with sudo apt install ipcalc -y . After installing it, run ipcalc [IP address] to display network information in binary. Since hexadecimal can represent up to the number 15 in each character, it can store more information in less space when compared with decimal and especially compared to binary. To view network information in hexadecimal form in Ubuntu, the command printf '%02X%02X%02X%02X\\n' 192 168 64 2 , where 192, 168, 64, and 2 should be replaced with the digits from your IP address (found with ifconfig ). The output was C0A84002. Breaking this down: C0 = 192 A8 = 168 40 = 64 02 = 2 MAC addresses are also written in hexadecimal form; each pair of characters represents a byte of the device's hardware address. To find the MAC address in Ubuntu, run ip addr show .","title":"Representing Network Information as Hexadecimal and Binary"},{"location":"courses/ap_networking/networking_data_movement/#exploring-lan-components-through-ubuntu","text":"Information about the LAN can be identified in Ubuntu through CLI commands such as ifconfig , ping , and traceroute . These three commands are essential for finding information about the LAN as well as troubleshooting.","title":"Exploring LAN Components through Ubuntu"},{"location":"courses/ap_networking/networking_data_movement/#what-each-command-does","text":"Command Purpose Example Output ifconfig Shows a computer's IP address on the network inet 192.168.1.202 ping Tests if another computer or website is reachable within the LAN or over the internet 64 bytes from google.com ... traceroute Shows every step data takes to reach a destination 1 192.168.54.1 \u21d2 2 10.0.0.1 \u21d2 3 142.250.190.78, etc. Before running any of these commands, they had to be installed on the computer with sudo apt install net-tools -y to install ifconfig , sudo apt install iputils-ping -y to install ping , and sudo apt install traceroute -y to install traceroute . ifconfig is mainly used to display information relating how a computer connects to the LAN. It outputs information such as inet , the IP address assigned to the computer, netmask , which defines the portion of the IP address identifies the network and device, and broadcast , which is used to send messages to all devices on the same LAN. In the image above, inet, netmask, and broadcast are all outlined in red. Inet is the IP address of the computer (192.168.64.2). Netmask (255.255.255.0) defines which part of the IP address identifies the network, and which part identifies the device. When translated to binary, the netmask is 11111111.11111111.11111111.00000000. Each 1 means \"network\", and each 0 means \"host\", meaning that 192.168.64 identifies the network, and 2 identifies the device on the network. The range of valid IP addresses in the network would be anywhere from 192.168.64.1 - 192.168.64.254 (since 192.168.64.255 is the broadcast). ping is mainly used to test internet connection and DNS. Pinging a public DNS server, such as 8.8.8.8 (Google's public DNS server) and 1.1.1.1 (Cloudflare's public DNS server), is a simple way to ensure that a computer can reach the internet. Once internet connection is verified, pinging a domain such as google.com or cloudflare.com can test the DNS. If 8.8.8.8 is reachable but google.com isn't, that means that there is an issue with the DNS. To run these commands in Ubuntu and macOS, run ping -c 4 8.8.8.8 then ping google.com . In the image below, a ping was sent to google.com and other domains, but they didn't respond. So, the DNS was reset with sudo resolvectl flush-caches , which made google.com and cloudlfare.com reachable. After each ping, the system prints how long the data took to travel to the domain, which is a good indicator of internet speed. traceroute is used to show the route data takes to reach a destination. To test it with google.com (any domain will work), run traceroute google.com . This command will return a list of \"hops\" that show the path the data takes across routers to reach Google's server. When it was tested, the data took 23 \"hops\" and 55.121 ms to reach google.com. The data from traceroute can be confusing. Below is a table explaining what each part of the output means: Part What it Means Example Hop # The number on the left that shows how many \"steps\" away that device is from the computer 1, 2, 3, ... Hostname/IP Address The name or IP of the router or device that handled the packet 10.12.16.1, 209.85.244.81, etc. ms (milliseconds) The round-trip time (how long it took to reach that hop and come back) 209.85.244.81: 20.175 ms * Stars represent a connection to a router or device that refuses to return information * * * In the example above: Hop 1: *_gateway (192.168.64.1) is the local network gateway (what connects the LAN to the internet) Hop 2-10: Local or ISP routers Hop 11-20: Internet backbone routers (large, powerful routers moving data across regions or countries) Hop 23: Google's server (64.233.176.113)","title":"What Each Command Does"},{"location":"courses/ap_networking/networking_data_movement/#roadmap-representing-osi-layers","text":"**Reflection: This roadmap represents a data packet as a plane flying from an airport to another airport. It represents the different OSI layers as aspects of an airport, such as the runway being the physical layer. I think that the Transport layer is the most important since it ensures that everything arrives safely and in order. This project helped me understand how the TCP/IP model is a simplified version of the OSI model but both aim to describe how data is transferred.","title":"Roadmap Representing OSI Layers"},{"location":"courses/ap_networking/networking_data_movement/#testing-and-evaluation","text":"Throughout the activity, many labs were performed involving many different commands. Below, a table summarizes all of the commands used and their purpose. Concept Test Performed Verification Result Network Path Mapping traceroute google.com Successfully traced the route from the local router to Google\u2019s servers, totaling 23 hops in roughly 3 seconds. Data Exchange netcat between virtual machines Verified reliable message delivery over TCP between two VMs. Binary and Decimal Conversion bc command Confirmed accurate conversions between binary and decimal formats. Hexadecimal Output printf command Correctly formatted and displayed IP address as hexadecimal (e.g., C0A84002 ). LAN Interface Details ifconfig command Displayed inet , netmask , and broadcast parameters for network interfaces. Address and Range Calculation ipcalc Produced valid network, broadcast, and usable host address ranges. Open Port Detection netstat Revealed listening ports and established TCP connections between devices. Internet Reachability Test ping google.com and ping 8.8.8.8 Reported full packet reception with ~22 ms average response time.","title":"Testing and Evaluation"},{"location":"courses/ap_networking/networking_data_movement/#reflection","text":"By working through the various labs exploring data movement, I learned how exactly data is transferred between devices, both over a LAN and through the internet. Learning about the OSI and TCP/IP models helped simplify and understand how data communication works. Designing a roadmap to visualize data transfer helped me understand OSI layers even better. Additionally, converting between binary, decimal, and hexadecimal helped me understand the different number systems and showed the different advantages of each system. By using ipcalc , I learned how IP addresses are represented in binary. Using ping and traceroute demonstrated the path that data takes to transfer from one system to another and demonstrated a real world application of the OSI layers.","title":"Reflection"},{"location":"courses/ap_networking/physical_logical_addressing/","text":"Physical and Logical Addressing Project Introduction This project was focused on exploring physical and logical addressing (MAC and IP addresses), such as their unique purposes, use cases, and differences. Table of Contents Understanding Physical Addressing (MAC Addresses) Understanding Logical Addressing (IPv4 and IPv6) Dynamic vs Static Addressing & When to Use Each Configuring and Verifying IP Addresses on a Linux VM Reflection Understanding Physical Addressing (MAC Addresses) Part A: Exploring a Network Interface Card (NIC) Every device that connects to a network\u2014computers, phones, printers, game consoles\u2014contains a small hardware component called a Network Interface Card (NIC). The NIC is what sends and receives data over the network. Each NIC is assigned a permanent hardware identifier when it is manufactured. This identifier is called a physical address because it is tied to the physical hardware itself. On modern Ethernet and Wi-Fi networks, this physical address is known as a MAC Address (Media Access Control Address) Key points: It is assigned at the factory. It does not normally change, even if you move the device to another network. It uniquely identifies your network interface among all others in the world. NICs can take on many form factors. In portable devices (cell phones, laptops, tablets, etc.), the NIC is usually soldered directly to the board, or exists in the M.2 form factor. In desktop computers and servers, NICs usually utilize the PCIe interface and take up a PCIe x4 slot on a motherboard. In the NICs pictured above, the Ethernet port is used to transfer and receive packets from the network (OSI Layer 1) and the PCIe connector transmits data to the rest of the system so it can be processed by the RAM and CPU. It needs a main chip to handle all of the network protocol work, such as assembling frames, and tagging packets with its MAC address, amongst many other responsibilities. Due to a MAC being permanently assigned to each NIC, a MAC address is considered the physical address for a device. It cannot be altered by the OS or any software. This is why switches use Ethernet to send packets to the right device, since they operate at OSI Level 2 along with MAC addresses. Part B: Interpreting MAC Addresses The MAC address of the Ubuntu VM used in prior lessons was 0e:d6:c7:3a:ea:8f. A MAC is composed of 2 parts: the OUI (Organizationally Unique Identifier), represented by the first half of the MAC, and the device-specific identifier, represented by the second half of the MAC. In this case, the OUI was 0e:d6:c7 and the device-specific identifier was 3a:ea:8f. The OUI was researched on maclookup.app , which stated that the MAC as a \"randomized MAC.\" This makes sense, since the VM's NIC is virtualized, and UTM generates a random MAC during the VM configuration. Overall, when compared to the physical NICs in Part A which get their MAC addresses permanently assigned to them upon being manufactured, the VM's virtual NIC gets a randomized NIC. Both the physical and virtual MAC addresses follow the same format (48 bit value, 6 pairs of hex numbers, etc.), and they both serve as Layer 2 identifiers. However, physical MACs contain OUIs that represent the manufacturer of the device, whereas virtual MACs have OUIs that don't represent anything - they're randomly generated. They tend to look \"fake\" in terms of vendor code, which is why the MAC lookup website reported the VM's MAC as randomized. Below is a comparison table that lists different MACs, their vendor, and whether they are physical or virtual MACs: Full Address OUI Vendor Type of Vendor Notes F0:18:98:AA:BB:CC F0:18:98 Apple Physical Apple makes physical devices, which is why their MACs and OUIs are associated with physical MACs 3C:5A:B4:11:22:33 3C:5A:B4 Google Physical Similarly to Apple, Google only makes physical devices 60:45:BD:12:34:56 60:45:BD Microsoft Physical Similarly to Apple and Google, Microsoft only makes physical devices A4:BA:DB:22:33:44 A4:BA:DB Dell Physical Similarly to Microsoft, Apple, and Google, Microsoft only makes physical devices 04:1A:04:55:66:77 04:1A:04 WaveIP Physical Similarly to Dell, Microsoft, Apple, and Google, Microsoft only makes physical devices (WaveIP makes routers) 00:50:56:AA:BB:CC 00:50:56 VMWare Both VMWare makes both physical NICs and software which virtualize NICs (like VMWare Fusion), meaning that a VMWare NIC could be either physical or virtual 52:54:00:12:34:56 52:54:00 None Virtual The OUI 52:52:00 is not registered to any organization, meaning that this MAC is likely from a VM software such as UTM which generates a random MAC From this table, many patterns arose. Most of the registered vendors were well-known device makers such as Apple and Dell. Also, virtualization companies who deploy large-scale VM software to large corporations and enterprises need a registered OUI to ensure that the VMs and their virtual NICs appear as legitimate Layer 2 devices on a large network and avoid collisions with physical NICs. This activity helped me understand how MAC addressing operates. Part C: Connecting the Physical and Digital: Interpreting MAC Address Structure This section of the activity involved analyzing a MAC address's structure to understand what each part means and its function inside a local network. The VM's MAC address was 0e:d6:c7:3a:ea:8f. MAC addresses contain two subsections: the OUI and the device identifier. For this MAC address, the OUI is 0e:d6:c7, and the identifier is 3a:ea:8f, represented by the first 3 and last 3 pairs, respectively. The OUI represents a specific manufacturer, like Apple, Dell, Microsoft, etc., and is important because it can help identify specific devices on a network. For example, if a network administrator is searching for a specific Mac computer, they can filter out any MAC addresses that don't have OUIs registered to Apple. The device identifier is equally important, since it is unique to each NIC and prevents any other NIC from having the exact same MAC address. If 2 devices have the same MAC address on a network, there will almost certainly be connectivity issues for both devices, and data will often end up at the wrong device since they appear as identical devices on the network. Physical and virtual NICs store their MAC addresses differently. While physical NICs store their permanent MAC address in flash memory located on the NIC card, VMs get MAC addresses assigned by the hypervisor. These MAC addresses can either be completely random (like in UTM), or be within a certain range with a limited set of valid OUIs (in large-scale enterprise VM software like VMWare Fusion). Both physical and virtual MACs use the same 48 bit structure and must be unique. However, physical MACs are the only ones that are truly unique, as a virtual MAC can be any group of 48 bits as long as they follow the format of a MAC address. Regardless of physical or virtual hardware, OSI Layer 2 works identically with physical and virtual NICs since all Layer 2 works with is MAC addresses - the hardware / lack of hardware has no involvement with Layer 2. As long as the MAC is unique on the network, Layer 2 behaves identically across physical and virtual NICs. Reflection MAC addresses operate at OSI Layer 2, where switches and routers use them to deliver frames within the same local network to the correct device. They never leave the local network because routers replace the endpoint device's MAC address with the router's own MAC address, since the router is what forwards packets to and from the internet. Understanding Logical Addressing (IPv4 and IPv6) While MAC addresses work extremely well for communication within a single network, they have one major limitation: a device cannot reach another device outside its local network with just MAC addresses. This is where logical addressing is used, as it uses IP (Internet Protocol) Addresses to communicate across the world. Logical addresses allow devices to be located anywhere on Earth, change networks and still remain reachable, and to transfer data across multiple interconnected networks. There are two different standards in logical addressing: IPv4: the fourth version of the Internet Protocol and the first widely deployed system for global addressing. 32 bit address, supporting \u2248 4.3 billion unique addresses Example: 192.168.1.10 IPv6: the sixth version of the Internet Protocol designed to solve limitations of IPv4. While IPv4 can support \u2248 4.3 billion unique addresses, IPv6 allows for virtually infinite addresses since it uses 128 bits instead of 32. In fact, there are enough IPv6 addresses for every grain of sand on Earth to each have billions of addresses . IPv6 is written in hexadecimal with colons to make it easier to read Example: 2001:0db8:85a3::8a2e:0370:7334 Comparison Table between IPv4 and IPv6: Category IPv4 IPv6 Bits 32 128 Capacity ~4.3 billion Virtually unlimited Notation Dotted decimal Hexadecimal with colons Example 10.0.0.25 fe80::1f4a:e3ff:fe21:bd10 Logical Addressing Lab An activity was completed that focused on the following: Understanding the role of logical addresses Interpreting IPv4 and IPv6 addresses Understanding why IPv6 link-local addresses always exist Connecting logical addressing to global communication Documenting IP Addresses The Ubuntu VM's IP addresses were found using ip addr show , which printed the IPv4, IPv6, and MAC address, along with other information. IPv4 Address: 192.168.64.7 IPv6 Link-Local Address: fe80::909f:78ff:fe7c:ae77** Purpose of IPv6 Link-Local Address: An IPv6 link-local address is a special type of IPv6 address that a device uses exclusively to communicate with other devices on the same subnet. Routers do not forward traffic associated with link-local addresses. They start with fe80:: since the IPv6 standard mandates that all link-local addresses begin with this prefix. These addresses are very important for local communication, such as router discovery, finding other IPv6 devices on the network, and basic connectivity without requiring advanced IP configuration. They also do not need a DHCP server (usually a router) to assign an address; IPv6 uses stateless auto configuration, meaning that a system automatically generates a link-local address for each network interface (such as en0, en1, enp0s1, etc.). WHy IPv6 is Important for the Future of Networking: Although IPv4 has been the backbone of networking and the internet for a very long time, it is no longer sufficient for modern use since its 32-bit address space only provides ~4.3 billion addresses, which has been exhausted due to the influx of connected devices since the turn of the century. In the 21st century, there has been exponential growth of the internet, mobile devices, cloud services, and IoT devices. IPv6 solves this problem by using a 128-bit address space, allowing for a virtually unlimited number of unique IP addresses. Along with a far higher capacity for addresses, IPv6 has many improvements in both efficiency and security when compared to IPv4. However, since IPv4 is deeply embedded in existing infrastructure, modern networks must support both IPv4 and IPv6 simultaneously, allowing for a gradual migration to IPv6 rather than a sudden change, which would almost certainly cause global infrastructure outages and a lack of support for billions of devices. Reflection: Why Both Logical and Physical Addresses are Needed on a Network, and how IP Addresses Allow Communication Beyond the Local Network: Logical addresses (IP addresses) are needed in addition to MAC addresses since MAC addresses only work for communication within a local network, whereas IP addresses can work both locally and globally. Also, MAC addresses identify individual devices but are unable to identify where a device is located on the internet, which is why MAC addresses are not reachable outside the local network. IP addresses compensate for these shortcomings since they can work across multiple routers. An example of this principle is when running traceroute [public IP] , which showcases how data travels across many different hops (different routers). This would be impossible without IP addresses. Dynamic vs Static Addressing & When to Use Each Devices receive IP addresses in one of two ways: Dynamically: IP is automatically assigned by router, uses protocol called DHCP (Dynamic Host Configuration Protocol) Statically: IP is manually assigned to a device and does not change Dynamic and Static Addressing each have benefits and disadvantages, such as: Dynamic: Automatically assigned (no need for users or admins to configure IPs manually) Lease-based (devices need to renew their IP once the \"lease\" is up) Ideal for end-user devices Highly scalable Static: Manually configured (gives admins more control) Never changes unless reconfigured Commonly used in servers, printers, routers, cameras, etc. Not common for user endpoint devices. DHCP works by following a four step sequence informally referred to as \"DORA\": DHCP D ISCOVER: Device broadcasts a message across the network looking for a DHCP server to assign it an IP DHCP O FFER: A DHCP server replies with an available IP address and network settings DHCP R EQUEST: The device responds to the server agreeing to use the assigned IP DHCP A CK: The server sends a final confirmation known as \"ACK.\" Once the confirmation is acknowledged, the device officially receives its IP and can use the network normally. Activity: Dynamic vs Static Addressing Across Two VMs This activity involved exploring how two differently configured Ubuntu VMs receive their IP addresses and whether each uses dynamic (DHCP) or static addressing. To start, ip addr show was run in both VMs to find their IPv4 and IPv6 link-local addresses. VM1: IPv4 address: 192.168.64.2 IPv6 link-local address: fe80::1863:23ff:fe39:a42c VM2: IPv4 address: 10.12.26.1 IPv6 link-local address: fe80::cd6:c7ff:fe3a:ea8f The next step was to examine each VM's config files. This was first completed in VM1, where the command cat /etc/netplan/*.yaml was run in order to print out any files ending in .yaml in the directory /etc/netplan, the folder where the network config files are located. The output revealed, among other things, that the VM receives its IPv4 address dynamically since it says \"dhcp4: true\". The same command was run in VM2, which printed \"dhcp4: true\" and \"dhcp6: true\". This means that the VM receives both IPv4 and IPv6 addresses dynamically. When looking at both VMs together, both VMs use Netplan, the configuration layer that defines whether an interface uses DHCP in most modern Linux distributions. While VM2 uses DHCP for IPv4 and IPv6, VM1 only uses DHCP for IPv4, meaning that IPv6 has to be manually configured by the user. The output from running cat /etc/netplan/*.yaml was very similar between the two VMs, the only difference was that VM2 included a bit more information. The two Linux systems might configure networking differently since VM1 is running Ubuntu 22.04 while VM2 is running Ubuntu 25.10, so the newer version may have more advanced networking features, leading it to automatically configuring IPv6. Scenario Analysis In this section of the activity, a list of devices was provided and it was decided whether they should use static or dynamic addressing. School web server: Static A web server needs a consistent IP so that students and staff can reliably access it. Changing its IP could make it unreachable or harder to find on the network. A classroom printer: Static Printers should have fixed IPs so that multiple devices can always locate and print to them without having to search the network. Static IP prevents connection issues caused by changing addresses. Student laptops: Dynamic Laptops are frequently connected and disconnected, and there are many of them; using DHCP allows efficient IP allocation without manual configuration. Dynamic addressing simplifies management and avoids IP conflicts. Security cameras: Static Cameras need a predictable IP for monitoring systems and remote access; dynamic IPs could disrupt video feeds or recording schedules. Static IP ensures continuous access and easier maintenance. A teacher workstation: Dynamic Since the teacher workstation is primarily used for daily tasks like web browsing, email, and printing, it doesn\u2019t require a fixed IP. Dynamic addressing via DHCP simplifies network management and reduces the risk of IP conflicts, especially if the teacher occasionally moves the device to different classrooms or networks. Configuring and Verifying IP Addresses on a Linux VM In this activity, Ubuntu's network settings were manually edited to assign a static IP address. VM #2 from earlier activities was used in this activity since VM #1 uses NetworkManager, an older network manager that does not allow for direct .yaml file editing. To configure the static IP, the following steps were taken: Run ip link show to identify the active network interface. In this case, the active interface was enp0s1. Reveal contents of /etc/netplan with ls /etc/netplan . Open the file(s) located with sudo nano /etc/netplan/<filename>.yaml . In the case of this VM, files 00-installer-config.yaml and 01-network-manager-all.yaml were in the folder. Since the first file had \"config\" in the name, it was assumed that the first file was responsible for configuring the IP addresses (this ended up being correct). In the file, change DHCP to Static IP by changing dhcp4 from \"true\" to \"no\", then specifying the desired IP address underneath it. Save the file with Ctrl + O & Ctrl + X Apply the changes by running sudo netplan apply Test that the IP is correct with ip addr show , that the traffic is routed through the correct IP with ip route show , and that the VM can connect to the internet with ping -c 4 8.8.8.8 Reflection The most challenging part of this activity was getting the YAML file correct. It is very sensitive to minute errors in syntax, and it is very difficult to tell what is actually wrong. I learned how important precision is in editing config files such as the netplan YAML files. Reflection Working on this project helped me understand how physical and logical addressing work together. MAC addresses give each device a unique identity on a local network, while IP addresses let devices communicate across different networks. Learning about DHCP versus static addressing showed that dynamic IPs make managing devices like laptops easier, while static IPs are important for servers, printers, and cameras that need consistent addresses. Comparing VM #1 and VM #2 showed that different Linux versions and network managers handle IPs differently\u2014VM #1 used DHCP for IPv4 only, while VM #2 got both IPv4 and IPv6 addresses automatically. Editing YAML files for static IPs taught me that even small mistakes in syntax or indentation can break the network. These lessons apply to real-world devices: web servers need static IPs to stay reachable, printers need fixed addresses so everyone can print, and routers use both static and dynamic addressing to keep networks running smoothly. Overall, this project showed that careful setup of addresses is essential for reliable network communication and that precision matters when configuring network settings.","title":"Physical Addressing and Logical Addressing"},{"location":"courses/ap_networking/physical_logical_addressing/#physical-and-logical-addressing","text":"","title":"Physical and Logical Addressing"},{"location":"courses/ap_networking/physical_logical_addressing/#project-introduction","text":"This project was focused on exploring physical and logical addressing (MAC and IP addresses), such as their unique purposes, use cases, and differences.","title":"Project Introduction"},{"location":"courses/ap_networking/physical_logical_addressing/#table-of-contents","text":"Understanding Physical Addressing (MAC Addresses) Understanding Logical Addressing (IPv4 and IPv6) Dynamic vs Static Addressing & When to Use Each Configuring and Verifying IP Addresses on a Linux VM Reflection","title":"Table of Contents"},{"location":"courses/ap_networking/physical_logical_addressing/#understanding-physical-addressing-mac-addresses","text":"","title":"Understanding Physical Addressing (MAC Addresses)"},{"location":"courses/ap_networking/physical_logical_addressing/#part-a-exploring-a-network-interface-card-nic","text":"Every device that connects to a network\u2014computers, phones, printers, game consoles\u2014contains a small hardware component called a Network Interface Card (NIC). The NIC is what sends and receives data over the network. Each NIC is assigned a permanent hardware identifier when it is manufactured. This identifier is called a physical address because it is tied to the physical hardware itself. On modern Ethernet and Wi-Fi networks, this physical address is known as a MAC Address (Media Access Control Address) Key points: It is assigned at the factory. It does not normally change, even if you move the device to another network. It uniquely identifies your network interface among all others in the world. NICs can take on many form factors. In portable devices (cell phones, laptops, tablets, etc.), the NIC is usually soldered directly to the board, or exists in the M.2 form factor. In desktop computers and servers, NICs usually utilize the PCIe interface and take up a PCIe x4 slot on a motherboard. In the NICs pictured above, the Ethernet port is used to transfer and receive packets from the network (OSI Layer 1) and the PCIe connector transmits data to the rest of the system so it can be processed by the RAM and CPU. It needs a main chip to handle all of the network protocol work, such as assembling frames, and tagging packets with its MAC address, amongst many other responsibilities. Due to a MAC being permanently assigned to each NIC, a MAC address is considered the physical address for a device. It cannot be altered by the OS or any software. This is why switches use Ethernet to send packets to the right device, since they operate at OSI Level 2 along with MAC addresses.","title":"Part A: Exploring a Network Interface Card (NIC)"},{"location":"courses/ap_networking/physical_logical_addressing/#part-b-interpreting-mac-addresses","text":"The MAC address of the Ubuntu VM used in prior lessons was 0e:d6:c7:3a:ea:8f. A MAC is composed of 2 parts: the OUI (Organizationally Unique Identifier), represented by the first half of the MAC, and the device-specific identifier, represented by the second half of the MAC. In this case, the OUI was 0e:d6:c7 and the device-specific identifier was 3a:ea:8f. The OUI was researched on maclookup.app , which stated that the MAC as a \"randomized MAC.\" This makes sense, since the VM's NIC is virtualized, and UTM generates a random MAC during the VM configuration. Overall, when compared to the physical NICs in Part A which get their MAC addresses permanently assigned to them upon being manufactured, the VM's virtual NIC gets a randomized NIC. Both the physical and virtual MAC addresses follow the same format (48 bit value, 6 pairs of hex numbers, etc.), and they both serve as Layer 2 identifiers. However, physical MACs contain OUIs that represent the manufacturer of the device, whereas virtual MACs have OUIs that don't represent anything - they're randomly generated. They tend to look \"fake\" in terms of vendor code, which is why the MAC lookup website reported the VM's MAC as randomized. Below is a comparison table that lists different MACs, their vendor, and whether they are physical or virtual MACs: Full Address OUI Vendor Type of Vendor Notes F0:18:98:AA:BB:CC F0:18:98 Apple Physical Apple makes physical devices, which is why their MACs and OUIs are associated with physical MACs 3C:5A:B4:11:22:33 3C:5A:B4 Google Physical Similarly to Apple, Google only makes physical devices 60:45:BD:12:34:56 60:45:BD Microsoft Physical Similarly to Apple and Google, Microsoft only makes physical devices A4:BA:DB:22:33:44 A4:BA:DB Dell Physical Similarly to Microsoft, Apple, and Google, Microsoft only makes physical devices 04:1A:04:55:66:77 04:1A:04 WaveIP Physical Similarly to Dell, Microsoft, Apple, and Google, Microsoft only makes physical devices (WaveIP makes routers) 00:50:56:AA:BB:CC 00:50:56 VMWare Both VMWare makes both physical NICs and software which virtualize NICs (like VMWare Fusion), meaning that a VMWare NIC could be either physical or virtual 52:54:00:12:34:56 52:54:00 None Virtual The OUI 52:52:00 is not registered to any organization, meaning that this MAC is likely from a VM software such as UTM which generates a random MAC From this table, many patterns arose. Most of the registered vendors were well-known device makers such as Apple and Dell. Also, virtualization companies who deploy large-scale VM software to large corporations and enterprises need a registered OUI to ensure that the VMs and their virtual NICs appear as legitimate Layer 2 devices on a large network and avoid collisions with physical NICs. This activity helped me understand how MAC addressing operates.","title":"Part B: Interpreting MAC Addresses"},{"location":"courses/ap_networking/physical_logical_addressing/#part-c-connecting-the-physical-and-digital-interpreting-mac-address-structure","text":"This section of the activity involved analyzing a MAC address's structure to understand what each part means and its function inside a local network. The VM's MAC address was 0e:d6:c7:3a:ea:8f. MAC addresses contain two subsections: the OUI and the device identifier. For this MAC address, the OUI is 0e:d6:c7, and the identifier is 3a:ea:8f, represented by the first 3 and last 3 pairs, respectively. The OUI represents a specific manufacturer, like Apple, Dell, Microsoft, etc., and is important because it can help identify specific devices on a network. For example, if a network administrator is searching for a specific Mac computer, they can filter out any MAC addresses that don't have OUIs registered to Apple. The device identifier is equally important, since it is unique to each NIC and prevents any other NIC from having the exact same MAC address. If 2 devices have the same MAC address on a network, there will almost certainly be connectivity issues for both devices, and data will often end up at the wrong device since they appear as identical devices on the network. Physical and virtual NICs store their MAC addresses differently. While physical NICs store their permanent MAC address in flash memory located on the NIC card, VMs get MAC addresses assigned by the hypervisor. These MAC addresses can either be completely random (like in UTM), or be within a certain range with a limited set of valid OUIs (in large-scale enterprise VM software like VMWare Fusion). Both physical and virtual MACs use the same 48 bit structure and must be unique. However, physical MACs are the only ones that are truly unique, as a virtual MAC can be any group of 48 bits as long as they follow the format of a MAC address. Regardless of physical or virtual hardware, OSI Layer 2 works identically with physical and virtual NICs since all Layer 2 works with is MAC addresses - the hardware / lack of hardware has no involvement with Layer 2. As long as the MAC is unique on the network, Layer 2 behaves identically across physical and virtual NICs.","title":"Part C: Connecting the Physical and Digital: Interpreting MAC Address Structure"},{"location":"courses/ap_networking/physical_logical_addressing/#reflection","text":"MAC addresses operate at OSI Layer 2, where switches and routers use them to deliver frames within the same local network to the correct device. They never leave the local network because routers replace the endpoint device's MAC address with the router's own MAC address, since the router is what forwards packets to and from the internet.","title":"Reflection"},{"location":"courses/ap_networking/physical_logical_addressing/#understanding-logical-addressing-ipv4-and-ipv6","text":"While MAC addresses work extremely well for communication within a single network, they have one major limitation: a device cannot reach another device outside its local network with just MAC addresses. This is where logical addressing is used, as it uses IP (Internet Protocol) Addresses to communicate across the world. Logical addresses allow devices to be located anywhere on Earth, change networks and still remain reachable, and to transfer data across multiple interconnected networks. There are two different standards in logical addressing: IPv4: the fourth version of the Internet Protocol and the first widely deployed system for global addressing. 32 bit address, supporting \u2248 4.3 billion unique addresses Example: 192.168.1.10 IPv6: the sixth version of the Internet Protocol designed to solve limitations of IPv4. While IPv4 can support \u2248 4.3 billion unique addresses, IPv6 allows for virtually infinite addresses since it uses 128 bits instead of 32. In fact, there are enough IPv6 addresses for every grain of sand on Earth to each have billions of addresses . IPv6 is written in hexadecimal with colons to make it easier to read Example: 2001:0db8:85a3::8a2e:0370:7334 Comparison Table between IPv4 and IPv6: Category IPv4 IPv6 Bits 32 128 Capacity ~4.3 billion Virtually unlimited Notation Dotted decimal Hexadecimal with colons Example 10.0.0.25 fe80::1f4a:e3ff:fe21:bd10","title":"Understanding Logical Addressing (IPv4 and IPv6)"},{"location":"courses/ap_networking/physical_logical_addressing/#logical-addressing-lab","text":"An activity was completed that focused on the following: Understanding the role of logical addresses Interpreting IPv4 and IPv6 addresses Understanding why IPv6 link-local addresses always exist Connecting logical addressing to global communication","title":"Logical Addressing Lab"},{"location":"courses/ap_networking/physical_logical_addressing/#documenting-ip-addresses","text":"The Ubuntu VM's IP addresses were found using ip addr show , which printed the IPv4, IPv6, and MAC address, along with other information. IPv4 Address: 192.168.64.7 IPv6 Link-Local Address: fe80::909f:78ff:fe7c:ae77** Purpose of IPv6 Link-Local Address: An IPv6 link-local address is a special type of IPv6 address that a device uses exclusively to communicate with other devices on the same subnet. Routers do not forward traffic associated with link-local addresses. They start with fe80:: since the IPv6 standard mandates that all link-local addresses begin with this prefix. These addresses are very important for local communication, such as router discovery, finding other IPv6 devices on the network, and basic connectivity without requiring advanced IP configuration. They also do not need a DHCP server (usually a router) to assign an address; IPv6 uses stateless auto configuration, meaning that a system automatically generates a link-local address for each network interface (such as en0, en1, enp0s1, etc.). WHy IPv6 is Important for the Future of Networking: Although IPv4 has been the backbone of networking and the internet for a very long time, it is no longer sufficient for modern use since its 32-bit address space only provides ~4.3 billion addresses, which has been exhausted due to the influx of connected devices since the turn of the century. In the 21st century, there has been exponential growth of the internet, mobile devices, cloud services, and IoT devices. IPv6 solves this problem by using a 128-bit address space, allowing for a virtually unlimited number of unique IP addresses. Along with a far higher capacity for addresses, IPv6 has many improvements in both efficiency and security when compared to IPv4. However, since IPv4 is deeply embedded in existing infrastructure, modern networks must support both IPv4 and IPv6 simultaneously, allowing for a gradual migration to IPv6 rather than a sudden change, which would almost certainly cause global infrastructure outages and a lack of support for billions of devices. Reflection: Why Both Logical and Physical Addresses are Needed on a Network, and how IP Addresses Allow Communication Beyond the Local Network: Logical addresses (IP addresses) are needed in addition to MAC addresses since MAC addresses only work for communication within a local network, whereas IP addresses can work both locally and globally. Also, MAC addresses identify individual devices but are unable to identify where a device is located on the internet, which is why MAC addresses are not reachable outside the local network. IP addresses compensate for these shortcomings since they can work across multiple routers. An example of this principle is when running traceroute [public IP] , which showcases how data travels across many different hops (different routers). This would be impossible without IP addresses.","title":"Documenting IP Addresses"},{"location":"courses/ap_networking/physical_logical_addressing/#dynamic-vs-static-addressing-when-to-use-each","text":"Devices receive IP addresses in one of two ways: Dynamically: IP is automatically assigned by router, uses protocol called DHCP (Dynamic Host Configuration Protocol) Statically: IP is manually assigned to a device and does not change Dynamic and Static Addressing each have benefits and disadvantages, such as: Dynamic: Automatically assigned (no need for users or admins to configure IPs manually) Lease-based (devices need to renew their IP once the \"lease\" is up) Ideal for end-user devices Highly scalable Static: Manually configured (gives admins more control) Never changes unless reconfigured Commonly used in servers, printers, routers, cameras, etc. Not common for user endpoint devices. DHCP works by following a four step sequence informally referred to as \"DORA\": DHCP D ISCOVER: Device broadcasts a message across the network looking for a DHCP server to assign it an IP DHCP O FFER: A DHCP server replies with an available IP address and network settings DHCP R EQUEST: The device responds to the server agreeing to use the assigned IP DHCP A CK: The server sends a final confirmation known as \"ACK.\" Once the confirmation is acknowledged, the device officially receives its IP and can use the network normally.","title":"Dynamic vs Static Addressing &amp; When to Use Each"},{"location":"courses/ap_networking/physical_logical_addressing/#activity-dynamic-vs-static-addressing-across-two-vms","text":"This activity involved exploring how two differently configured Ubuntu VMs receive their IP addresses and whether each uses dynamic (DHCP) or static addressing. To start, ip addr show was run in both VMs to find their IPv4 and IPv6 link-local addresses. VM1: IPv4 address: 192.168.64.2 IPv6 link-local address: fe80::1863:23ff:fe39:a42c VM2: IPv4 address: 10.12.26.1 IPv6 link-local address: fe80::cd6:c7ff:fe3a:ea8f The next step was to examine each VM's config files. This was first completed in VM1, where the command cat /etc/netplan/*.yaml was run in order to print out any files ending in .yaml in the directory /etc/netplan, the folder where the network config files are located. The output revealed, among other things, that the VM receives its IPv4 address dynamically since it says \"dhcp4: true\". The same command was run in VM2, which printed \"dhcp4: true\" and \"dhcp6: true\". This means that the VM receives both IPv4 and IPv6 addresses dynamically. When looking at both VMs together, both VMs use Netplan, the configuration layer that defines whether an interface uses DHCP in most modern Linux distributions. While VM2 uses DHCP for IPv4 and IPv6, VM1 only uses DHCP for IPv4, meaning that IPv6 has to be manually configured by the user. The output from running cat /etc/netplan/*.yaml was very similar between the two VMs, the only difference was that VM2 included a bit more information. The two Linux systems might configure networking differently since VM1 is running Ubuntu 22.04 while VM2 is running Ubuntu 25.10, so the newer version may have more advanced networking features, leading it to automatically configuring IPv6.","title":"Activity: Dynamic vs Static Addressing Across Two VMs"},{"location":"courses/ap_networking/physical_logical_addressing/#scenario-analysis","text":"In this section of the activity, a list of devices was provided and it was decided whether they should use static or dynamic addressing. School web server: Static A web server needs a consistent IP so that students and staff can reliably access it. Changing its IP could make it unreachable or harder to find on the network. A classroom printer: Static Printers should have fixed IPs so that multiple devices can always locate and print to them without having to search the network. Static IP prevents connection issues caused by changing addresses. Student laptops: Dynamic Laptops are frequently connected and disconnected, and there are many of them; using DHCP allows efficient IP allocation without manual configuration. Dynamic addressing simplifies management and avoids IP conflicts. Security cameras: Static Cameras need a predictable IP for monitoring systems and remote access; dynamic IPs could disrupt video feeds or recording schedules. Static IP ensures continuous access and easier maintenance. A teacher workstation: Dynamic Since the teacher workstation is primarily used for daily tasks like web browsing, email, and printing, it doesn\u2019t require a fixed IP. Dynamic addressing via DHCP simplifies network management and reduces the risk of IP conflicts, especially if the teacher occasionally moves the device to different classrooms or networks.","title":"Scenario Analysis"},{"location":"courses/ap_networking/physical_logical_addressing/#configuring-and-verifying-ip-addresses-on-a-linux-vm","text":"In this activity, Ubuntu's network settings were manually edited to assign a static IP address. VM #2 from earlier activities was used in this activity since VM #1 uses NetworkManager, an older network manager that does not allow for direct .yaml file editing. To configure the static IP, the following steps were taken: Run ip link show to identify the active network interface. In this case, the active interface was enp0s1. Reveal contents of /etc/netplan with ls /etc/netplan . Open the file(s) located with sudo nano /etc/netplan/<filename>.yaml . In the case of this VM, files 00-installer-config.yaml and 01-network-manager-all.yaml were in the folder. Since the first file had \"config\" in the name, it was assumed that the first file was responsible for configuring the IP addresses (this ended up being correct). In the file, change DHCP to Static IP by changing dhcp4 from \"true\" to \"no\", then specifying the desired IP address underneath it. Save the file with Ctrl + O & Ctrl + X Apply the changes by running sudo netplan apply Test that the IP is correct with ip addr show , that the traffic is routed through the correct IP with ip route show , and that the VM can connect to the internet with ping -c 4 8.8.8.8","title":"Configuring and Verifying IP Addresses on a Linux VM"},{"location":"courses/ap_networking/physical_logical_addressing/#reflection_1","text":"The most challenging part of this activity was getting the YAML file correct. It is very sensitive to minute errors in syntax, and it is very difficult to tell what is actually wrong. I learned how important precision is in editing config files such as the netplan YAML files.","title":"Reflection"},{"location":"courses/ap_networking/physical_logical_addressing/#reflection_2","text":"Working on this project helped me understand how physical and logical addressing work together. MAC addresses give each device a unique identity on a local network, while IP addresses let devices communicate across different networks. Learning about DHCP versus static addressing showed that dynamic IPs make managing devices like laptops easier, while static IPs are important for servers, printers, and cameras that need consistent addresses. Comparing VM #1 and VM #2 showed that different Linux versions and network managers handle IPs differently\u2014VM #1 used DHCP for IPv4 only, while VM #2 got both IPv4 and IPv6 addresses automatically. Editing YAML files for static IPs taught me that even small mistakes in syntax or indentation can break the network. These lessons apply to real-world devices: web servers need static IPs to stay reachable, printers need fixed addresses so everyone can print, and routers use both static and dynamic addressing to keep networks running smoothly. Overall, this project showed that careful setup of addresses is essential for reliable network communication and that precision matters when configuring network settings.","title":"Reflection"},{"location":"courses/ap_networking/security-controls/","text":"Determining Security Controls for Devices Project Introduction This project was focused on securing physical devices, specifically, patching vulnerabilities, recognizing examples of phishing, and other similar skills. Hackers and other malicious actors are constantly trying to break into devices, therefore knowing how to secure a device is vital for both personal devices and in commercial settings. Planning and Design Technical Development Testing and Evaluation Reflection Planning and Design The main objectives for this project were to identify types of attacks and recommend security controls for devices. Cybersecurity Basics The main purpose of cybersecurity can be represented by the CIA triad: C onfidentiality: keeping information secret from malicious actors I ntegrity: keeping information trustworthy, uncorrupted, accurate, and ensuring it hasn't been tampered with A vailability: making sure that information and systems are available when needed Checking Ubuntu in UTM To ensure that Ubuntu is secure, it is vital to be on the latest release. To check the version, the command uname -a can be used, and should output an example similar to Linux ubuntu 5.15.0-87-generic #97-Ubuntu SMP Tue Oct 3 09:52:42 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux . This output means that the system is running Ubuntu on a 64-bit ARM processor (makes sense since M1 is an ARM chip) with Linux Kernel Version 5.15 which was built on October 3, 2023. Checking MacOS A similar process can be applied to macOS, albeit, with a slightly different command. system_profiler SPSoftwareDataType can be used to output data such as the macOS version, kernel version, and the build number. On my M1 Pro MacBook Pro, it outputted System Version: macOS 26.1 (25B5042k) Kernel Version: Darwin 25.1.0 Boot Volume: Macintosh HD Boot Mode: Normal Computer Name: Raaj's MacBook Pro User Name: Raaj Thakur (raajthakur) Secure Virtual Memory: Enabled System Integrity Protection: Enabled Time since boot: 2 days, 12 hours, 44 minutes The main takeaways from these data is that the computer is running macOS 26.1 with Darwin 25.1.0 (Darwin is the kernel of Apple software (such as macOS, VisionOS, iOS, etc.)). Common Device Vulnerabilities Devices are very complex, and therefore have many different aspects that could be vulnerable if ignored. Common vulnerabilities include: Outdated OS: Outdated operating systems almost always have vulnerabilities which can be exploited by malicious actors. Weak Passwords: Weak passwords can easily be guessed, allowing malicious actors to access sensitive data such as financial information, medical records, and more. Open Ports: Ports are \"doors\" that let data in and out of a computer. More open ports are more access points that malicious actors can exploit to access data on a computer. Therefore, only ports required for functionality should be open, and unused ports should be closed. Common ports include: Port 80 = HTTP (hyper text transfer protocol) Port 443 = HTTPS (secure hyper text transfer protocol) Port 22 = SSH (secure shell) Port 25 = SMTP (simple mail transfer protocol) Port 110 = POP3 (retrieve mail) Port 143 = IMAP (mail on server) Port 3389 = RDP (remote desktop protocol) Port 53 = DNS (domain name system) Port 67 = DHCP (dynamic host configuration protocol) Unpatched Software: Unpatched software is similar to outdated operating systems, as older versions of software often have vulnerabilities which malicious actors can use to do bad things. Vulnerability Tracking (CVE) Vulnerabilities are in virtually every software, and it is essential that these vulnerabilities are discovered and patched before hackers can exploit them. They are tracked using CVE, a standardized system for identifying and naming known vulnerabilities. Each vulnerability is assigned a unique CVE ID. Social Engineering Attacks Humans are usually the weakest link of the security of a device. Millions of people are targeted every day by social engineering attacks, attacks by hackers that exploit human behavior to trick them into installing malware, giving away information, or allowing hackers to directly control a system. Types of attacks include: Phishing: Broad attacks where malicious actors send mass emails, texts, or other similar things to trick users into clicking a fraudulent link. Spear Phishing: More targeted version of phishing that targets a specific group or organization. Pretexting: When an attacker invents a story or identity to trick a victim to sending them money, sharing sensitive information, or granting access to a device. Baiting: Offering a prize to trick a victim into doing something that installs malware or sharing sensitive information. Tailgating: A physical tactic where a malicious actor follows an authorized person into a secure location, giving them access to sensitive data. Technical Development Recognizing and Updating Outdated Software In order to secure the VM, I checked the version of important softwares in Ubuntu, then either installed them if they weren't previously installed or updated them if they were outdated. Software Status What the Software Does OpenSSL Up to Date Encrypts network traffic Firefox Up to Date Web browser LibreOffice Outdated Office software (text, spreadsheets, slideshows, etc.) Python Up to Date Popular programming language that underpins many modern apps and programs Apache HTTP Server Not Installed HTTP server; important for hosting websites GIMP Not Installed Photo editing software Java Not Installed Programming language that underpins many programs OpenSSH Up to Date Package for the SSH protocol, important for direct communication between 2 devices Checking Open Ports Open ports can be a vulnerability in a system and can easily be secured by closing them if the port is not necessary. The open ports can be checked on Linudx with netstat -tuln . netstat is a CLI tool that shows network connections, routing tables, and open ports, and the -tuln argument tells netstat to show T CP connections ( t ), U DP connections ( u ), only ports that are l istening ( l ), and show port n umbers instead of service names ( n ). On my VM, ports 445, 139, 53, 22, 54, 9843, 631, 5353, 43433, 68, 137, 139, and 53918 were open. Setting up the Firewall (UFW) On Linux, a commonly used firewall is UFW (Uncomplicated Firewall). A firewall is akin to a security guard for a computer: it decides which ports should stay open and which should be locked. To use UFW, follow the following steps: Install it with sudo apt update && sudo apt upgrade , then sudo apt install ufw -y Check UFW's status with sudo ufw status If it returns inactive , then enable it with sudo ufw enable . At this point, UFW should be enabled Testing and Evaluation Testing for Disk Encryption in Ubuntu and macOS Disk encryption is a common method of securing devices. It makes data on the SSD/HDD unreadable by anything other than the computer that has the encryption key for the drive. Both Linux and macOS have their own encryption services: LUKS and FileVault, respectively. Testing Encryption on Linux lsblk -f can be run to see if the disk is encrypted in Ubuntu. If the output mentions drives formatted as ext4 or vfat, that means that LUKS is off, and if it returns the type of drive as crypto_LUKS, then the drive is encrypted. My system was not encrypted, indicated by the ext4 and vfat at the bottom of the output. Testing Encryption on macOS To check FileVault (disk encryption) status on macOS, run fdesetup status , which either returns \"FileVault is On\" or \"FileVault is Off\". Without encryption, someone could physically access a device's data by removing the SSD/HDD and plugging it into their own computer (this can't be done on modern Macs since their SSDs are soldered to the board). Reflection This project taught me about the many types of vulnerabilities in computers, how to secure devices, different types of social engineering attacks, and more. Through completing this project, I learned how to use the CLI to secure devices on both Linux and macOS, and about how to work with many different elements such as ports, firewalls, encryption, and more. I learned a lot about the dangers of having outdated software, and about CVE. Overall, this project provided a thorough overview of how to determine security controls for devices.","title":"Determining Security Controls for Devices"},{"location":"courses/ap_networking/security-controls/#determining-security-controls-for-devices","text":"","title":"Determining Security Controls for Devices"},{"location":"courses/ap_networking/security-controls/#project-introduction","text":"This project was focused on securing physical devices, specifically, patching vulnerabilities, recognizing examples of phishing, and other similar skills. Hackers and other malicious actors are constantly trying to break into devices, therefore knowing how to secure a device is vital for both personal devices and in commercial settings. Planning and Design Technical Development Testing and Evaluation Reflection","title":"Project Introduction"},{"location":"courses/ap_networking/security-controls/#planning-and-design","text":"The main objectives for this project were to identify types of attacks and recommend security controls for devices.","title":"Planning and Design"},{"location":"courses/ap_networking/security-controls/#cybersecurity-basics","text":"The main purpose of cybersecurity can be represented by the CIA triad: C onfidentiality: keeping information secret from malicious actors I ntegrity: keeping information trustworthy, uncorrupted, accurate, and ensuring it hasn't been tampered with A vailability: making sure that information and systems are available when needed","title":"Cybersecurity Basics"},{"location":"courses/ap_networking/security-controls/#checking-ubuntu-in-utm","text":"To ensure that Ubuntu is secure, it is vital to be on the latest release. To check the version, the command uname -a can be used, and should output an example similar to Linux ubuntu 5.15.0-87-generic #97-Ubuntu SMP Tue Oct 3 09:52:42 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux . This output means that the system is running Ubuntu on a 64-bit ARM processor (makes sense since M1 is an ARM chip) with Linux Kernel Version 5.15 which was built on October 3, 2023.","title":"Checking Ubuntu in UTM"},{"location":"courses/ap_networking/security-controls/#checking-macos","text":"A similar process can be applied to macOS, albeit, with a slightly different command. system_profiler SPSoftwareDataType can be used to output data such as the macOS version, kernel version, and the build number. On my M1 Pro MacBook Pro, it outputted System Version: macOS 26.1 (25B5042k) Kernel Version: Darwin 25.1.0 Boot Volume: Macintosh HD Boot Mode: Normal Computer Name: Raaj's MacBook Pro User Name: Raaj Thakur (raajthakur) Secure Virtual Memory: Enabled System Integrity Protection: Enabled Time since boot: 2 days, 12 hours, 44 minutes The main takeaways from these data is that the computer is running macOS 26.1 with Darwin 25.1.0 (Darwin is the kernel of Apple software (such as macOS, VisionOS, iOS, etc.)).","title":"Checking MacOS"},{"location":"courses/ap_networking/security-controls/#common-device-vulnerabilities","text":"Devices are very complex, and therefore have many different aspects that could be vulnerable if ignored. Common vulnerabilities include: Outdated OS: Outdated operating systems almost always have vulnerabilities which can be exploited by malicious actors. Weak Passwords: Weak passwords can easily be guessed, allowing malicious actors to access sensitive data such as financial information, medical records, and more. Open Ports: Ports are \"doors\" that let data in and out of a computer. More open ports are more access points that malicious actors can exploit to access data on a computer. Therefore, only ports required for functionality should be open, and unused ports should be closed. Common ports include: Port 80 = HTTP (hyper text transfer protocol) Port 443 = HTTPS (secure hyper text transfer protocol) Port 22 = SSH (secure shell) Port 25 = SMTP (simple mail transfer protocol) Port 110 = POP3 (retrieve mail) Port 143 = IMAP (mail on server) Port 3389 = RDP (remote desktop protocol) Port 53 = DNS (domain name system) Port 67 = DHCP (dynamic host configuration protocol) Unpatched Software: Unpatched software is similar to outdated operating systems, as older versions of software often have vulnerabilities which malicious actors can use to do bad things.","title":"Common Device Vulnerabilities"},{"location":"courses/ap_networking/security-controls/#vulnerability-tracking-cve","text":"Vulnerabilities are in virtually every software, and it is essential that these vulnerabilities are discovered and patched before hackers can exploit them. They are tracked using CVE, a standardized system for identifying and naming known vulnerabilities. Each vulnerability is assigned a unique CVE ID.","title":"Vulnerability Tracking (CVE)"},{"location":"courses/ap_networking/security-controls/#social-engineering-attacks","text":"Humans are usually the weakest link of the security of a device. Millions of people are targeted every day by social engineering attacks, attacks by hackers that exploit human behavior to trick them into installing malware, giving away information, or allowing hackers to directly control a system. Types of attacks include: Phishing: Broad attacks where malicious actors send mass emails, texts, or other similar things to trick users into clicking a fraudulent link. Spear Phishing: More targeted version of phishing that targets a specific group or organization. Pretexting: When an attacker invents a story or identity to trick a victim to sending them money, sharing sensitive information, or granting access to a device. Baiting: Offering a prize to trick a victim into doing something that installs malware or sharing sensitive information. Tailgating: A physical tactic where a malicious actor follows an authorized person into a secure location, giving them access to sensitive data.","title":"Social Engineering Attacks"},{"location":"courses/ap_networking/security-controls/#technical-development","text":"","title":"Technical Development"},{"location":"courses/ap_networking/security-controls/#recognizing-and-updating-outdated-software","text":"In order to secure the VM, I checked the version of important softwares in Ubuntu, then either installed them if they weren't previously installed or updated them if they were outdated. Software Status What the Software Does OpenSSL Up to Date Encrypts network traffic Firefox Up to Date Web browser LibreOffice Outdated Office software (text, spreadsheets, slideshows, etc.) Python Up to Date Popular programming language that underpins many modern apps and programs Apache HTTP Server Not Installed HTTP server; important for hosting websites GIMP Not Installed Photo editing software Java Not Installed Programming language that underpins many programs OpenSSH Up to Date Package for the SSH protocol, important for direct communication between 2 devices","title":"Recognizing and Updating Outdated Software"},{"location":"courses/ap_networking/security-controls/#checking-open-ports","text":"Open ports can be a vulnerability in a system and can easily be secured by closing them if the port is not necessary. The open ports can be checked on Linudx with netstat -tuln . netstat is a CLI tool that shows network connections, routing tables, and open ports, and the -tuln argument tells netstat to show T CP connections ( t ), U DP connections ( u ), only ports that are l istening ( l ), and show port n umbers instead of service names ( n ). On my VM, ports 445, 139, 53, 22, 54, 9843, 631, 5353, 43433, 68, 137, 139, and 53918 were open.","title":"Checking Open Ports"},{"location":"courses/ap_networking/security-controls/#setting-up-the-firewall-ufw","text":"On Linux, a commonly used firewall is UFW (Uncomplicated Firewall). A firewall is akin to a security guard for a computer: it decides which ports should stay open and which should be locked. To use UFW, follow the following steps: Install it with sudo apt update && sudo apt upgrade , then sudo apt install ufw -y Check UFW's status with sudo ufw status If it returns inactive , then enable it with sudo ufw enable . At this point, UFW should be enabled","title":"Setting up the Firewall (UFW)"},{"location":"courses/ap_networking/security-controls/#testing-and-evaluation","text":"","title":"Testing and Evaluation"},{"location":"courses/ap_networking/security-controls/#testing-for-disk-encryption-in-ubuntu-and-macos","text":"Disk encryption is a common method of securing devices. It makes data on the SSD/HDD unreadable by anything other than the computer that has the encryption key for the drive. Both Linux and macOS have their own encryption services: LUKS and FileVault, respectively.","title":"Testing for Disk Encryption in Ubuntu and macOS"},{"location":"courses/ap_networking/security-controls/#testing-encryption-on-linux","text":"lsblk -f can be run to see if the disk is encrypted in Ubuntu. If the output mentions drives formatted as ext4 or vfat, that means that LUKS is off, and if it returns the type of drive as crypto_LUKS, then the drive is encrypted. My system was not encrypted, indicated by the ext4 and vfat at the bottom of the output.","title":"Testing Encryption on Linux"},{"location":"courses/ap_networking/security-controls/#testing-encryption-on-macos","text":"To check FileVault (disk encryption) status on macOS, run fdesetup status , which either returns \"FileVault is On\" or \"FileVault is Off\". Without encryption, someone could physically access a device's data by removing the SSD/HDD and plugging it into their own computer (this can't be done on modern Macs since their SSDs are soldered to the board).","title":"Testing Encryption on macOS"},{"location":"courses/ap_networking/security-controls/#reflection","text":"This project taught me about the many types of vulnerabilities in computers, how to secure devices, different types of social engineering attacks, and more. Through completing this project, I learned how to use the CLI to secure devices on both Linux and macOS, and about how to work with many different elements such as ports, firewalls, encryption, and more. I learned a lot about the dangers of having outdated software, and about CVE. Overall, this project provided a thorough overview of how to determine security controls for devices.","title":"Reflection"},{"location":"courses/ap_networking/troubleshooting/","text":"Troubleshooting Project Introduction This project was focused on troubleshooting various aspects of networks through CLI commands. Both MacOS and Ubuntu in a VM were used. Important Terminology Term Definition Wi-Fi Protocol that uses radio waves to wirelessly connect devices to a local network Ethernet Protocol that uses cables to connect devices to a local network Network Adapter Hardware that lets a computer communicate with the network (NIC in a PC) IP Address Identifiable address given to any device on a network Default Gateway (Router) Device that connects the local network to the internet. Metaphor: exit ramp connecting a neighborhood to a highway. DNS Domain Name System; translates website names to IP addresses Ping CLI command that tests communication between devices NAT/Shared Networking Network Address Translation; VM shares host's IP address Bridged Networking VM appears as its own device with a separate IP from the host Four Step Troubleshooting Workflow Test the Physical Connection Check if Wi-Fi is turned on or if Ethernet is plugged in Check Wi-Fi/Ethernet settings in MacOS/Windows/Linux Check the IP Address MacOS: Run ifconfig , then look for en0/en1 and make sure that at least one of them lists inet: 192.168.x.x or 10.x.x.x Linux: Run ip addr , then look for interfaces named like enpXsY (for example, enp0s3 or enp0s1 ), which are common on modern Linux systems, and make sure that at least one of them lists inet: 192.168.x.x or 10.x.x.x If it lists 169.254.x.x, that means that the device has a self-assigned IP address , meaning it likely can't connect to the internet Test Basic Reachability Ping a public server from Google or Cloudflare with ping -c 4 8.8.8.8 or ping -c 4 1.1.1.1 , respectively, to make sure that the computer can communicate with them If this works, then the computer can connect to the internet Test the DNS After confirming that ping -c 4 8.8.8.8 works, check the DNS with ping -c 4 google.com . 8.8.8.8 is Google's public DNS, meaning that when you connect to 8.8.8.8, you connect to google.com, and vice versa. If the computer can connect to the IP address (8.8.8.8) but not the domain name (google.com), then there is a DNS issue IP Range Meaning The type of IP address that is assigned to a device has a specific meaning. Below are common private IP address ranges and what they mean. IP Range What it Means Example 192.168.x.x Private IP common in home/school network; assigned by router/DHCP Common on Wi-Fi networks 10.x.x.x Private IP common in large organizations and VMs Ubuntu VM in Bridged mode 172.16.x.x \u2013 172.31.x.x Less common private IP Pretty rare, usually is assigned when there are a very large amount of devices on a network 169.254.x.x Self-assigned IP (device could not get IP from router) Happens if Wi-Fi is on but router is unresponsive or if computer cannot communicate with router for any reason Network Troubleshooting from MacOS CLI When troubleshooting networks on a Mac, vital commands to know are: - ifconfig : reveals IP address and other network information - ping : tests connection - route -n get default : outputs default gateway (router) information Network Troubleshooting from Linux CLI When troubleshooting networks on Linux, vital commands to know are: - ip addr : reveals IP address and other network information - ping : tests connection - ip route : outputs default gateway (router) information (preferred modern command) - netstat -rn : also outputs routing information, but may not be installed by default on modern Linux distributions Shared vs Bridged Network in UTM (Ubuntu) In UTM, the two major network modes are Shared and Bridged . While Shared mode shares the host device's IP address and appears as the host on the network, Bridged mode acts as its own separate computer with a unique IP address. Shared Mode Exploration I started the exploration with Shared mode. Running ip addr returned an IP address of 192.168.64.6 under enp0s1, which is standard behavior for a VM. The network adapter is circled in red, the IP address in blue, and the MAC address in green. Bridged Mode Exploration After exploring Shared mode, I powered down the VM, edited the VM's network settings from UTM, and set network mode to Bridged and selected Emulated Network to be en1 (since the Mac mini was on Wi-Fi, not Ethernet). Running ip addr in the Bridged VM returned an IP address of 192.168.1.132 under enp0s1, which is standard behavior for a VM. Notably, the IP address was different than it was under Shared mode, exhibiting how Bridged mode makes the router think that the VM is an individual computer. After testing the IP address, basic reachability needed to be tested. This was done with ping -c 4 8.8.8.8 (pings the Google public server 4 times). This did not work at first, but this was due to a firewall issue in Ubuntu. Running sudo ufw disable stopped the firewall and allowed Ubuntu to ping Google's server. Caution: Disabling the firewall with sudo ufw disable is not recommended in production environments, as it can expose your system to security risks. Only disable the firewall temporarily for troubleshooting, and re-enable it ( sudo ufw enable ) when finished. Additionally, the VM in Bridged mode could ping the Mac mini's IP address, since they act as 2 separate computers. The latency between the Mac mini and the VM was considerably lower than between the VM and Google (0.838 ms vs 12.694 ms), which makes sense since the Mac mini is on the same local network as the VM. Next, the DNS needed to be tested. Running ping -c 4 google.com returned virtually the exact same output as ping -c 4 8.8.8.8 (there was a very slight difference in average latency within margin of error), meaning that the DNS worked. Reflection This assignment provided a strong overview of how to troubleshoot networks from MacOS and Linux. Learning CLI tools to diagnose networks is extremely important in many settings, since virtually all networking equipment is accessible solely through a CLI. The 4 step workflow is used by virtually all professionals in their industry to troubleshoot networks, so learning those skills now is very valuable.","title":"Network Troubleshooting"},{"location":"courses/ap_networking/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"courses/ap_networking/troubleshooting/#project-introduction","text":"This project was focused on troubleshooting various aspects of networks through CLI commands. Both MacOS and Ubuntu in a VM were used.","title":"Project Introduction"},{"location":"courses/ap_networking/troubleshooting/#important-terminology","text":"Term Definition Wi-Fi Protocol that uses radio waves to wirelessly connect devices to a local network Ethernet Protocol that uses cables to connect devices to a local network Network Adapter Hardware that lets a computer communicate with the network (NIC in a PC) IP Address Identifiable address given to any device on a network Default Gateway (Router) Device that connects the local network to the internet. Metaphor: exit ramp connecting a neighborhood to a highway. DNS Domain Name System; translates website names to IP addresses Ping CLI command that tests communication between devices NAT/Shared Networking Network Address Translation; VM shares host's IP address Bridged Networking VM appears as its own device with a separate IP from the host","title":"Important Terminology"},{"location":"courses/ap_networking/troubleshooting/#four-step-troubleshooting-workflow","text":"Test the Physical Connection Check if Wi-Fi is turned on or if Ethernet is plugged in Check Wi-Fi/Ethernet settings in MacOS/Windows/Linux Check the IP Address MacOS: Run ifconfig , then look for en0/en1 and make sure that at least one of them lists inet: 192.168.x.x or 10.x.x.x Linux: Run ip addr , then look for interfaces named like enpXsY (for example, enp0s3 or enp0s1 ), which are common on modern Linux systems, and make sure that at least one of them lists inet: 192.168.x.x or 10.x.x.x If it lists 169.254.x.x, that means that the device has a self-assigned IP address , meaning it likely can't connect to the internet Test Basic Reachability Ping a public server from Google or Cloudflare with ping -c 4 8.8.8.8 or ping -c 4 1.1.1.1 , respectively, to make sure that the computer can communicate with them If this works, then the computer can connect to the internet Test the DNS After confirming that ping -c 4 8.8.8.8 works, check the DNS with ping -c 4 google.com . 8.8.8.8 is Google's public DNS, meaning that when you connect to 8.8.8.8, you connect to google.com, and vice versa. If the computer can connect to the IP address (8.8.8.8) but not the domain name (google.com), then there is a DNS issue","title":"Four Step Troubleshooting Workflow"},{"location":"courses/ap_networking/troubleshooting/#ip-range-meaning","text":"The type of IP address that is assigned to a device has a specific meaning. Below are common private IP address ranges and what they mean. IP Range What it Means Example 192.168.x.x Private IP common in home/school network; assigned by router/DHCP Common on Wi-Fi networks 10.x.x.x Private IP common in large organizations and VMs Ubuntu VM in Bridged mode 172.16.x.x \u2013 172.31.x.x Less common private IP Pretty rare, usually is assigned when there are a very large amount of devices on a network 169.254.x.x Self-assigned IP (device could not get IP from router) Happens if Wi-Fi is on but router is unresponsive or if computer cannot communicate with router for any reason","title":"IP Range Meaning"},{"location":"courses/ap_networking/troubleshooting/#network-troubleshooting-from-macos-cli","text":"When troubleshooting networks on a Mac, vital commands to know are: - ifconfig : reveals IP address and other network information - ping : tests connection - route -n get default : outputs default gateway (router) information","title":"Network Troubleshooting from MacOS CLI"},{"location":"courses/ap_networking/troubleshooting/#network-troubleshooting-from-linux-cli","text":"When troubleshooting networks on Linux, vital commands to know are: - ip addr : reveals IP address and other network information - ping : tests connection - ip route : outputs default gateway (router) information (preferred modern command) - netstat -rn : also outputs routing information, but may not be installed by default on modern Linux distributions","title":"Network Troubleshooting from Linux CLI"},{"location":"courses/ap_networking/troubleshooting/#shared-vs-bridged-network-in-utm-ubuntu","text":"In UTM, the two major network modes are Shared and Bridged . While Shared mode shares the host device's IP address and appears as the host on the network, Bridged mode acts as its own separate computer with a unique IP address.","title":"Shared vs Bridged Network in UTM (Ubuntu)"},{"location":"courses/ap_networking/troubleshooting/#shared-mode-exploration","text":"I started the exploration with Shared mode. Running ip addr returned an IP address of 192.168.64.6 under enp0s1, which is standard behavior for a VM. The network adapter is circled in red, the IP address in blue, and the MAC address in green.","title":"Shared Mode Exploration"},{"location":"courses/ap_networking/troubleshooting/#bridged-mode-exploration","text":"After exploring Shared mode, I powered down the VM, edited the VM's network settings from UTM, and set network mode to Bridged and selected Emulated Network to be en1 (since the Mac mini was on Wi-Fi, not Ethernet). Running ip addr in the Bridged VM returned an IP address of 192.168.1.132 under enp0s1, which is standard behavior for a VM. Notably, the IP address was different than it was under Shared mode, exhibiting how Bridged mode makes the router think that the VM is an individual computer. After testing the IP address, basic reachability needed to be tested. This was done with ping -c 4 8.8.8.8 (pings the Google public server 4 times). This did not work at first, but this was due to a firewall issue in Ubuntu. Running sudo ufw disable stopped the firewall and allowed Ubuntu to ping Google's server. Caution: Disabling the firewall with sudo ufw disable is not recommended in production environments, as it can expose your system to security risks. Only disable the firewall temporarily for troubleshooting, and re-enable it ( sudo ufw enable ) when finished. Additionally, the VM in Bridged mode could ping the Mac mini's IP address, since they act as 2 separate computers. The latency between the Mac mini and the VM was considerably lower than between the VM and Google (0.838 ms vs 12.694 ms), which makes sense since the Mac mini is on the same local network as the VM. Next, the DNS needed to be tested. Running ping -c 4 google.com returned virtually the exact same output as ping -c 4 8.8.8.8 (there was a very slight difference in average latency within margin of error), meaning that the DNS worked.","title":"Bridged Mode Exploration"},{"location":"courses/ap_networking/troubleshooting/#reflection","text":"This assignment provided a strong overview of how to troubleshoot networks from MacOS and Linux. Learning CLI tools to diagnose networks is extremely important in many settings, since virtually all networking equipment is accessible solely through a CLI. The 4 step workflow is used by virtually all professionals in their industry to troubleshoot networks, so learning those skills now is very valuable.","title":"Reflection"},{"location":"courses/ap_networking/why_cant_these_two_computers_talk/","text":"Why Can't These Two Computers Talk to Each Other? Project Introduction This project was a 1 day activity where a partner and I investigated why two computers connected with a working Ethernet cable are still unable to communicate. Both computers were running 2 different Ubuntu virtual machines, however they were unable to ping each other. Part 1 Before doing anything, the connection was verified by inspecting the Ethernet port and cable on both of the Mac mini computers. Once the Ethernet connection was confirmed to be good, 2 VMs were opened on each Mac mini: an Ubuntu 25.10 ARM64 VM using Apple Virtualization, and an Ubuntu 22.04 ARM64 VM using QEMU. Then, ip a was run on each VM to obtain the IP address and status for each VM. All of them returned an IP address and reported \"state UP\", meaning that there is an active connection. Ubuntu 25.10 w/ Apple Virtualization: Ubuntu 22.04 w/ QEMU: Part 2 After it was verified that all of the VMs had IP addresses, we made sure that they were on the same Layer 2 network. This was tested by pinging each other's IP address. A pattern was observed: the Ubuntu 22.04 VM could only ping my partner's Ubuntu 25.10 VM and not his 22.04 VM, whereas the Ubuntu 25.10 VM could only ping his 22.04 VM, not his 25.10 VM. He observed the same phenomenon. Pinging 22.04 from 25.10 Here, pinging my partner's 22.04 VM from my 25.10 VM was successful. Pinging 25.10 from 25.10 Here, pinging my partner's 25.10 VM from my 25.10 VM was unsuccessful. Pinging 25.10 from 22.04 Here, pinging my partner's 25.10 VM from my 22.04 VM was successful. Pinging 22.04 from 22.04 Here, pinging my partner's 22.04 VM from my 22.04 VM was unsuccessful. In this test, OSI Layer 2 and 3 was tested since they use MAC and IP addresses, respectively, to reach the correct device. Part 3 After observing the interactions between the VMs, we had to determine whether the issue comes from IP configuration. Direct connections only work if both computers are on the same subnet , but in VMs in UTM, the machines are in two separate private host-only networks created automatically by UTM. This means that they appear like the share a subnet but are actually behind different invisible switches , meaning that they can't see each other. In the VMs, the IPs are not identical since they're separate devices and the router assigns them different IP addresses. If the IPs were to match, they VMs may not necessarily be on the same network. For example, two different devices could have the IP 192.168.4.2 but be on different networks. Since this IP is private , they have no meaning outside the LAN. Under this configuration, despite being on the same subnet and being on the same network (since the first 20 bits are identical), UTM places each VM into is own isolated virtual network. This means that the two machines are on completely different networks and cannot see each other's ping requests. Layer 3 is impossible since ARP cannot succeed across two separate virtual switches. Part 4 To verify this claim, the VMs were pinged again. The Ubuntu 25.10 machines still could not ping each other, and the Ubuntu 22.04 machines still could not ping each other. This happens because each virtualization system (Apple Virtualization for the 25.10 VM and QEMU for the 22.04 VM) places its VM into a private, host-only network that exists only within each Mac mini. These private networks are completely independent from one another, so the VMs are not sharing the same Layer 2 network\u2014the layer where devices learn each other\u2019s MAC addresses. Since their OSI Layer 2s are independent, they can\u2019t discover each other, and communication fails before it even reaches the IP level (Layer 3). Surprisingly, the 25.10 VM can ping the 22.04 VM on the other computer (Apple Virtualization to QEMU or vice versa) because each Mac mini is able to route traffic between its own internal virtual networks before sending it over the Ethernet cable. This makes cross-platform pings work, while same-platform pings fail due to Layer-2 isolation. Pinging 22.04 from 25.10 Here, pinging my partner's 22.04 VM from my 25.10 VM was successful. Pinging 25.10 from 25.10 Here, pinging my partner's 25.10 VM from my 25.10 VM was unsuccessful. Pinging 25.10 from 22.04 Here, pinging my partner's 25.10 VM from my 22.04 VM was successful. Pinging 22.04 from 22.04 Here, pinging my partner's 22.04 VM from my 22.04 VM was unsuccessful. Part 5 Even though the two computers were connected with a working Ethernet cable, they could not communicate because each VM was in its own isolated VLAN, meaning that the VMs on different Mac minis were on different Layer 2 domains. The failure happened at OSI Layer 2 since the ARP could not find the correct MAC address associated with the IP address which prevented data from transferring. UTM prevents the VMs from connecting since it creates a virtual LAN for each Bridged VM to simulate an entire LAN and device. To allow for the VMs to easily connect, they should be placed in Shared (NAT) mode so they can piggyback off of the Mac mini's IP address and communicate with other devices on the LAN. In a real SOHO network, routers and switches prevent similar issues since they create multiple VLANs with different subnets and facilitate data transfer between the subnets to allow for many devices to be on the same LAN.","title":"Why Can't These Two Computers Talk to Each Other?"},{"location":"courses/ap_networking/why_cant_these_two_computers_talk/#why-cant-these-two-computers-talk-to-each-other","text":"","title":"Why Can't These Two Computers Talk to Each Other?"},{"location":"courses/ap_networking/why_cant_these_two_computers_talk/#project-introduction","text":"This project was a 1 day activity where a partner and I investigated why two computers connected with a working Ethernet cable are still unable to communicate. Both computers were running 2 different Ubuntu virtual machines, however they were unable to ping each other.","title":"Project Introduction"},{"location":"courses/ap_networking/why_cant_these_two_computers_talk/#part-1","text":"Before doing anything, the connection was verified by inspecting the Ethernet port and cable on both of the Mac mini computers. Once the Ethernet connection was confirmed to be good, 2 VMs were opened on each Mac mini: an Ubuntu 25.10 ARM64 VM using Apple Virtualization, and an Ubuntu 22.04 ARM64 VM using QEMU. Then, ip a was run on each VM to obtain the IP address and status for each VM. All of them returned an IP address and reported \"state UP\", meaning that there is an active connection. Ubuntu 25.10 w/ Apple Virtualization: Ubuntu 22.04 w/ QEMU:","title":"Part 1"},{"location":"courses/ap_networking/why_cant_these_two_computers_talk/#part-2","text":"After it was verified that all of the VMs had IP addresses, we made sure that they were on the same Layer 2 network. This was tested by pinging each other's IP address. A pattern was observed: the Ubuntu 22.04 VM could only ping my partner's Ubuntu 25.10 VM and not his 22.04 VM, whereas the Ubuntu 25.10 VM could only ping his 22.04 VM, not his 25.10 VM. He observed the same phenomenon. Pinging 22.04 from 25.10 Here, pinging my partner's 22.04 VM from my 25.10 VM was successful. Pinging 25.10 from 25.10 Here, pinging my partner's 25.10 VM from my 25.10 VM was unsuccessful. Pinging 25.10 from 22.04 Here, pinging my partner's 25.10 VM from my 22.04 VM was successful. Pinging 22.04 from 22.04 Here, pinging my partner's 22.04 VM from my 22.04 VM was unsuccessful. In this test, OSI Layer 2 and 3 was tested since they use MAC and IP addresses, respectively, to reach the correct device.","title":"Part 2"},{"location":"courses/ap_networking/why_cant_these_two_computers_talk/#part-3","text":"After observing the interactions between the VMs, we had to determine whether the issue comes from IP configuration. Direct connections only work if both computers are on the same subnet , but in VMs in UTM, the machines are in two separate private host-only networks created automatically by UTM. This means that they appear like the share a subnet but are actually behind different invisible switches , meaning that they can't see each other. In the VMs, the IPs are not identical since they're separate devices and the router assigns them different IP addresses. If the IPs were to match, they VMs may not necessarily be on the same network. For example, two different devices could have the IP 192.168.4.2 but be on different networks. Since this IP is private , they have no meaning outside the LAN. Under this configuration, despite being on the same subnet and being on the same network (since the first 20 bits are identical), UTM places each VM into is own isolated virtual network. This means that the two machines are on completely different networks and cannot see each other's ping requests. Layer 3 is impossible since ARP cannot succeed across two separate virtual switches.","title":"Part 3"},{"location":"courses/ap_networking/why_cant_these_two_computers_talk/#part-4","text":"To verify this claim, the VMs were pinged again. The Ubuntu 25.10 machines still could not ping each other, and the Ubuntu 22.04 machines still could not ping each other. This happens because each virtualization system (Apple Virtualization for the 25.10 VM and QEMU for the 22.04 VM) places its VM into a private, host-only network that exists only within each Mac mini. These private networks are completely independent from one another, so the VMs are not sharing the same Layer 2 network\u2014the layer where devices learn each other\u2019s MAC addresses. Since their OSI Layer 2s are independent, they can\u2019t discover each other, and communication fails before it even reaches the IP level (Layer 3). Surprisingly, the 25.10 VM can ping the 22.04 VM on the other computer (Apple Virtualization to QEMU or vice versa) because each Mac mini is able to route traffic between its own internal virtual networks before sending it over the Ethernet cable. This makes cross-platform pings work, while same-platform pings fail due to Layer-2 isolation. Pinging 22.04 from 25.10 Here, pinging my partner's 22.04 VM from my 25.10 VM was successful. Pinging 25.10 from 25.10 Here, pinging my partner's 25.10 VM from my 25.10 VM was unsuccessful. Pinging 25.10 from 22.04 Here, pinging my partner's 25.10 VM from my 22.04 VM was successful. Pinging 22.04 from 22.04 Here, pinging my partner's 22.04 VM from my 22.04 VM was unsuccessful.","title":"Part 4"},{"location":"courses/ap_networking/why_cant_these_two_computers_talk/#part-5","text":"Even though the two computers were connected with a working Ethernet cable, they could not communicate because each VM was in its own isolated VLAN, meaning that the VMs on different Mac minis were on different Layer 2 domains. The failure happened at OSI Layer 2 since the ARP could not find the correct MAC address associated with the IP address which prevented data from transferring. UTM prevents the VMs from connecting since it creates a virtual LAN for each Bridged VM to simulate an entire LAN and device. To allow for the VMs to easily connect, they should be placed in Shared (NAT) mode so they can piggyback off of the Mac mini's IP address and communicate with other devices on the LAN. In a real SOHO network, routers and switches prevent similar issues since they create multiple VLANs with different subnets and facilitate data transfer between the subnets to allow for many devices to be on the same LAN.","title":"Part 5"},{"location":"courses/civil_engineering/","text":"","title":"Overview"},{"location":"courses/civil_engineering/bridge_building_and_testing/","text":"","title":"Bridge Building and Testing"},{"location":"courses/civil_engineering/water_turbidity/","text":"","title":"Water Turbidity Testing"},{"location":"courses/civil_engineering/windmill_optimization/","text":"","title":"Windmill Design and Optimization"},{"location":"courses/data_analytics/","text":"","title":"Overview"},{"location":"courses/data_analytics/anova/","text":"","title":"ANOVA"},{"location":"courses/data_analytics/bootstrapping/","text":"","title":"Bootstrapping"},{"location":"courses/data_analytics/data_cleaning/","text":"","title":"Data Cleaning"},{"location":"courses/data_analytics/excel/","text":"","title":"Excel"},{"location":"courses/data_analytics/machine_learning/","text":"","title":"Machine Learning"},{"location":"courses/data_analytics/python/","text":"","title":"Python"},{"location":"courses/engineering_1/","text":"","title":"Overview"},{"location":"courses/engineering_1/3d_printing/","text":"","title":"Plane and Simple"},{"location":"courses/engineering_1/box/","text":"","title":"Thinking Out of the Box"},{"location":"courses/engineering_1/cold_solder/","text":"","title":"A Cold Solder"},{"location":"courses/engineering_1/final_project/","text":"","title":"Final Project"},{"location":"courses/engineering_1/grab_a_byte/","text":"","title":"Grab a Byte"},{"location":"courses/engineering_1/keychain/","text":"","title":"Easily Suede"},{"location":"courses/engineering_1/ohm_depot/","text":"","title":"The Ohm Depot"},{"location":"courses/engineering_2/","text":"","title":"Overview"},{"location":"courses/engineering_2/cutting_board/","text":"","title":"Board to be Wild"},{"location":"courses/engineering_2/final_project/","text":"","title":"Final Project"},{"location":"courses/engineering_2/fusion360/","text":"","title":"Deep Dive into Fusion 360"},{"location":"courses/engineering_2/milling_about/","text":"","title":"Milling About"},{"location":"courses/engineering_2/pressing_charges/","text":"","title":"Pressing Charges"},{"location":"courses/english_b2bx/projects/final_project/","text":"Final Project The final project for this course was to","title":"Final Project"},{"location":"courses/english_b2bx/projects/final_project/#final-project","text":"The final project for this course was to","title":"Final Project"},{"location":"courses/english_b2bx/projects/princess_bride/","text":"Princess Bride Essay Prompt In a 3\u20135 page essay, compare and contrast how Goldman\u2019s novel and Reiner\u2019s film explore the nature and purpose of storytelling. How does each version use the tools of its medium\u2014prose, metafiction, framing devices, performance, visual language\u2014to comment on the power (or limits) of fairy tales in shaping human experience? Link to Essay","title":"The Princess Bride Essay"},{"location":"courses/english_b2bx/projects/princess_bride/#princess-bride-essay","text":"","title":"Princess Bride Essay"},{"location":"courses/english_b2bx/projects/princess_bride/#prompt","text":"In a 3\u20135 page essay, compare and contrast how Goldman\u2019s novel and Reiner\u2019s film explore the nature and purpose of storytelling. How does each version use the tools of its medium\u2014prose, metafiction, framing devices, performance, visual language\u2014to comment on the power (or limits) of fairy tales in shaping human experience?","title":"Prompt"},{"location":"courses/english_b2bx/projects/princess_bride/#link-to-essay","text":"","title":"Link to Essay"},{"location":"courses/english_b2bx/projects/princess_bride_outline/","text":"Princess Bride Essay Outline","title":"The Princess Bride Outline"},{"location":"courses/english_b2bx/projects/princess_bride_outline/#princess-bride-essay-outline","text":"","title":"Princess Bride Essay Outline"},{"location":"courses/english_b2bx/projects/wiseguy_goodfellas_essay/","text":"","title":"Wiseguy/Goodfellas Essay"},{"location":"courses/english_b2bx/projects/writer_bio/","text":"Writer's Bio and Overview Table of Contents Writer Statement Wiseguy/Goodfellas Essay Wiseguy/Goodfellas Journal #1 Wiseguy/Goodfellas Journal #2 The Princess Bride Essay The Princess Bride Outline The Princess Bride Journal #1 The Princess Bride Journal #2 Final Project About Me Hello! My name is Raaj, and welcome to my Book to Box Office Portfolio, where I share all of the major assignments I have completed throughout the course. I am a senior in high school, and I intend to major in engineering, but at the time of writing, I do not know where I am attending university.","title":"Writer's Bio and Overview"},{"location":"courses/english_b2bx/projects/writer_bio/#writers-bio-and-overview","text":"","title":"Writer's Bio and Overview"},{"location":"courses/english_b2bx/projects/writer_bio/#table-of-contents","text":"Writer Statement Wiseguy/Goodfellas Essay Wiseguy/Goodfellas Journal #1 Wiseguy/Goodfellas Journal #2 The Princess Bride Essay The Princess Bride Outline The Princess Bride Journal #1 The Princess Bride Journal #2 Final Project","title":"Table of Contents"},{"location":"courses/english_b2bx/projects/writer_bio/#about-me","text":"Hello! My name is Raaj, and welcome to my Book to Box Office Portfolio, where I share all of the major assignments I have completed throughout the course. I am a senior in high school, and I intend to major in engineering, but at the time of writing, I do not know where I am attending university.","title":"About Me"},{"location":"courses/english_b2bx/projects/writer_statement/","text":"Writer's Statement","title":"Writer's Statement"},{"location":"courses/english_b2bx/projects/writer_statement/#writers-statement","text":"","title":"Writer's Statement"},{"location":"courses/english_b2bx/projects/journals/tpb_j1/","text":"The Princess Bride Journal #1 Prompt Now that you have completed reading Goldman\u2019s text and have a basic understanding of metafiction, consider how this device has influenced your reading of the story. Do you feel it drew you in, or did you find it distracting? Link to Response","title":"The Princess Bride Journal 1"},{"location":"courses/english_b2bx/projects/journals/tpb_j1/#the-princess-bride-journal-1","text":"","title":"The Princess Bride Journal #1"},{"location":"courses/english_b2bx/projects/journals/tpb_j1/#prompt","text":"Now that you have completed reading Goldman\u2019s text and have a basic understanding of metafiction, consider how this device has influenced your reading of the story. Do you feel it drew you in, or did you find it distracting?","title":"Prompt"},{"location":"courses/english_b2bx/projects/journals/tpb_j1/#link-to-response","text":"","title":"Link to Response"},{"location":"courses/english_b2bx/projects/journals/tpb_j2/","text":"The Princess Bride Journal #2 Prompt Before watching Rob Reiner\u2019s The Princess Bride, imagine how a filmmaker might bring William Goldman\u2019s layered story to the screen. You already know how the novel blurs fantasy and reality through the \u201cbook within a book\u201d structure, the interruptions from Goldman, and the exaggerated fairy-tale tone. For this journal, predict how the movie might handle those metafictional elements.How might a director visually or narratively show the story-within-a-story structure? What might the mise-en-sc\u00e8ne (the look and feel of the setting, costumes, and lighting) communicate about tone and genre? How do you imagine the performance style of characters like Westley, Buttercup, or Inigo will shape your understanding of them compared to how you imagined them while reading? What might be challenging or impossible to show on screen? Conclude with a prediction: what will the overall mood of the film be\u2014more like an epic adventure, a parody, or a fairy-tale romance? Link to Response","title":"The Princess Bride Journal 2"},{"location":"courses/english_b2bx/projects/journals/tpb_j2/#the-princess-bride-journal-2","text":"","title":"The Princess Bride Journal #2"},{"location":"courses/english_b2bx/projects/journals/tpb_j2/#prompt","text":"Before watching Rob Reiner\u2019s The Princess Bride, imagine how a filmmaker might bring William Goldman\u2019s layered story to the screen. You already know how the novel blurs fantasy and reality through the \u201cbook within a book\u201d structure, the interruptions from Goldman, and the exaggerated fairy-tale tone. For this journal, predict how the movie might handle those metafictional elements.How might a director visually or narratively show the story-within-a-story structure? What might the mise-en-sc\u00e8ne (the look and feel of the setting, costumes, and lighting) communicate about tone and genre? How do you imagine the performance style of characters like Westley, Buttercup, or Inigo will shape your understanding of them compared to how you imagined them while reading? What might be challenging or impossible to show on screen? Conclude with a prediction: what will the overall mood of the film be\u2014more like an epic adventure, a parody, or a fairy-tale romance?","title":"Prompt"},{"location":"courses/english_b2bx/projects/journals/tpb_j2/#link-to-response","text":"","title":"Link to Response"},{"location":"courses/english_b2bx/projects/journals/wggf_j1/","text":"Wiseguy / Goodfellas Journal #1 Prompt The prompt for this journal was: \"Think about Pileggi\u2019s concept of 'family,' and what he thinks that must mean for Hill and Hill\u2019s associates. What is your takeaway, based on what you have read thus far, and why is this important?\" Response Pileggi\u2019s concept of \u201cfamily\u201d is centered around trust and loyalty rather than blood. Although Henry Hill is not biologically related to any of the mobsters and is not even fully Italian-American, he views the mob as his family because he knows he can trust them and that they are loyal to each other. Henry doesn\u2019t have a strong relationship with his father, and instead of seeing him as a role model, he sees the mobsters as a role model due to their strong bond. Their relationship is created through loyalty, trust, and respect.","title":"Wiseguy/Goodfellas Journal 1"},{"location":"courses/english_b2bx/projects/journals/wggf_j1/#wiseguygoodfellas-journal-1","text":"","title":"Wiseguy/Goodfellas Journal #1"},{"location":"courses/english_b2bx/projects/journals/wggf_j1/#prompt","text":"The prompt for this journal was: \"Think about Pileggi\u2019s concept of 'family,' and what he thinks that must mean for Hill and Hill\u2019s associates. What is your takeaway, based on what you have read thus far, and why is this important?\"","title":"Prompt"},{"location":"courses/english_b2bx/projects/journals/wggf_j1/#response","text":"Pileggi\u2019s concept of \u201cfamily\u201d is centered around trust and loyalty rather than blood. Although Henry Hill is not biologically related to any of the mobsters and is not even fully Italian-American, he views the mob as his family because he knows he can trust them and that they are loyal to each other. Henry doesn\u2019t have a strong relationship with his father, and instead of seeing him as a role model, he sees the mobsters as a role model due to their strong bond. Their relationship is created through loyalty, trust, and respect.","title":"Response"},{"location":"courses/english_b2bx/projects/journals/wggf_j2/","text":"Wiseguy / Goodfellas Journal #2 Prompt The prompt for this journal was: \"Martin Scorsese once said, 'Authenticity in storytelling lies in capturing the truth of human imperfections and complexities.'\" In Wiseguy and Goodfellas, we encounter language that includes profanity, racial, ethnic, and religious slurs, and other offensive terms. These words are not used casually by the authors or filmmakers, but instead are part of depicting a specific world\u2014one rooted in crime, loyalty, power, and identity. These words can be uncomfortable, shocking, and harmful\u2014and that is part of why they matter. Write in first person about your response to this kind of language in the book and the film. Response I think that Nicholas Pileggi and Martin Scorsese chose to include harsh and vulgar language throughout the novel/film in order to immerse the reader/viewer in a truly authentic representation of Henry Hill\u2019s life rather than not representing aspects of mob life as they truly are. The profanities, slurs, and other vulgar language are used by the mobsters in real life, and to portray them as being well spoken and cautious with their words would be inaccurate and misleading, and neither Pileggi nor Scorsese would want that to happen. Also, the use of profane language by the mobsters showcases their power and how non-mobsters are scared to tell them anything. If anyone else was as loud and used language as profane as them in a public space like a restaurant, they would likely get kicked out. However, since the mobsters are so feared, nobody dares to say anything to them.","title":"Wiseguy/Goodfellas Journal 2"},{"location":"courses/english_b2bx/projects/journals/wggf_j2/#wiseguygoodfellas-journal-2","text":"","title":"Wiseguy/Goodfellas Journal #2"},{"location":"courses/english_b2bx/projects/journals/wggf_j2/#prompt","text":"The prompt for this journal was: \"Martin Scorsese once said, 'Authenticity in storytelling lies in capturing the truth of human imperfections and complexities.'\" In Wiseguy and Goodfellas, we encounter language that includes profanity, racial, ethnic, and religious slurs, and other offensive terms. These words are not used casually by the authors or filmmakers, but instead are part of depicting a specific world\u2014one rooted in crime, loyalty, power, and identity. These words can be uncomfortable, shocking, and harmful\u2014and that is part of why they matter. Write in first person about your response to this kind of language in the book and the film.","title":"Prompt"},{"location":"courses/english_b2bx/projects/journals/wggf_j2/#response","text":"I think that Nicholas Pileggi and Martin Scorsese chose to include harsh and vulgar language throughout the novel/film in order to immerse the reader/viewer in a truly authentic representation of Henry Hill\u2019s life rather than not representing aspects of mob life as they truly are. The profanities, slurs, and other vulgar language are used by the mobsters in real life, and to portray them as being well spoken and cautious with their words would be inaccurate and misleading, and neither Pileggi nor Scorsese would want that to happen. Also, the use of profane language by the mobsters showcases their power and how non-mobsters are scared to tell them anything. If anyone else was as loud and used language as profane as them in a public space like a restaurant, they would likely get kicked out. However, since the mobsters are so feared, nobody dares to say anything to them.","title":"Response"},{"location":"courses/senior_engineering/","text":"Honors Advanced Topics in Engineering Welcome to my documentation for the Advanced Topics in Engineering (Honors) course for the '25 - '26 school year. Projects Capstone Project Daily Log Pen Owl PCB Milling Topography Course Overview Advanced Topics in Engineering is a senior-only course where students can work on a capstone project throughout the whole year. In the class, I decided to work on a custom-designed drone with advanced AI capabilities (see more under capstone project page ).","title":"Overview"},{"location":"courses/senior_engineering/#honors-advanced-topics-in-engineering","text":"Welcome to my documentation for the Advanced Topics in Engineering (Honors) course for the '25 - '26 school year.","title":"Honors Advanced Topics in Engineering"},{"location":"courses/senior_engineering/#projects","text":"Capstone Project Daily Log Pen Owl PCB Milling Topography","title":"Projects"},{"location":"courses/senior_engineering/#course-overview","text":"Advanced Topics in Engineering is a senior-only course where students can work on a capstone project throughout the whole year. In the class, I decided to work on a custom-designed drone with advanced AI capabilities (see more under capstone project page ).","title":"Course Overview"},{"location":"courses/senior_engineering/capstone/","text":"Capstone For the majority of the year, I worked towards completing my capstone project: a fully custom-designed 3D printed drone with custom electronics and AI-powered object classification, pose tracking, facial recognition, and more. Navigation Build Goals From the start, I wanted my drone to do the following: Have a long battery life Be able to record video in at least 1080p Have high processing power for AI-powered tasks Custom design as much as possible Part Selection The first step of making the drone was to figure out what parts I wanted to use. I had to consider many questions, such as: - What purpose will the drone be made for (racing, cinematography, autonomous missions, etc.)? - How big will the drone be? - What firmware will it use (ArduPilot, Betaflight, etc.)? After considering many factors, I decided that I wanted to prioritize having a long flight time and powerful computers for autonomous missions. Those two choices meant that I would have to use a very large battery (meaning I would need a large drone to match), and that I need to use ArduPilot due to its support for GPS as well as a companion computer (more on this under the firmware section). I did some more research on drones, and I knew I wanted to build a drone with 7\" propellers (I ended up changing to 9\"), since it would allow me to support a chassis large enough to house advanced electronics and a very large battery. For that size, I saw that 6S (6 cell) LiPo batteries were most popular in the community due to their high voltage (up to 25.2V) which allows for the drone to be more efficient. I settled on the Ovonic 4500 mAh 6S LiPo 100C XT90 Battery , since it had a very large capacity (99.9 wh!), a very high discharge rate, a bulky XT90 connector to allow for high power draw, and it was 6S. After choosing the battery, I had to choose the motors, ESC, and electronic components. Since I chose a high-voltage battery, I went with relatively low 1050KV motors made by BrotherHobby. I saw good reviews about these online and they are made of high quality materials, which is why I chose them. The ESC is also made by BrotherHobby. It's a 4 in 1 ESC with support up to a 6S battery and can provide 65A per motor. I chose it since it had good reviews and was by far the cheapest ESC from a legitimate brand that could provide 65A per motor. For the electronic components, I needed to ensure that they could support ArduPilot, and that I chose a comprehensive suite of sensors to allow for autonomous flight. For the MCU, I chose the STM32F767ZIT6 since it is powerful and has ample I/O for all of the sensors. For the sensors, I chose the ICM 29048 (IMU), LIS2MDL (magnetometer), LPS22HB (barometer), and NEO-M9N (GPS). I found boards from Adafruit at competitive prices that featured the sensors, power management systems (capacitors, step downs, resistors, etc.), along with a STEMMA QT connector for easy I2C connectivity. Insted of soldering the extremely tiny sensors directly to the board, I decided to go with these Adafruit boards to reduce complexity and increase reliability. The NEO M9N is large enough where I can solder it myself, so I bought the bare chip and added the power management system for the NEO M9N directly to the board (more on this under the Electronics Architecture section). Additionally, I wanted a separate computer to drive the video recording and AI aspect. From the start, I knew that I wanted to use a Raspberry Pi 5 with the Raspberry Pi AI Hat+, capable of 26 TOPS. This combination allows for very powerful edge AI capabilities with high power efficiency, meaning that the drone can do real-time AI calculations in the air without drawing too much power. After all this consideration, I ended up with a very high end parts list that would create a drone with very powerful capabilities. The next step was to design the chassis . Chassis Design The first component I designed was the chassis. Originally, I wanted to use the Source One , an open source drone chassis supporting up to 7\" props. However, in its standard form, it had nowhere near enough space to fit my large battery, a Raspberry Pi 5, a camera, and all of the components for the flight controller (the board was not designed at this point). So, I downloaded the STEP files into Onshape and made significant changes to allow for the battery to fit. However, after printing it out, I realized that the drone is far too heavy and that the chassis is too complicated with too many parts. Additionally, I realized that for the weight of the components, I would need bigger propellers. I looked at the website for the motors, and it said that they support up to 9\" propellers, so I decided to switch. Instead of modifying the chassis more to be stiffer and to support bigger props, I decided to start fresh with a clean sheet design. I wanted to keep the design as simple as possible and to make sure that it would support the larger propellers. PCB Design Electronics Architecture Firmware Assembly TBD Testing and Calibration TBD Next Steps TBD","title":"Capstone Project"},{"location":"courses/senior_engineering/capstone/#capstone","text":"For the majority of the year, I worked towards completing my capstone project: a fully custom-designed 3D printed drone with custom electronics and AI-powered object classification, pose tracking, facial recognition, and more.","title":"Capstone"},{"location":"courses/senior_engineering/capstone/#navigation","text":"","title":"Navigation"},{"location":"courses/senior_engineering/capstone/#build-goals","text":"From the start, I wanted my drone to do the following: Have a long battery life Be able to record video in at least 1080p Have high processing power for AI-powered tasks Custom design as much as possible","title":"Build Goals"},{"location":"courses/senior_engineering/capstone/#part-selection","text":"The first step of making the drone was to figure out what parts I wanted to use. I had to consider many questions, such as: - What purpose will the drone be made for (racing, cinematography, autonomous missions, etc.)? - How big will the drone be? - What firmware will it use (ArduPilot, Betaflight, etc.)? After considering many factors, I decided that I wanted to prioritize having a long flight time and powerful computers for autonomous missions. Those two choices meant that I would have to use a very large battery (meaning I would need a large drone to match), and that I need to use ArduPilot due to its support for GPS as well as a companion computer (more on this under the firmware section). I did some more research on drones, and I knew I wanted to build a drone with 7\" propellers (I ended up changing to 9\"), since it would allow me to support a chassis large enough to house advanced electronics and a very large battery. For that size, I saw that 6S (6 cell) LiPo batteries were most popular in the community due to their high voltage (up to 25.2V) which allows for the drone to be more efficient. I settled on the Ovonic 4500 mAh 6S LiPo 100C XT90 Battery , since it had a very large capacity (99.9 wh!), a very high discharge rate, a bulky XT90 connector to allow for high power draw, and it was 6S. After choosing the battery, I had to choose the motors, ESC, and electronic components. Since I chose a high-voltage battery, I went with relatively low 1050KV motors made by BrotherHobby. I saw good reviews about these online and they are made of high quality materials, which is why I chose them. The ESC is also made by BrotherHobby. It's a 4 in 1 ESC with support up to a 6S battery and can provide 65A per motor. I chose it since it had good reviews and was by far the cheapest ESC from a legitimate brand that could provide 65A per motor. For the electronic components, I needed to ensure that they could support ArduPilot, and that I chose a comprehensive suite of sensors to allow for autonomous flight. For the MCU, I chose the STM32F767ZIT6 since it is powerful and has ample I/O for all of the sensors. For the sensors, I chose the ICM 29048 (IMU), LIS2MDL (magnetometer), LPS22HB (barometer), and NEO-M9N (GPS). I found boards from Adafruit at competitive prices that featured the sensors, power management systems (capacitors, step downs, resistors, etc.), along with a STEMMA QT connector for easy I2C connectivity. Insted of soldering the extremely tiny sensors directly to the board, I decided to go with these Adafruit boards to reduce complexity and increase reliability. The NEO M9N is large enough where I can solder it myself, so I bought the bare chip and added the power management system for the NEO M9N directly to the board (more on this under the Electronics Architecture section). Additionally, I wanted a separate computer to drive the video recording and AI aspect. From the start, I knew that I wanted to use a Raspberry Pi 5 with the Raspberry Pi AI Hat+, capable of 26 TOPS. This combination allows for very powerful edge AI capabilities with high power efficiency, meaning that the drone can do real-time AI calculations in the air without drawing too much power. After all this consideration, I ended up with a very high end parts list that would create a drone with very powerful capabilities. The next step was to design the chassis .","title":"Part Selection"},{"location":"courses/senior_engineering/capstone/#chassis-design","text":"The first component I designed was the chassis. Originally, I wanted to use the Source One , an open source drone chassis supporting up to 7\" props. However, in its standard form, it had nowhere near enough space to fit my large battery, a Raspberry Pi 5, a camera, and all of the components for the flight controller (the board was not designed at this point). So, I downloaded the STEP files into Onshape and made significant changes to allow for the battery to fit. However, after printing it out, I realized that the drone is far too heavy and that the chassis is too complicated with too many parts. Additionally, I realized that for the weight of the components, I would need bigger propellers. I looked at the website for the motors, and it said that they support up to 9\" propellers, so I decided to switch. Instead of modifying the chassis more to be stiffer and to support bigger props, I decided to start fresh with a clean sheet design. I wanted to keep the design as simple as possible and to make sure that it would support the larger propellers.","title":"Chassis Design"},{"location":"courses/senior_engineering/capstone/#pcb-design","text":"","title":"PCB Design"},{"location":"courses/senior_engineering/capstone/#electronics-architecture","text":"","title":"Electronics Architecture"},{"location":"courses/senior_engineering/capstone/#firmware","text":"","title":"Firmware"},{"location":"courses/senior_engineering/capstone/#assembly","text":"TBD","title":"Assembly"},{"location":"courses/senior_engineering/capstone/#testing-and-calibration","text":"TBD","title":"Testing and Calibration"},{"location":"courses/senior_engineering/capstone/#next-steps","text":"TBD","title":"Next Steps"},{"location":"courses/senior_engineering/daily_log/","text":"Honors Advanced Topics in Engineering Daily Log Welcome to my daily log for engineering! Here, I will outline what I do every day in class. Navigation September October November December January February March April May June September 09.03.2025 Today, I continued work on my wooden pen. I took my two blocks of wood and turned them on a lathe. I took a chisel and removed material until the blocks were cylindrical, and when they got to the desired thickness, I used fine grit sandpaper to smooth the two blocks. Now, I have the two wooden components for my pen ready, and now, I can assemble the pen and do all of the finishing touches next class. 09.04.2025 Today, I finished my pen. I started off by using the pen press to: Press the pen tip into the bottom end of the lower barrel Press the ink chamber into the top end of the lower barrel Press the clip assembly into the top end of the upper barrel Once I pressed these components, I could assemble the main sections together. I screwed the ink refill into the ink chamber, slid the ring onto the chamber above the lower barrel, and slid the upper barrel above the ring. Once I did that, my pen was done, and it wrote super well, along with looking very cool. 09.05.2025 Today, I did some research and work on my capstone project. My goal is to assemble the board as soon as possible, so today, I practiced soldering random components to practice boards to prepare for soldering intricate components on the board. 09.08.2025 Today, I researched more about how to configure ArduPilot for a custom board. I decided that I would have to compile ArduPilot from the source code with a custom hwdef.dat file for my specific hardware configuration. From the start, I knew that I wanted to do as much as possible in VSCode, as I am very familiar with it. After doing some researching, I discovered that ArduPilot provides a VSCode integration which allows you to configure and flash ArduPilot directly from VSCode. All I have to do is make a custom board definition (hwdef.dat) for my specific hardware detailing what components I have and how they are connected, then I can use that board definition in the ArduPilot configurator to flash it. Conveniently, the extension has a built-in tool to make sure that your machine's ArduPilot environment has all of the necessary tools to build and flash the software. I had to install a lot of stuff, such as: Python MAVLink ( pip install pymavlink ) MAV Proxy ( pip install mavproxy ) J-Link ( through SEGGER application ) In addition, I had to create symlinks between ccache and g++, gcc, arm-none-eabi-gcc, and arm-none-eabi-g++. I did so by adding this line to my ZSH profile (~/.zshrc: export PATH=\"/opt/homebrew/opt/ccache/libexec:$PATH\" , then verifying that the installations of ccache, gcc, g++, arm-none-eabi-gcc, and arm-none-eabi-g++ were correct by typing which + the name of the toolchain. I made sure that they were all installed, but I was unclear as to what exactly their purpose was. I did some digging and these were the results I found: GCC: GNU Compiler Collection's C compiler G++: GNU Compiler Collection's C++ compiler arm-none-eabi-gcc: Compiles .c code into machine code for the ARM Cortex-M family of CPUs arm-none-eabi-gcc: Compiles .cpp code into machine code for the ARM Cortex-M family of CPUs Together, these toolchains work together to compile the .c and .cpp files that make up the ArduPilot source code in order to create machine code for the ARM Cortex-M CPU that powers my flight controller (STM32F767ZIT6). { width=400 } 09.09.2025 Today, I did some more research on how to set up the software. I read the ArduPilot documentation, STMicroelectronics documentation on their various apps such as STM32CubeMX, STM32CubeIDE, etc. They have many apps, so it was confusing trying to figure out exactly what purpose each app had and whether or not I needed them. The only STMicroelectronics app that I will need is the STM32CubeProgrammer which will allow me to flash the ArduPilot software to the STM32 with an ST-Link via Serial Wire Debug. 09.15.2025 I dedicated today to working on my GitHub documentation. Mr. Dubick taught the class on how to use GitHub, and I worked on refining format and writing out some pages on Github. 09.16.2025 Today, I printed out the chassis for my drone. Although I intend to make my final parts out of either PETG or ABS with 50-80% infill, I printed this part out of PLA since all of the printers in the lab are loaded with PLA, and since it is easier to work with. To save time, I used 15% infill, and to support overhangs, I used tree supports. After printing the parts out, I confirmed that my battery would fit in the space. The battery was a perfect fit for the space, although I was a little worried about not having enough clearance for screw heads. Although there is space in the CAD mockup, I may inset the screw heads to allow for more room. The main issues with the parts had to do with durability. The parts have long cylinders for screws to slot into and clamp down on the chassis. Although the screws will add a lot of support, the cylinders are brittle and break easily. To fix this issue, I will add fillets to the base of the cylinders. Also, the plates are pretty thin, so I will have to thicken them by 1-2 in order to reduce flexing. 09.17 - 09.24.2025 In this period of time, I continued work on the 3D CAD design of the drone chassis. I made many small changes in order to increase interior volume, reduce weight, increase strength, and cut down on parts. However, I never printed it out since I was not happy with the final result (and eventually I made a new design from the ground up, more on this later). 09.25 - 09.29.2025 I worked on a mini project to practice soldering. The project is an owl with LEDs which activate by touching a capacitive sensor on the front of the board. While the through hole components were very easy to solder, the 2 ICs on the board with small pin pitches were relatively difficult to solder. 09.30.2025 Today, I finished soldering all of the LEDs, then tested the board. Unfortunately, only the outside ring of lights turned on and the \"eyes\" did not work. This is due to an issue with an IC. I'm not sure exactly how I will fix it, but I will likely have to de-solder the chip, clean the IC, clean the pads with a solder wick and flux, then re-solder it. October 10.01.2025 Today, I started setting up my Raspberry Pi 5 with the AI Hat+. I installed the latest release of Pi OS Bookworm onto a microSD card, then plugged the Pi into a monitor to configure it. I then followed this guide to set up the Pi. I started off by setting up PCIe Gen 3.0 by typing sudo raspi-config to bring up the Raspi-Config CLI tool, then enabling PCIe Gen 3.0 speeds under Advanced Options. After that, I ran sudo apt install hailo-all in order to install the following: Hailo kernel device driver and firmware (allows Pi OS to communicate directly with the Hailo-8 NPU) HailoRT middleware software (runtime that handles tasks such as loading the AI model onto the chip and managing inference execution) Hailo Tappas core post-processing libraries (computer vision libraries that handle post-processing tasks such as decoding bounding boxes, converting raw data into masks, and mapping points onto human body parts) rpicam-apps Hailo post-processing software demo stages (Pi OS's camera stack allowing for video recording, image capturing, live feeds, etc.) 10.07.2025 Today, I printed out the bottom plate for my drone, which is where most of the electronics are mounted. printed base plate v1 and front top plate, installed raspberry pi: - holes too big for camera not gripping screws - stands too weak to support camera properly - npu chip slightly pressing on camera --> heating up connector as well as forcing the plate to bend very slightly - redesigned in cad to strengthen camera plate and recess pi mounts by 0.25 mm to get rid of flexing issue 10.08.2025 Today, I focused on getting the AI working on the Raspberry Pi. Up to this point, I was using the pre-existing hailo_inf_fl.json file in Raspberry Pi OS that uses 3 models: yolov8 (object detection/classification), yolov8 pose (pose detection), and scrfd (facial tracking). While the yolov8 models were correctly compiled for the Hailo 8 (the NPU I am using), the scrfd model was compiled for the Hailo 8L NPU, the lower performance version of the Hailo 8. This returned a warning message that I will likely experience lower performance than expected, since the model was compiled for the incorrect architecture. I wanted to get rid of this error, so I looked at the Hailo Model Zoo Github and looked through the models until I found the link for scrfd compiled for the Hailo 8 NPU. Once I found the correct .hef file, I looked at the hailo_inf_fl.json file to see where the current scrfd file is located. It was located at /usr/bin/rpi-camera-assets/scrfd_2.5g.hef, so I deleted it and copied the new scrfd_2.5g.hef file to the same location to ensure that the json file would work as expected and know where to locate the model. When I re-ran rpicam-hello -t 0 --rotation 180 --post-process-file /usr/share/rpi-camera-assets/hailo_inf_fl.json , I no longer got the warning that the model is compiled for the wrong NPU. Although it would have previously worked fine, I wanted to ensure that everything was as optimized as possible to ensure maximum performance and the lowest power consumption possible. printed new base plate, camera is more secure and no longer any flexing issues may add ribs to reduce overall flexing/lack of structural rigidity, but will wait to use petg since it will flex less in petg w/ 67% gyroid infill 10.09.2025 did some refining in Onshape to finish up the back top part (added standoffs to sit flush with base plate) as well as searched for models to use. also fixed owl project by taking a fine tip solder and melting the solder of the ics and melting the excess solder paste that was likely bridging some pads on the small ic. doing this made the owl work perfectly 10.10.2025 printed back top part stress tested new models to test overheating/throttling/melting the chassis (it got hot, but not hot enough to cause damage) by running model the whole class (45 minutes) and periodically checking to see if it was still at 30fps or if it was dropping frames. it didn't drop frames and was able to run continuously mostly thanks to the active cooler turning on its fan and blowing out the hot air. And, when the drone is flying, the pi is at the very front and will get a lot of cool fresh air to cool it down, so even in the worst case scenario, it doesn't throttle. insert video here 10.13.2025 Today, I started off with refining my original chassis design even more. However, I was not very happy with the level of complexity of the parts They were too weak, complicated, and I decided to start fresh by designing a brand new chassis from the ground up. While the original chassis was a heavily modified version of the Source One open-source drone chassis, the new design was my own from the start and was designed around my specific hardware. My overall goals for the new chassis were to increase structural rigidity, reduce the amount of parts, increase 10.16.2025 I dedicated today to working on my portfolio and catching up on my daily log. I also created pages for the smaller projects such as the pen and owl. 10.17.2025 Today, Mr. Dubick taught us how to use Jekyll and GitHub pages. Although I currently use MKDocs for my portfolio, it's useful to know how to use Jekyll in case I want to November December January February March April May June","title":"Daily Log"},{"location":"courses/senior_engineering/daily_log/#honors-advanced-topics-in-engineering-daily-log","text":"Welcome to my daily log for engineering! Here, I will outline what I do every day in class.","title":"Honors Advanced Topics in Engineering Daily Log"},{"location":"courses/senior_engineering/daily_log/#navigation","text":"September October November December January February March April May June","title":"Navigation"},{"location":"courses/senior_engineering/daily_log/#september","text":"","title":"September"},{"location":"courses/senior_engineering/daily_log/#09032025","text":"Today, I continued work on my wooden pen. I took my two blocks of wood and turned them on a lathe. I took a chisel and removed material until the blocks were cylindrical, and when they got to the desired thickness, I used fine grit sandpaper to smooth the two blocks. Now, I have the two wooden components for my pen ready, and now, I can assemble the pen and do all of the finishing touches next class.","title":"09.03.2025"},{"location":"courses/senior_engineering/daily_log/#09042025","text":"Today, I finished my pen. I started off by using the pen press to: Press the pen tip into the bottom end of the lower barrel Press the ink chamber into the top end of the lower barrel Press the clip assembly into the top end of the upper barrel Once I pressed these components, I could assemble the main sections together. I screwed the ink refill into the ink chamber, slid the ring onto the chamber above the lower barrel, and slid the upper barrel above the ring. Once I did that, my pen was done, and it wrote super well, along with looking very cool.","title":"09.04.2025"},{"location":"courses/senior_engineering/daily_log/#09052025","text":"Today, I did some research and work on my capstone project. My goal is to assemble the board as soon as possible, so today, I practiced soldering random components to practice boards to prepare for soldering intricate components on the board.","title":"09.05.2025"},{"location":"courses/senior_engineering/daily_log/#09082025","text":"Today, I researched more about how to configure ArduPilot for a custom board. I decided that I would have to compile ArduPilot from the source code with a custom hwdef.dat file for my specific hardware configuration. From the start, I knew that I wanted to do as much as possible in VSCode, as I am very familiar with it. After doing some researching, I discovered that ArduPilot provides a VSCode integration which allows you to configure and flash ArduPilot directly from VSCode. All I have to do is make a custom board definition (hwdef.dat) for my specific hardware detailing what components I have and how they are connected, then I can use that board definition in the ArduPilot configurator to flash it. Conveniently, the extension has a built-in tool to make sure that your machine's ArduPilot environment has all of the necessary tools to build and flash the software. I had to install a lot of stuff, such as: Python MAVLink ( pip install pymavlink ) MAV Proxy ( pip install mavproxy ) J-Link ( through SEGGER application ) In addition, I had to create symlinks between ccache and g++, gcc, arm-none-eabi-gcc, and arm-none-eabi-g++. I did so by adding this line to my ZSH profile (~/.zshrc: export PATH=\"/opt/homebrew/opt/ccache/libexec:$PATH\" , then verifying that the installations of ccache, gcc, g++, arm-none-eabi-gcc, and arm-none-eabi-g++ were correct by typing which + the name of the toolchain. I made sure that they were all installed, but I was unclear as to what exactly their purpose was. I did some digging and these were the results I found: GCC: GNU Compiler Collection's C compiler G++: GNU Compiler Collection's C++ compiler arm-none-eabi-gcc: Compiles .c code into machine code for the ARM Cortex-M family of CPUs arm-none-eabi-gcc: Compiles .cpp code into machine code for the ARM Cortex-M family of CPUs Together, these toolchains work together to compile the .c and .cpp files that make up the ArduPilot source code in order to create machine code for the ARM Cortex-M CPU that powers my flight controller (STM32F767ZIT6). { width=400 }","title":"09.08.2025"},{"location":"courses/senior_engineering/daily_log/#09092025","text":"Today, I did some more research on how to set up the software. I read the ArduPilot documentation, STMicroelectronics documentation on their various apps such as STM32CubeMX, STM32CubeIDE, etc. They have many apps, so it was confusing trying to figure out exactly what purpose each app had and whether or not I needed them. The only STMicroelectronics app that I will need is the STM32CubeProgrammer which will allow me to flash the ArduPilot software to the STM32 with an ST-Link via Serial Wire Debug.","title":"09.09.2025"},{"location":"courses/senior_engineering/daily_log/#09152025","text":"I dedicated today to working on my GitHub documentation. Mr. Dubick taught the class on how to use GitHub, and I worked on refining format and writing out some pages on Github.","title":"09.15.2025"},{"location":"courses/senior_engineering/daily_log/#09162025","text":"Today, I printed out the chassis for my drone. Although I intend to make my final parts out of either PETG or ABS with 50-80% infill, I printed this part out of PLA since all of the printers in the lab are loaded with PLA, and since it is easier to work with. To save time, I used 15% infill, and to support overhangs, I used tree supports. After printing the parts out, I confirmed that my battery would fit in the space. The battery was a perfect fit for the space, although I was a little worried about not having enough clearance for screw heads. Although there is space in the CAD mockup, I may inset the screw heads to allow for more room. The main issues with the parts had to do with durability. The parts have long cylinders for screws to slot into and clamp down on the chassis. Although the screws will add a lot of support, the cylinders are brittle and break easily. To fix this issue, I will add fillets to the base of the cylinders. Also, the plates are pretty thin, so I will have to thicken them by 1-2 in order to reduce flexing.","title":"09.16.2025"},{"location":"courses/senior_engineering/daily_log/#0917-09242025","text":"In this period of time, I continued work on the 3D CAD design of the drone chassis. I made many small changes in order to increase interior volume, reduce weight, increase strength, and cut down on parts. However, I never printed it out since I was not happy with the final result (and eventually I made a new design from the ground up, more on this later).","title":"09.17 - 09.24.2025"},{"location":"courses/senior_engineering/daily_log/#0925-09292025","text":"I worked on a mini project to practice soldering. The project is an owl with LEDs which activate by touching a capacitive sensor on the front of the board. While the through hole components were very easy to solder, the 2 ICs on the board with small pin pitches were relatively difficult to solder.","title":"09.25 - 09.29.2025"},{"location":"courses/senior_engineering/daily_log/#09302025","text":"Today, I finished soldering all of the LEDs, then tested the board. Unfortunately, only the outside ring of lights turned on and the \"eyes\" did not work. This is due to an issue with an IC. I'm not sure exactly how I will fix it, but I will likely have to de-solder the chip, clean the IC, clean the pads with a solder wick and flux, then re-solder it.","title":"09.30.2025"},{"location":"courses/senior_engineering/daily_log/#october","text":"","title":"October"},{"location":"courses/senior_engineering/daily_log/#10012025","text":"Today, I started setting up my Raspberry Pi 5 with the AI Hat+. I installed the latest release of Pi OS Bookworm onto a microSD card, then plugged the Pi into a monitor to configure it. I then followed this guide to set up the Pi. I started off by setting up PCIe Gen 3.0 by typing sudo raspi-config to bring up the Raspi-Config CLI tool, then enabling PCIe Gen 3.0 speeds under Advanced Options. After that, I ran sudo apt install hailo-all in order to install the following: Hailo kernel device driver and firmware (allows Pi OS to communicate directly with the Hailo-8 NPU) HailoRT middleware software (runtime that handles tasks such as loading the AI model onto the chip and managing inference execution) Hailo Tappas core post-processing libraries (computer vision libraries that handle post-processing tasks such as decoding bounding boxes, converting raw data into masks, and mapping points onto human body parts) rpicam-apps Hailo post-processing software demo stages (Pi OS's camera stack allowing for video recording, image capturing, live feeds, etc.)","title":"10.01.2025"},{"location":"courses/senior_engineering/daily_log/#10072025","text":"Today, I printed out the bottom plate for my drone, which is where most of the electronics are mounted. printed base plate v1 and front top plate, installed raspberry pi: - holes too big for camera not gripping screws - stands too weak to support camera properly - npu chip slightly pressing on camera --> heating up connector as well as forcing the plate to bend very slightly - redesigned in cad to strengthen camera plate and recess pi mounts by 0.25 mm to get rid of flexing issue","title":"10.07.2025"},{"location":"courses/senior_engineering/daily_log/#10082025","text":"Today, I focused on getting the AI working on the Raspberry Pi. Up to this point, I was using the pre-existing hailo_inf_fl.json file in Raspberry Pi OS that uses 3 models: yolov8 (object detection/classification), yolov8 pose (pose detection), and scrfd (facial tracking). While the yolov8 models were correctly compiled for the Hailo 8 (the NPU I am using), the scrfd model was compiled for the Hailo 8L NPU, the lower performance version of the Hailo 8. This returned a warning message that I will likely experience lower performance than expected, since the model was compiled for the incorrect architecture. I wanted to get rid of this error, so I looked at the Hailo Model Zoo Github and looked through the models until I found the link for scrfd compiled for the Hailo 8 NPU. Once I found the correct .hef file, I looked at the hailo_inf_fl.json file to see where the current scrfd file is located. It was located at /usr/bin/rpi-camera-assets/scrfd_2.5g.hef, so I deleted it and copied the new scrfd_2.5g.hef file to the same location to ensure that the json file would work as expected and know where to locate the model. When I re-ran rpicam-hello -t 0 --rotation 180 --post-process-file /usr/share/rpi-camera-assets/hailo_inf_fl.json , I no longer got the warning that the model is compiled for the wrong NPU. Although it would have previously worked fine, I wanted to ensure that everything was as optimized as possible to ensure maximum performance and the lowest power consumption possible. printed new base plate, camera is more secure and no longer any flexing issues may add ribs to reduce overall flexing/lack of structural rigidity, but will wait to use petg since it will flex less in petg w/ 67% gyroid infill","title":"10.08.2025"},{"location":"courses/senior_engineering/daily_log/#10092025","text":"did some refining in Onshape to finish up the back top part (added standoffs to sit flush with base plate) as well as searched for models to use. also fixed owl project by taking a fine tip solder and melting the solder of the ics and melting the excess solder paste that was likely bridging some pads on the small ic. doing this made the owl work perfectly","title":"10.09.2025"},{"location":"courses/senior_engineering/daily_log/#10102025","text":"printed back top part stress tested new models to test overheating/throttling/melting the chassis (it got hot, but not hot enough to cause damage) by running model the whole class (45 minutes) and periodically checking to see if it was still at 30fps or if it was dropping frames. it didn't drop frames and was able to run continuously mostly thanks to the active cooler turning on its fan and blowing out the hot air. And, when the drone is flying, the pi is at the very front and will get a lot of cool fresh air to cool it down, so even in the worst case scenario, it doesn't throttle. insert video here","title":"10.10.2025"},{"location":"courses/senior_engineering/daily_log/#10132025","text":"Today, I started off with refining my original chassis design even more. However, I was not very happy with the level of complexity of the parts They were too weak, complicated, and I decided to start fresh by designing a brand new chassis from the ground up. While the original chassis was a heavily modified version of the Source One open-source drone chassis, the new design was my own from the start and was designed around my specific hardware. My overall goals for the new chassis were to increase structural rigidity, reduce the amount of parts, increase","title":"10.13.2025"},{"location":"courses/senior_engineering/daily_log/#10162025","text":"I dedicated today to working on my portfolio and catching up on my daily log. I also created pages for the smaller projects such as the pen and owl.","title":"10.16.2025"},{"location":"courses/senior_engineering/daily_log/#10172025","text":"Today, Mr. Dubick taught us how to use Jekyll and GitHub pages. Although I currently use MKDocs for my portfolio, it's useful to know how to use Jekyll in case I want to","title":"10.17.2025"},{"location":"courses/senior_engineering/daily_log/#november","text":"","title":"November"},{"location":"courses/senior_engineering/daily_log/#december","text":"","title":"December"},{"location":"courses/senior_engineering/daily_log/#january","text":"","title":"January"},{"location":"courses/senior_engineering/daily_log/#february","text":"","title":"February"},{"location":"courses/senior_engineering/daily_log/#march","text":"","title":"March"},{"location":"courses/senior_engineering/daily_log/#april","text":"","title":"April"},{"location":"courses/senior_engineering/daily_log/#may","text":"","title":"May"},{"location":"courses/senior_engineering/daily_log/#june","text":"","title":"June"},{"location":"courses/senior_engineering/owl/","text":"Owl Project In order to practice soldering, I completed a mini-project where I soldered LEDs and other components to a board in the shape of an owl. This project featured both through-hole and surface-mount soldering, which was useful for refreshing my soldering skills. Manufacturing Since I am more experienced with through hole soldering and find it easier, I started the project off with soldering all of the through hole components, such as resistors, transistors, capacitors, LEDs, and the USB-C port. After I successfully soldered all of the through hole components, I had to solder the 2 surface mount components. It was quite challenging to do so, and I used solder paste and a fine tip soldering iron to solder the components. Unfortunately, after soldering the ICs on, only the outer ring of LEDs lit up; the inner LEDs did not work. I suspected that there was a bridge between two pads of the IC due to unmelted solder paste, so I took a fine tip soldering iron, ran it between all of the legs of the ICs, and melted all excess solder paste. After doing so, all of the LEDs lit up and the board functioned perfectly. What I Learned From doing this project, I learned a lot about surface mount soldering. I learned use solder paste to solder SMD components, and I learned how to troubleshoot surface mount components that aren't working as expected. Practicing will help me eventually surface the intricate SMD components to the flight controller board of my drone.","title":"Owl"},{"location":"courses/senior_engineering/owl/#owl-project","text":"In order to practice soldering, I completed a mini-project where I soldered LEDs and other components to a board in the shape of an owl. This project featured both through-hole and surface-mount soldering, which was useful for refreshing my soldering skills.","title":"Owl Project"},{"location":"courses/senior_engineering/owl/#manufacturing","text":"Since I am more experienced with through hole soldering and find it easier, I started the project off with soldering all of the through hole components, such as resistors, transistors, capacitors, LEDs, and the USB-C port. After I successfully soldered all of the through hole components, I had to solder the 2 surface mount components. It was quite challenging to do so, and I used solder paste and a fine tip soldering iron to solder the components. Unfortunately, after soldering the ICs on, only the outer ring of LEDs lit up; the inner LEDs did not work. I suspected that there was a bridge between two pads of the IC due to unmelted solder paste, so I took a fine tip soldering iron, ran it between all of the legs of the ICs, and melted all excess solder paste. After doing so, all of the LEDs lit up and the board functioned perfectly.","title":"Manufacturing"},{"location":"courses/senior_engineering/owl/#what-i-learned","text":"From doing this project, I learned a lot about surface mount soldering. I learned use solder paste to solder SMD components, and I learned how to troubleshoot surface mount components that aren't working as expected. Practicing will help me eventually surface the intricate SMD components to the flight controller board of my drone.","title":"What I Learned"},{"location":"courses/senior_engineering/pcb_milling/","text":"PCB Milling In this project, I used MakerCAM and the CNC machines in the Fab Lab in order to create GCode for a board and manufacture it when given gerber files. Software and Hardware Used: Carvera Desktop CNC Machine : Machine that milled out the board Makera CAM : Software to generate GCode from gerber and drill files Carvera Controller : Software to control the CNC Machine Quick Downloads GCode for Board KiCAD Files Workflow for Milling a Board with the Carvera Desktop CNC Machine Open Makera CAM and open a 3-Axis project Edit the material settings to be a 127mm x 101mm x 1.7mm PCB Import the files with File \u21d2 Import PCB Select all of the imported vectors and click the letter M to move them. Set the origin to the bottom left corner and type in the coordinates (6,6) to move the bottom left corner of the vectors to (6,6) in the workspace. Create a 2D Pocket cut toolpath by selecting only the edge.cuts and f.cu vectors, then clicking 2D Path \u21d2 2D Pocket. Edit the 2D Pocket settings with the following settings: End depth: 0.05mm Tools: 0.8mm Corn and 0.2mm*30\u00b0 Engraving (metal) Ensure that for these tools, the material selected is \"PCB\" Once these settings are applied, select \"calculate,\" which should make a toolpath appear on the screen. If it looks right, then hide it. If not, re-do the process. Hide everything except for the .drl files. Then, select all of the holes and click 2D Path \u21d2 2D Drilling. Edit the 2D Drilling settings with the following settings: End depth: 1.70mm Tools: 0.8mm Corn (ensure material is set to \"PCB\") Once these settings are applied, select \"calculate,\" which should make a toolpath appear on the screen. If it looks right, then hide it. If not, re-do the process. Deselect everything, and hide all vectors except for the edge.cuts layer. Select only the inner line, then click 2D Path \u21d2 2D Contour. Edit the 2D Contour settings with the following settings: End depth: 1.70mm Tools: 0.8mm Corn (ensure material is set to \"PCB\") Once these settings are applied, select \"calculate,\" which should make a toolpath appear on the screen. If it looks right, then hide it. If not, re-do the process. Hide everything except for the 3 toolpaths. If everything looks correct, export them by clicking Export Toolpaths in the top right of the screen and selecting the 3 toolpaths. It should save a .nc file. Creating GCode in Makera CAM At the start, Mr. Dubick provided the class with gerber files ( Download ) to create the board from. Once I downloaded these files, I imported them into Makera CAM with File \u21d2 Import PCB. Before doing anything else, I changed the material settings to 127mm x 101mm x 1.7mm (L x W x H) since the blank PCB that I milled was that size. After setting the workspace up, I used the Translate tool to move the board to the coordinates (6,6). Moving the board here allows for the board to be milled with very little wasted space while still allowing ample room for securing the board to the CNC machine. Next, I hid the .drl and f.cu_pad files, leaving only the edge.cuts and f.cu files visible. I selected them and selected 2D Path \u21d2 2D Pocket, which brought up many options. I made sure to change the end depth to 0.05mm and to change the tools to 0.8mm Corn and 0.2mm*30\u00b0 Engraving (Metal) for the 2D Pocket cut. The purpose of the 2D Pocket is to take off the very top layer of copper on the PCB board, leaving behind only the desired traces. Once I had all of my desired settings, I pressed \"calculate\", and it displayed the toolpath for the 2D Pocket cut. Once I created the toolpath for the 2D Pocket cut, the next step was to create a toolpath for drilling the holes in the board. To do so, I hid the edge.cuts, f.cu, and 2D Pocket and showed only the 2 drill files. I selected them, then selected 2D Path \u21d2 2D Drilling in order to drill holes in the board. I set the tool to 0.8mm Corn and end depth to 1.7mm to ensure that the holes are drilled completely though the board. Lastly, I needed to cut out the board. To do so, I hid everything except the edge.cuts vector, then selected 2D Path \u21d2 2D Contour. Again, I chose 0.8mm Corn and set the end depth to 1.7mm so the board gets cut out completely. I also added 3 tabs to make sure that the board would stay in place when it gets cut. Once I created the 3 different toolpaths, I exported them to a .nc (GCode) file ( Download ) so I could import it into the Carvera Controller application. Milling the Board with the CNC Machine using Carvera Controller (Workflow) After I successfully created a GCode file for the CNC machine, I had to mill the board out. To do so, I obtained a blank PCB that matched the dimensions I specified in Makera CAM (127mm x 101mm x 1.7mm), then I put it in the machine and clamped it down. After doing so, I used the Carvera Controller Software to load my GCode and start the job. To do so, I followed this workflow: Open Carvera Controller and connect to the CNC via USB Before doing anything, ensure that the probe is charged to at least 3.6V under \"Tool Status and Control\" In the top right corner, click \"Switch to Display Manual Control Interface,\" then click the \"Home\" button In the bottom left corner, open the GCode file from the computer. To check that everything is correct, click \"Switch to Display File Preview Interface\" in order to preview the toolpath If everything looks as expected, click \"Config and Run\" and ensure that the options for \"auto vacuum\" and \"auto leveling\" are enabled. Once everything looks good, click \"run.\" After following these steps, the machine took ~30 minutes to mill out my board. After it was done, all I had to do was break the 3 tabs that I set, and I was done! Reflection In this project, I learned how to use MakeraCAM to take KiCAD files and turn them into GCode for the Carvera Desktop CNC Machines. I learned about the different types of cuts and their purposes (2D Pocket for removing material off the top, 2D Contour for cutting the board out of the larger material, and 2D Drilling for drilling holes for THT components). Learning these skills will allow me to manufacture boards I custom design for my projects in house rather than having them manufactured overseas, which is super useful for rapid prototyping and cost reduction.","title":"PCB Milling"},{"location":"courses/senior_engineering/pcb_milling/#pcb-milling","text":"In this project, I used MakerCAM and the CNC machines in the Fab Lab in order to create GCode for a board and manufacture it when given gerber files.","title":"PCB Milling"},{"location":"courses/senior_engineering/pcb_milling/#software-and-hardware-used","text":"Carvera Desktop CNC Machine : Machine that milled out the board Makera CAM : Software to generate GCode from gerber and drill files Carvera Controller : Software to control the CNC Machine","title":"Software and Hardware Used:"},{"location":"courses/senior_engineering/pcb_milling/#quick-downloads","text":"GCode for Board KiCAD Files","title":"Quick Downloads"},{"location":"courses/senior_engineering/pcb_milling/#workflow-for-milling-a-board-with-the-carvera-desktop-cnc-machine","text":"Open Makera CAM and open a 3-Axis project Edit the material settings to be a 127mm x 101mm x 1.7mm PCB Import the files with File \u21d2 Import PCB Select all of the imported vectors and click the letter M to move them. Set the origin to the bottom left corner and type in the coordinates (6,6) to move the bottom left corner of the vectors to (6,6) in the workspace. Create a 2D Pocket cut toolpath by selecting only the edge.cuts and f.cu vectors, then clicking 2D Path \u21d2 2D Pocket. Edit the 2D Pocket settings with the following settings: End depth: 0.05mm Tools: 0.8mm Corn and 0.2mm*30\u00b0 Engraving (metal) Ensure that for these tools, the material selected is \"PCB\" Once these settings are applied, select \"calculate,\" which should make a toolpath appear on the screen. If it looks right, then hide it. If not, re-do the process. Hide everything except for the .drl files. Then, select all of the holes and click 2D Path \u21d2 2D Drilling. Edit the 2D Drilling settings with the following settings: End depth: 1.70mm Tools: 0.8mm Corn (ensure material is set to \"PCB\") Once these settings are applied, select \"calculate,\" which should make a toolpath appear on the screen. If it looks right, then hide it. If not, re-do the process. Deselect everything, and hide all vectors except for the edge.cuts layer. Select only the inner line, then click 2D Path \u21d2 2D Contour. Edit the 2D Contour settings with the following settings: End depth: 1.70mm Tools: 0.8mm Corn (ensure material is set to \"PCB\") Once these settings are applied, select \"calculate,\" which should make a toolpath appear on the screen. If it looks right, then hide it. If not, re-do the process. Hide everything except for the 3 toolpaths. If everything looks correct, export them by clicking Export Toolpaths in the top right of the screen and selecting the 3 toolpaths. It should save a .nc file.","title":"Workflow for Milling a Board with the Carvera Desktop CNC Machine"},{"location":"courses/senior_engineering/pcb_milling/#creating-gcode-in-makera-cam","text":"At the start, Mr. Dubick provided the class with gerber files ( Download ) to create the board from. Once I downloaded these files, I imported them into Makera CAM with File \u21d2 Import PCB. Before doing anything else, I changed the material settings to 127mm x 101mm x 1.7mm (L x W x H) since the blank PCB that I milled was that size. After setting the workspace up, I used the Translate tool to move the board to the coordinates (6,6). Moving the board here allows for the board to be milled with very little wasted space while still allowing ample room for securing the board to the CNC machine. Next, I hid the .drl and f.cu_pad files, leaving only the edge.cuts and f.cu files visible. I selected them and selected 2D Path \u21d2 2D Pocket, which brought up many options. I made sure to change the end depth to 0.05mm and to change the tools to 0.8mm Corn and 0.2mm*30\u00b0 Engraving (Metal) for the 2D Pocket cut. The purpose of the 2D Pocket is to take off the very top layer of copper on the PCB board, leaving behind only the desired traces. Once I had all of my desired settings, I pressed \"calculate\", and it displayed the toolpath for the 2D Pocket cut. Once I created the toolpath for the 2D Pocket cut, the next step was to create a toolpath for drilling the holes in the board. To do so, I hid the edge.cuts, f.cu, and 2D Pocket and showed only the 2 drill files. I selected them, then selected 2D Path \u21d2 2D Drilling in order to drill holes in the board. I set the tool to 0.8mm Corn and end depth to 1.7mm to ensure that the holes are drilled completely though the board. Lastly, I needed to cut out the board. To do so, I hid everything except the edge.cuts vector, then selected 2D Path \u21d2 2D Contour. Again, I chose 0.8mm Corn and set the end depth to 1.7mm so the board gets cut out completely. I also added 3 tabs to make sure that the board would stay in place when it gets cut. Once I created the 3 different toolpaths, I exported them to a .nc (GCode) file ( Download ) so I could import it into the Carvera Controller application.","title":"Creating GCode in Makera CAM"},{"location":"courses/senior_engineering/pcb_milling/#milling-the-board-with-the-cnc-machine-using-carvera-controller-workflow","text":"After I successfully created a GCode file for the CNC machine, I had to mill the board out. To do so, I obtained a blank PCB that matched the dimensions I specified in Makera CAM (127mm x 101mm x 1.7mm), then I put it in the machine and clamped it down. After doing so, I used the Carvera Controller Software to load my GCode and start the job. To do so, I followed this workflow: Open Carvera Controller and connect to the CNC via USB Before doing anything, ensure that the probe is charged to at least 3.6V under \"Tool Status and Control\" In the top right corner, click \"Switch to Display Manual Control Interface,\" then click the \"Home\" button In the bottom left corner, open the GCode file from the computer. To check that everything is correct, click \"Switch to Display File Preview Interface\" in order to preview the toolpath If everything looks as expected, click \"Config and Run\" and ensure that the options for \"auto vacuum\" and \"auto leveling\" are enabled. Once everything looks good, click \"run.\" After following these steps, the machine took ~30 minutes to mill out my board. After it was done, all I had to do was break the 3 tabs that I set, and I was done!","title":"Milling the Board with the CNC Machine using Carvera Controller (Workflow)"},{"location":"courses/senior_engineering/pcb_milling/#reflection","text":"In this project, I learned how to use MakeraCAM to take KiCAD files and turn them into GCode for the Carvera Desktop CNC Machines. I learned about the different types of cuts and their purposes (2D Pocket for removing material off the top, 2D Contour for cutting the board out of the larger material, and 2D Drilling for drilling holes for THT components). Learning these skills will allow me to manufacture boards I custom design for my projects in house rather than having them manufactured overseas, which is super useful for rapid prototyping and cost reduction.","title":"Reflection"},{"location":"courses/senior_engineering/pen/","text":"Custom Wooden Pen In this project, I created a custom pen with a pen kit and wood. I used tools such as a drill, lathe, and band saw to custom make the wood accents on the pen. Making the Wooden Barrels I started the process by assembling the two pieces of wood that go around the barrels. I decided to make the pen out of purpleheart and birch, since I really liked the look of the dark purple and the light wood contrasting each other. The first step was to cut two pieces of 1\" long purpleheart wood on the band saw, then cutting those in half to form 4 pieces of 0.5\" thick wood. Then, I cut 2 thin pieces of birch to put in between the purpleheart pieces. Once I had all the pieces, I clamped and glued together the wood to form two blocks of wood that was mostly purpleheart with a thin piece of birch in the middle. I let the glue set overnight. While the glue was drying, I took the 2 brass barrels from the pen kit and made them rough by sanding them. This allows for the barrel to bond better with the superglue when they get glued inside the wood. After the glue dried, I drilled a hole down the middle. Then, I lined the hole in each wood block with superglue, then inserted a sanded barrel into each block. I let it dry overnight. After the barrel fully dried, I sanded the corners in order to make turning it on the lathe easier. After doing so, I took my pieces of wood and put them on the lathe. I then used various chisels to trim off most of the material to make the wood blocks round and easy to grip. Assembling the Pen Now that I had the two wooden barrels, I could assemble the pen. I used the pen press to do the following: Press the pen tip into the bottom end of the lower barrel Press the ink chamber into the top end of the lower barrel Press the clip assembly into the top end of the upper barrel Once I pressed these components, I could assemble the main sections together. I screwed the ink refill into the ink chamber, slid the ring onto the chamber above the lower barrel, and slid the upper barrel above the ring. Once I did that, my pen was done, and it wrote super well, along with looking very cool.","title":"Pen"},{"location":"courses/senior_engineering/pen/#custom-wooden-pen","text":"In this project, I created a custom pen with a pen kit and wood. I used tools such as a drill, lathe, and band saw to custom make the wood accents on the pen.","title":"Custom Wooden Pen"},{"location":"courses/senior_engineering/pen/#making-the-wooden-barrels","text":"I started the process by assembling the two pieces of wood that go around the barrels. I decided to make the pen out of purpleheart and birch, since I really liked the look of the dark purple and the light wood contrasting each other. The first step was to cut two pieces of 1\" long purpleheart wood on the band saw, then cutting those in half to form 4 pieces of 0.5\" thick wood. Then, I cut 2 thin pieces of birch to put in between the purpleheart pieces. Once I had all the pieces, I clamped and glued together the wood to form two blocks of wood that was mostly purpleheart with a thin piece of birch in the middle. I let the glue set overnight. While the glue was drying, I took the 2 brass barrels from the pen kit and made them rough by sanding them. This allows for the barrel to bond better with the superglue when they get glued inside the wood. After the glue dried, I drilled a hole down the middle. Then, I lined the hole in each wood block with superglue, then inserted a sanded barrel into each block. I let it dry overnight. After the barrel fully dried, I sanded the corners in order to make turning it on the lathe easier. After doing so, I took my pieces of wood and put them on the lathe. I then used various chisels to trim off most of the material to make the wood blocks round and easy to grip.","title":"Making the Wooden Barrels"},{"location":"courses/senior_engineering/pen/#assembling-the-pen","text":"Now that I had the two wooden barrels, I could assemble the pen. I used the pen press to do the following: Press the pen tip into the bottom end of the lower barrel Press the ink chamber into the top end of the lower barrel Press the clip assembly into the top end of the upper barrel Once I pressed these components, I could assemble the main sections together. I screwed the ink refill into the ink chamber, slid the ring onto the chamber above the lower barrel, and slid the upper barrel above the ring. Once I did that, my pen was done, and it wrote super well, along with looking very cool.","title":"Assembling the Pen"},{"location":"courses/senior_engineering/topography/","text":"Topography In this project, I milled the topography of the Himalayan Mountains out of a block of hardwood using a CNC machine and toolpaths generated in Vectric Aspire. Table of Contents Workflow Challenges I Overcame and Lessons I Learned File Downloads Workflow Creating the .CNC File in Aspire Download the STL file with the topography Go to jthatch.com/Terrain2STL Move the box over the desired geographical location adjust the size of the box, then increase the water and base settings to the max (this will make height differences more exaggerated). Generate the model by clicking the \"Generate Model\" button, then download the ZIP file and extract the STL from it Import the Model and Set Up Aspire to Generate Toolpaths Open Aspire, then define the material settings to match what is being cut Import the STL file by clicking \"Import a Component or 3D Model\" and selecting the STL file Orient the model by setting \"Initial Orientation\" to \"Top\" then changing the width, height, and depth to match the stock size. Ensure to uncheck \"Lock XYZ ratio\". Click \"Apply\" then \"Center Model\", then if the model looks correct, click \"Position and Import\" Use the slider bar on the left of the screen to set the \"Position relative to Modeling plane\" to the maximum (drag the slider all the way down) Create Toolpaths Switch to the \"Design\" tab and select \"2D View\". Then, center the object and draw a rectangle around the edge of the model Switch to the \"Toolpaths\" tab. Start by making a 3D Roughing Toolpath by clicking the \"3D Roughing Toolpath\" icon. The roughing toolpath cuts away the bulk of the unused material with a large, strong bit. For the topography, use the following settings: Tool: 25 mm Flute End Mill (3.175 mm) Machining Limit Boundary: Model Boundary Machining Allowance: 0.024\" Strategy: Z Level Name: 3D Rough - 25 mm Flute End Mill (can be anything) Click \"Calculate\" when finished Next, create the \"3D Finishing Toolpath\". This uses a much more precise tool to slowly go over the entire model, creating a smooth, detailed final surface. For the topography project, use the following settings: Tool: 0.125\" Ball Nose bit Machining Limit Boundary: Model Boundary Strategy: Raster (Set \"Raster Angle\" to 0\u00b0 to go back and forth along the X-axis) Name: 3D Finish - 0.125 Ballnose (can be anything) Click \"Calculate\" when finished Finally, create a 2D Roughing Toolpath. The 2D Profile Toolpath cuts the model out of the larger block of wood. However, for this project, since the wood is thick, the cutting depth was set to 0.5\" to create a guide for cutting the whole model out with a saw. For the topography project, use the following settings: Tool:25 mm Flute End Mill Machine Vectors: On Direction: Climb Name: 2D Profile - 25 mm Flute End Mill Click \"Calculate\" when finished Once all the toolpaths have been calculated, preview all of the toolpaths by clicking \"Preview All Toolpaths\". Check that the end product looks okay. Save the G-Code (toolpath files) by clicking \"Close\" to exit the \"Preview Toolpaths\" menu then clicking the \"Save Toolpaths\" button (looks like a floppy disk). Ensure that all 3 toolpaths are checked before clicking \"Save Toolpaths\", and that the right machine is selected. Once the button is clicked, either a .CNC or .GCODE file should be exported. Milling the Topography on the Carvera Desktop CNC Machine Open the Carvera Controller software on the desktop In the top toolbar, click on the button with the status \u201cN/A disconnected\u201d Select the appropriate COM port to connect the Carvera to the computer (if the COM port is already connected, leave it as is) In the menu in the top right corner, click \u201cSwitch to display manual control interface\u201d followed by the \u201cHome\u201d button Under \u201cTool Status and Control,\u201d ensure that the probe is charged to at least 3.6V (this ensures the machine operates in the z-axis as intended) In the bottom left corner, open the G-code from your files Before starting the mill, open the menu in the top right corner and click the \u201cSwitch to display file preview interface\u201d to preview the toolpaths Click \u201cConfig and run,\u201d and ensure that the \u201cauto vacuum\u201d is on and \u201cauto leveling\u201d options is off. Once all settings are verified, click \u201cRun\u201d Challenges I Overcame and Lessons I Learned Downloads","title":"Topography"},{"location":"courses/senior_engineering/topography/#topography","text":"In this project, I milled the topography of the Himalayan Mountains out of a block of hardwood using a CNC machine and toolpaths generated in Vectric Aspire.","title":"Topography"},{"location":"courses/senior_engineering/topography/#table-of-contents","text":"Workflow Challenges I Overcame and Lessons I Learned File Downloads","title":"Table of Contents"},{"location":"courses/senior_engineering/topography/#workflow","text":"","title":"Workflow"},{"location":"courses/senior_engineering/topography/#creating-the-cnc-file-in-aspire","text":"Download the STL file with the topography Go to jthatch.com/Terrain2STL Move the box over the desired geographical location adjust the size of the box, then increase the water and base settings to the max (this will make height differences more exaggerated). Generate the model by clicking the \"Generate Model\" button, then download the ZIP file and extract the STL from it Import the Model and Set Up Aspire to Generate Toolpaths Open Aspire, then define the material settings to match what is being cut Import the STL file by clicking \"Import a Component or 3D Model\" and selecting the STL file Orient the model by setting \"Initial Orientation\" to \"Top\" then changing the width, height, and depth to match the stock size. Ensure to uncheck \"Lock XYZ ratio\". Click \"Apply\" then \"Center Model\", then if the model looks correct, click \"Position and Import\" Use the slider bar on the left of the screen to set the \"Position relative to Modeling plane\" to the maximum (drag the slider all the way down) Create Toolpaths Switch to the \"Design\" tab and select \"2D View\". Then, center the object and draw a rectangle around the edge of the model Switch to the \"Toolpaths\" tab. Start by making a 3D Roughing Toolpath by clicking the \"3D Roughing Toolpath\" icon. The roughing toolpath cuts away the bulk of the unused material with a large, strong bit. For the topography, use the following settings: Tool: 25 mm Flute End Mill (3.175 mm) Machining Limit Boundary: Model Boundary Machining Allowance: 0.024\" Strategy: Z Level Name: 3D Rough - 25 mm Flute End Mill (can be anything) Click \"Calculate\" when finished Next, create the \"3D Finishing Toolpath\". This uses a much more precise tool to slowly go over the entire model, creating a smooth, detailed final surface. For the topography project, use the following settings: Tool: 0.125\" Ball Nose bit Machining Limit Boundary: Model Boundary Strategy: Raster (Set \"Raster Angle\" to 0\u00b0 to go back and forth along the X-axis) Name: 3D Finish - 0.125 Ballnose (can be anything) Click \"Calculate\" when finished Finally, create a 2D Roughing Toolpath. The 2D Profile Toolpath cuts the model out of the larger block of wood. However, for this project, since the wood is thick, the cutting depth was set to 0.5\" to create a guide for cutting the whole model out with a saw. For the topography project, use the following settings: Tool:25 mm Flute End Mill Machine Vectors: On Direction: Climb Name: 2D Profile - 25 mm Flute End Mill Click \"Calculate\" when finished Once all the toolpaths have been calculated, preview all of the toolpaths by clicking \"Preview All Toolpaths\". Check that the end product looks okay. Save the G-Code (toolpath files) by clicking \"Close\" to exit the \"Preview Toolpaths\" menu then clicking the \"Save Toolpaths\" button (looks like a floppy disk). Ensure that all 3 toolpaths are checked before clicking \"Save Toolpaths\", and that the right machine is selected. Once the button is clicked, either a .CNC or .GCODE file should be exported.","title":"Creating the .CNC File in Aspire"},{"location":"courses/senior_engineering/topography/#milling-the-topography-on-the-carvera-desktop-cnc-machine","text":"Open the Carvera Controller software on the desktop In the top toolbar, click on the button with the status \u201cN/A disconnected\u201d Select the appropriate COM port to connect the Carvera to the computer (if the COM port is already connected, leave it as is) In the menu in the top right corner, click \u201cSwitch to display manual control interface\u201d followed by the \u201cHome\u201d button Under \u201cTool Status and Control,\u201d ensure that the probe is charged to at least 3.6V (this ensures the machine operates in the z-axis as intended) In the bottom left corner, open the G-code from your files Before starting the mill, open the menu in the top right corner and click the \u201cSwitch to display file preview interface\u201d to preview the toolpaths Click \u201cConfig and run,\u201d and ensure that the \u201cauto vacuum\u201d is on and \u201cauto leveling\u201d options is off. Once all settings are verified, click \u201cRun\u201d","title":"Milling the Topography on the Carvera Desktop CNC Machine"},{"location":"courses/senior_engineering/topography/#challenges-i-overcame-and-lessons-i-learned","text":"","title":"Challenges I Overcame and Lessons I Learned"},{"location":"courses/senior_engineering/topography/#downloads","text":"","title":"Downloads"}]}